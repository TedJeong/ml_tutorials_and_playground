{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Contents and References"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Collections of lectures, notes, codes. The best way to learn is  \n",
    "1. Listen the reference lecture for a general description\n",
    "2. Read the paper for a detailed description\n",
    "3. Write codes along with reference codes and compare them to 1,2\n",
    "4. Memorize the structure and play with it"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Contents\n",
    "Chapter 1. [Basic Machine learning Models](ml_basic_collections.ipynb)  \n",
    "\n",
    "Chapter 2 to 5 : [ML_collections](#ML_collections.ipynb)  \n",
    "\n",
    "Chapter 6. Unsupervised learning to RL  \n",
    "  - section 1. [GAN_collections](GAN_collections.ipynb) : Generative Models  \n",
    "  - section 2. [RL_collections](RL_collections.ipynb) : Reinforcement Learning Models  \n",
    "  \n",
    "Chaper 7. Library tutorials and aside\n",
    "- [scikit-learn](sklearn_playground.ipynb) : scikit-learn basic  \n",
    "- [tensorflow](tensorflow_playground.ipynb) : tensorflow basic  \n",
    "- [pytorch](pytorch_playground.ipynb) : pytorch basic  \n",
    "- [AL_collections](AL_collections.ipynb) : Algorithms and design patterns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Requirements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import sklearn\n",
    "import tensorflow as tf\n",
    "import keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.12.1\n",
      "0.19.1\n",
      "0.18.1\n",
      "1.0.1\n",
      "2.0.2\n"
     ]
    }
   ],
   "source": [
    "print(np.__version__)\n",
    "print(pd.__version__)\n",
    "print(sklearn.__version__)\n",
    "print(tf.__version__)\n",
    "print(keras.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "DATASET_DIR = './dataset/'\n",
    "PROJECT_DIR = './projects/ML_collections/' + 'Seg_Upsampling_Berkeley/'\n",
    "SUMMARY_DIR = PROJECT_DIR+'summaries/'\n",
    "SAVER_DIR = PROJECT_DIR+'models/'\n",
    "CHECKPOINT_DIR = PROJECT_DIR+'checkpoints/'\n",
    "RESULT_DIR = PROJECT_DIR+'results/'\n",
    "\n",
    "import os\n",
    "path_dics = dict(DATASET_DIR = './dataset/',\n",
    "PROJECT_DIR = './projects/ML_collections/' + 'Seg_Upsampling_Berkeley/',\n",
    "SUMMARY_DIR = PROJECT_DIR+'summaries/',\n",
    "SAVER_DIR = PROJECT_DIR+'models/',\n",
    "CHECKPOINT_DIR = PROJECT_DIR+'checkpoints/',\n",
    "RESULT_DIR = PROJECT_DIR+'results/')\n",
    "\n",
    "for dir_, path in path_dics.items():\n",
    "    if not os.path.exists(path):\n",
    "        os.makedirs(path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reference\n",
    " - Reference Codes\n",
    " - Journals\n",
    " - Blogs\n",
    " - Docs\n",
    "    * [Scikit-learn](http://scikit-learn.org/stable/user_guide.htmlS)\n",
    "    * [github book](https://leonardoaraujosantos.gitbooks.io/artificial-inteligence/content/object_localization_and_detection.html)\n",
    " ## courses\n",
    "    * Basic :  \n",
    "      ** Coursera **  \n",
    "        -  \n",
    "        - c  \n",
    "      ** kim **\n",
    "        -  \n",
    "        - [test](./native)  \n",
    "    \n",
    "    * ConvNet :  \n",
    "      **=[Stanford cs231n 2016](../courses/cs231n)=**\n",
    "        - **Lec 2,3 : KNN, Linear Classification**\n",
    "        - **Lec 4,5,6 : Neural Nets**  \n",
    "            0. BackPropagation lec4[:]\n",
    "            1. One time setup  \n",
    "              Activation fuction lec5[14:10]  \n",
    "              Data Preprocessing lec5[33:57]  \n",
    "              Weight Initialization lec5[37:00]  \n",
    "              Batch Normalization lec5[51:00]  \n",
    "            2. Training dynamics\n",
    "              Learning Process lec5[58:25]  \n",
    "              Parameter update scheme lec6[3:54]  \n",
    "              Learning rate scheduling lec  \n",
    "              \n",
    "              Regularization : Dropout lec6[38:27]  \n",
    "              Gradient Checking lec6[56:58]   \n",
    "              \n",
    "              Model ensembles lec6[36:19]  \n",
    "        - **Lec 7 : Convolution Neural Nets** lec6[57:35]  \n",
    "        - **Lec 8 : Localization, object detection**  \n",
    "            0. Class Activation Map (fc week 6b,c,d)\n",
    "              weakly supervised learning\n",
    "            1. Localization  \n",
    "              Localization as regression : variable size output for the number of object  \n",
    "              Localization using sliding window [12:50]  [slide 22]  \n",
    "              [Sermanet et al, “Integrated Recognition, Localization and Detection using Convolutional Networks”, ICLR 2014](https://arxiv.org/abs/1312.6229)  \n",
    "            2. Object Detection [slide 37]  \n",
    "              Detection as classification  \n",
    "              Region Proposal : Selective Search [31:30]  \n",
    "              [Jan Hosang et al, \"What makes for effective detection proposals?\", PAMI, 2015](https://arxiv.org/abs/1502.05082)  \n",
    "                  R-CNN : Selective search -> CNN for each ; note its dataset goes up to~200GB for pascal VOC, 84h training [32:48]  \n",
    "                  [Girschick et al, “Rich feature hierarchies for accurate object detection and semantic segmentation”, CVPR 2014](https://arxiv.org/abs/1311.2524)  \n",
    "                  [SPP](https://arxiv.org/pdf/1406.4729.pdf)\n",
    "                  Fast R-CNN : CNN -> ROI pooling (single level Spatial Pyramid Pooling), ~ 9.5h training [41:20]  \n",
    "                  [Girschick, “Fast R-CNN”, ICCV 2015](https://arxiv.org/abs/1504.08083)  \n",
    "                  Faster R-CNN : cnn for region proposal network [47:00]  \n",
    "                  [Ren et al, “Faster R-CNN: Towards Real-Time Object Detection with Region Proposal Networks”, NIPS 2015](https://papers.nips.cc/paper/5638-faster-r-cnn-towards-real-time-object-detection-with-region-proposal-networks.pdf)  \n",
    "                  YOLO : dividing , low recall high precision.  \n",
    "                  [Joseph Redmon et al, \"You Only Look Once: Unified, Real-Time Object Detection\"](https://arxiv.org/abs/1506.02640)  \n",
    "                  SSD(Single Shot MultiBox Detector) : RPN + YOLO + skip connection  \n",
    "                  [Wei Liu et al, SSD: Single Shot MultiBox Detector, arxiv](https://arxiv.org/pdf/1512.02325.pdf)  \n",
    "                  YOLO9000 :   \n",
    "                  [YOLO9000](https://arxiv.org/pdf/1612.08242.pdf)  \n",
    "                  Mask-RCNN  \n",
    "                  [Mask R-CNN](https://arxiv.org/pdf/1703.06870.pdf)  \n",
    "              \n",
    "        - **Lec 9 : Visualization**\n",
    "            1. convvis wiht deconv [16:18]  \n",
    "                Backprop, guided backpropagation  \n",
    "                [Visualizing and Understanding Convolutional Networks, Zeiler and Fergus 2013](https://arxiv.org/pdf/1311.2901.pdf)  \n",
    "                [Deep Inside Convolutional Networks: Visualising Image Classification Models and Saliency Maps, Simonyan et al., 2014](https://arxiv.org/pdf/1312.6034.pdf)  \n",
    "                [Striving for Simplicity: The all convolutional net, Springenberg, Dosovitskiy, et al., 2015](https://arxiv.org/abs/1412.6806)  \n",
    "                \n",
    "            2. convvis with Optimization [29:17]  \n",
    "            3. Reconstruction [40:03]  \n",
    "                [Understanding Deep Image Representations by Inverting Them Mahendran and Vedaldi, 2014](https://arxiv.org/pdf/1412.0035.pdf)  \n",
    "            4. DeepDream [42:44]  \n",
    "            5. Neural Style [52:00]  \n",
    "                [A Neural Algorithm of Artistic Style by Leon A. Gatys, Alexander S. Ecker, and Matthias Bethge, 2015](https://arxiv.org/abs/1508.06576)\n",
    "                Content Activation  \n",
    "                Style gram matrices   \n",
    "                Fooling Neural Net [1:02:08]  \n",
    "        - **[-]Lec 10 : Recurrent Neural Networks, Image Captioning**  \n",
    "            [Razvan Pascanu et al., \"On the difficulty of training recurrent neural networks\"](http://www.jmlr.org/proceedings/papers/v28/pascanu13.pdf)\n",
    "            1. char RNN\n",
    "            2. Image Captioning [31:19], [slide 52]\n",
    "            3. LSTM [46:00]  \n",
    "            ~ related works about attention models in lec 13  \n",
    "            [GRU Learning phrase representations using rnn encoder-decoder for statistical machine translation, Cho et al. 2014](https://arxiv.org/abs/1406.1078)  \n",
    "            [An Empirical Exploration of Recurrent Network Architectures, Jozefowicz et al., 2015](http://jmlr.org/proceedings/papers/v37/jozefowicz15.pdf)  \n",
    "        - **[]Lec 11 : ConvNets in practice**\n",
    "            1. Data augmentation  \n",
    "            2. Transfer Learning  \n",
    "            3. Building ConvNet  \n",
    "                Part 1 : How to stack convolution layers : Power of small filter [25:24]  \n",
    "                Part 2 : How to calculate [36:32]    \n",
    "                  - Convolution to matrix multiplication (im2col)  \n",
    "                  - Convolution to Fast fourier transformation [43:00]  \n",
    "                  - Convolution to Fast algorithm [46:40]  \n",
    "            4. precision, bit precision  \n",
    "        - **Lec 12 : Deep learning packages**\n",
    "        - **[x]Lec 13 : Segmentation, Attention**\n",
    "            1. Semantic Segmentation  \n",
    "              - Multiscale  \n",
    "                - [Farabet et al, “Learning Hierarchical Features for Scene Labeling,” TPAMI 2013](http://yann.lecun.com/exdb/publis/pdf/farabet-pami-13.pdf)  \n",
    "              - Refinement  \n",
    "                - [Pinheiro and Collobert, “Recurrent Convolutional Neural Networks for Scene Labeling”, ICML 2014](https://ronan.collobert.com/pub/matos/2014_scene_icml.pdf)  \n",
    "              - Upsampling  \n",
    "                - [Long, Shelhamer, and Darrell, “Fully Convolutional Networks for Semantic Segmentation”, CVPR 2015](https://people.eecs.berkeley.edu/~jonlong/long_shelhamer_fcn.pdf)  \n",
    "                    : Fully-connected layers are exchanged to 1xn and stride 2 convolution for upsampling which makes end-to-end learning, skip connections for various receptive field resolution.  \n",
    "                - [Noh et al, “Learning Deconvolution Network for Semantic Segmentation”, ICCV 2015](http://www.cv-foundation.org/openaccess/content_iccv_2015/papers/Noh_Learning_Deconvolution_Network_ICCV_2015_paper.pdf)  \n",
    "               \n",
    "                    : Unpooling is used to do upsampling.  \n",
    "                - [Liang-Chieh Chen et al, \"DeepLab: Semantic Image Segmentation with Deep Convolutional Nets, Atrous Convolution, and Fully Connected CRFs\"](https://arxiv.org/pdf/1606.00915.pdf)  \n",
    "                    : Astrous convolution, Astrous Spatial Pyramid Pooling for upsampling with CRF for postprocessing.\n",
    "            2. Instance Segmentation  \n",
    "                  [Hariharan et al, “Simultaneous Detection and Segmentation”, ECCV 2014](https://arxiv.org/abs/1407.1808)  \n",
    "                Hypercolumns ~ R-CNN  \n",
    "                  [Hariharan et al, “Hypercolumns for Object Segmentation and Fine-grained Localization”, CVPR 2015](https://arxiv.org/pdf/1411.5752.pdf)  \n",
    "                Cascades ~ FasterR-CNN [34:00]  \n",
    "                  [Dai et al, “Instance-aware Semantic Segmentation via Multi-task Network Cascades”, arXiv 2015](https://arxiv.org/abs/1512.04412)  \n",
    "            3. Attention on Discrete\n",
    "                Soft Attention for Captioning  \n",
    "                  [Xu et al, “Show, Attend and Tell: Neural Image Caption Generation with Visual Attention”, ICML 2015](https://arxiv.org/abs/1502.03044)  [40:00]  \n",
    "                  [Bahdanau et al, “Neural Machine Translation by Jointly Learning to Align and Translate”, ICLR 2015](https://arxiv.org/abs/1409.0473)  \n",
    "                Attending Arbitrary Regions\n",
    "                  [Graves, “Generating Sequences with Recurrent Neural Networks”, arXiv 2013](https://arxiv.org/pdf/1308.0850.pdf)\n",
    "            4. Attention on Continuous : Attending Arbitrary Regions  \n",
    "                on Arbitrary Region with grid  \n",
    "                  [DRAW: A Recurrent Neural Network For Image Generation](https://arxiv.org/abs/1502.04623)  \n",
    "                Spatial transformers  [59:00]  \n",
    "                  [Jaderberg et al, “Spatial Transformer Networks”, NIPS 2015](https://papers.nips.cc/paper/5854-spatial-transformer-networks.pdf)\n",
    "        - **Lec 14 : Videos**\n",
    "            1. Model temporal motion locally (3d Conv)\n",
    "            2. Model temporal motion globally (LSTM / RNN)\n",
    "        - **Lec 14 : Unsupervised Learning** [32:00]\n",
    "            1. Auto Encoder\n",
    "            2. Variational Auto Encoder [48:44]  \n",
    "                [Diederik P Kingma, Max Welling, \"Auto-Encoding Variational Bayes\"](https://arxiv.org/abs/1312.6114)\n",
    "            3. Generative Adversarial Nets\n",
    "    * NLP :  \n",
    "      **=[Stanford cs224d](https://www.youtube.com/playlist?list=PLmImxx8Char9Ig0ZHSyTqGsdhb9weEGamS)=**  \n",
    "      **=[Stanford 2017](https://www.youtube.com/watch?v=OQQ-W_63UgQ&list=PL3FW7Lu3i5Jsnh1rnUwq_TcylNr7EkRe6)=**\n",
    "    * Deep Reinforcement learning :  \n",
    "      **=[UCB CS294 2017](http://rll.berkeley.edu/deeprlcourse/#lecture-videos)=**  \n",
    "      **=MIT CS 6.034=**  \n",
    "    * Generative Adversarial Network : NIPS tutorial 2016  \n",
    "    \n",
    "  - Conference\n",
    "    * [[TED talks]Chris Urmson: How a driverless car sees the road](https://www.youtube.com/watch?v=tiwVMrTLUWg)\n",
    "    * [[TED talks] Autodesk Generative Design](https://www.youtube.com/watch?v=aR5N2Jl8k14)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
