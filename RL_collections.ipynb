{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Reinforcement Learning Reference Collections"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Status quo label\n",
    "[.]:copied [-]:on progress [=]:written [%]:completed [@]:memorized\n",
    "\n",
    "## Contents\n",
    "\n",
    "\n",
    "1. [QLEARNING] : q-table-simple learning\n",
    "    - [%][Q-learning tensorflow](#Q-learning tensorflow)  \n",
    "2. [RL_PBA] : policy based learning  \n",
    "    - [-][Policy Based Agent tensorflow](#Policy Based Agent tensorflow)  \n",
    "3. [RL_MBA] : model based learning  \n",
    "    - [-][Model Based Agent tensorflow](#Model Based Agent tensorflow)  \n",
    "4. [DQN] : deep qlearning network  \n",
    "    - [%][Deep Q-Learning Network tensorflow](#Deep Q-Learning Network tensorflow)  \n",
    "5. [DDQN] : deep qlearning Network + dueling, double  \n",
    "    - [=][Double-Dualing Deep Q-Learning Network tensorflow](#Double-Dualing Deep Q-Learning Network tensorflow)      \n",
    "6. [A3C] : asynchronous advantage actor critic model  \n",
    "    - [-][Asynchronous Advantages Actor-Critic Model tensorflow on Breakout-v0](#Asynchronous Advantages Actor-Critic Model tensorflow)  \n",
    "7. [Meta_RL](#Meta_RL) : meta reinforcement learning [paper1](https://arxiv.org/pdf/1611.05763.pdf) [paper2](https://arxiv.org/abs/1611.02779)  \n",
    "    - [][]()\n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "[][DRQN] : deep recurrent q-network  \n",
    "\n",
    "Partially observable Markov Decision Process to maximize cumulative reward  \n",
    "Decision problems,  \n",
    "Imitation learning  \n",
    "- behavioral learning  \n",
    "- inverse reinforcement learning  \n",
    "DAgger (Dataset Aggregation)  \n",
    "Q-learning  \n",
    "\n",
    "\n",
    "Policy gradient learning : observation → action  \n",
    "\n",
    "Q-learning : long term reward : state with reward → action  \n",
    "\n",
    "Experience replay  \n",
    "Freezing target network  \n",
    "\n",
    " \n",
    "\n",
    "## Reference  \n",
    "\n",
    "\n",
    "[Arthur Juliani](https://medium.com/emergent-future/simple-reinforcement-learning-with-tensorflow-part-0-q-learning-with-tables-and-neural-networks-d195264329d0#.u07v7laru)  \n",
    "\n",
    "[Arthur Juliani github](https://github.com/awjuliani/DeepRL-Agents)\n",
    "\n",
    "  Aside\n",
    "  [Neural Stack Machine](https://iamtrask.github.io/2016/02/25/deepminds-neural-stack-machine/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "DATASET_DIR = './dataset/'\n",
    "PROJECT_DIR = './projects/RL_collections/'\n",
    "SUMMARY_DIR = PROJECT_DIR+'summaries/'\n",
    "SAVER_DIR = PROJECT_DIR+'models/'\n",
    "CHECKPOINT_DIR = PROJECT_DIR+'checkpoints/'\n",
    "RESULT_DIR = PROJECT_DIR+'results/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# NOTE: In the verbose version, There are example codes with doc string inside function which is not a doctest.\n",
    "# NOTE: For the sake of educational purpose, these codes are note well organized.\n",
    "# TODO: RL_PBA agent version NOT DONE, DQN: , DDQN: MEM error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "====================\n",
      "running time : \n",
      "5.001129865646362 s\n",
      "====================\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "def print_runtime(func):\n",
    "    start = time.time()\n",
    "    func()\n",
    "    end = time.time()\n",
    "    print(\"=\"*20)\n",
    "    print(\"running time : \")\n",
    "    print(end-start, \"s\")\n",
    "    print(\"=\"*20)\n",
    "    \n",
    "def testfun():\n",
    "    time.sleep(5)\n",
    "    \n",
    "print_runtime(testfun)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='Q-learning tensorflow'></a>\n",
    "## Q-learning tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2017-03-15 22:09:21,129] Making new env: FrozenLake-v0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode : 0, reward : 0.00000\n",
      "episode : 1, reward : 0.00000\n",
      "episode : 2, reward : 0.00000\n",
      "episode : 3, reward : 0.00000\n",
      "episode : 4, reward : 0.00000\n",
      "episode : 5, reward : 1.00000\n",
      "episode : 6, reward : 0.00000\n",
      "episode : 7, reward : 0.00000\n",
      "episode : 8, reward : 1.00000\n",
      "episode : 9, reward : 0.00000\n",
      "episode : 10, reward : 0.00000\n",
      "episode : 11, reward : 0.00000\n",
      "episode : 12, reward : 0.00000\n",
      "episode : 13, reward : 0.00000\n",
      "episode : 14, reward : 1.00000\n",
      "episode : 15, reward : 0.00000\n",
      "episode : 16, reward : 0.00000\n",
      "episode : 17, reward : 1.00000\n",
      "episode : 18, reward : 0.00000\n",
      "episode : 19, reward : 0.00000\n",
      "episode : 20, reward : 0.00000\n",
      "episode : 21, reward : 0.00000\n",
      "episode : 22, reward : 0.00000\n",
      "episode : 23, reward : 1.00000\n",
      "episode : 24, reward : 0.00000\n",
      "episode : 25, reward : 0.00000\n",
      "episode : 26, reward : 1.00000\n",
      "episode : 27, reward : 1.00000\n",
      "episode : 28, reward : 0.00000\n",
      "episode : 29, reward : 0.00000\n",
      "episode : 30, reward : 0.00000\n",
      "episode : 31, reward : 1.00000\n",
      "episode : 32, reward : 0.00000\n",
      "episode : 33, reward : 0.00000\n",
      "episode : 34, reward : 1.00000\n",
      "episode : 35, reward : 0.00000\n",
      "episode : 36, reward : 0.00000\n",
      "episode : 37, reward : 1.00000\n",
      "episode : 38, reward : 0.00000\n",
      "episode : 39, reward : 0.00000\n",
      "episode : 40, reward : 0.00000\n",
      "episode : 41, reward : 0.00000\n",
      "episode : 42, reward : 0.00000\n",
      "episode : 43, reward : 1.00000\n",
      "episode : 44, reward : 0.00000\n",
      "episode : 45, reward : 0.00000\n",
      "episode : 46, reward : 1.00000\n",
      "episode : 47, reward : 0.00000\n",
      "episode : 48, reward : 1.00000\n",
      "episode : 49, reward : 0.00000\n",
      "episode : 50, reward : 0.00000\n",
      "episode : 51, reward : 0.00000\n",
      "episode : 52, reward : 0.00000\n",
      "episode : 53, reward : 0.00000\n",
      "episode : 54, reward : 0.00000\n",
      "episode : 55, reward : 0.00000\n",
      "episode : 56, reward : 0.00000\n",
      "episode : 57, reward : 1.00000\n",
      "episode : 58, reward : 0.00000\n",
      "episode : 59, reward : 0.00000\n",
      "episode : 60, reward : 0.00000\n",
      "episode : 61, reward : 0.00000\n",
      "episode : 62, reward : 0.00000\n",
      "episode : 63, reward : 0.00000\n",
      "episode : 64, reward : 0.00000\n",
      "episode : 65, reward : 0.00000\n",
      "episode : 66, reward : 0.00000\n",
      "episode : 67, reward : 0.00000\n",
      "episode : 68, reward : 0.00000\n",
      "episode : 69, reward : 0.00000\n",
      "episode : 70, reward : 0.00000\n",
      "episode : 71, reward : 0.00000\n",
      "episode : 72, reward : 0.00000\n",
      "episode : 73, reward : 0.00000\n",
      "episode : 74, reward : 1.00000\n",
      "episode : 75, reward : 1.00000\n",
      "episode : 76, reward : 1.00000\n",
      "episode : 77, reward : 1.00000\n",
      "episode : 78, reward : 0.00000\n",
      "episode : 79, reward : 0.00000\n",
      "episode : 80, reward : 0.00000\n",
      "episode : 81, reward : 0.00000\n",
      "episode : 82, reward : 0.00000\n",
      "episode : 83, reward : 0.00000\n",
      "episode : 84, reward : 1.00000\n",
      "episode : 85, reward : 0.00000\n",
      "episode : 86, reward : 0.00000\n",
      "episode : 87, reward : 0.00000\n",
      "episode : 88, reward : 0.00000\n",
      "episode : 89, reward : 1.00000\n",
      "episode : 90, reward : 1.00000\n",
      "episode : 91, reward : 0.00000\n",
      "episode : 92, reward : 1.00000\n",
      "episode : 93, reward : 0.00000\n",
      "episode : 94, reward : 1.00000\n",
      "episode : 95, reward : 0.00000\n",
      "episode : 96, reward : 0.00000\n",
      "episode : 97, reward : 0.00000\n",
      "episode : 98, reward : 0.00000\n",
      "episode : 99, reward : 0.00000\n",
      "episode : 100, reward : 0.00000\n",
      "episode : 101, reward : 0.00000\n",
      "episode : 102, reward : 0.00000\n",
      "episode : 103, reward : 0.00000\n",
      "episode : 104, reward : 0.00000\n",
      "episode : 105, reward : 0.00000\n",
      "episode : 106, reward : 1.00000\n",
      "episode : 107, reward : 0.00000\n",
      "episode : 108, reward : 0.00000\n",
      "episode : 109, reward : 1.00000\n",
      "episode : 110, reward : 0.00000\n",
      "episode : 111, reward : 0.00000\n",
      "episode : 112, reward : 0.00000\n",
      "episode : 113, reward : 1.00000\n",
      "episode : 114, reward : 0.00000\n",
      "episode : 115, reward : 1.00000\n",
      "episode : 116, reward : 0.00000\n",
      "episode : 117, reward : 0.00000\n",
      "episode : 118, reward : 0.00000\n",
      "episode : 119, reward : 0.00000\n",
      "episode : 120, reward : 0.00000\n",
      "episode : 121, reward : 0.00000\n",
      "episode : 122, reward : 0.00000\n",
      "episode : 123, reward : 1.00000\n",
      "episode : 124, reward : 0.00000\n",
      "episode : 125, reward : 1.00000\n",
      "episode : 126, reward : 0.00000\n",
      "episode : 127, reward : 0.00000\n",
      "episode : 128, reward : 0.00000\n",
      "episode : 129, reward : 0.00000\n",
      "episode : 130, reward : 0.00000\n",
      "episode : 131, reward : 0.00000\n",
      "episode : 132, reward : 0.00000\n",
      "episode : 133, reward : 0.00000\n",
      "episode : 134, reward : 1.00000\n",
      "episode : 135, reward : 1.00000\n",
      "episode : 136, reward : 1.00000\n",
      "episode : 137, reward : 0.00000\n",
      "episode : 138, reward : 0.00000\n",
      "episode : 139, reward : 0.00000\n",
      "episode : 140, reward : 0.00000\n",
      "episode : 141, reward : 1.00000\n",
      "episode : 142, reward : 1.00000\n",
      "episode : 143, reward : 0.00000\n",
      "episode : 144, reward : 0.00000\n",
      "episode : 145, reward : 1.00000\n",
      "episode : 146, reward : 0.00000\n",
      "episode : 147, reward : 0.00000\n",
      "episode : 148, reward : 0.00000\n",
      "episode : 149, reward : 0.00000\n",
      "episode : 150, reward : 0.00000\n",
      "episode : 151, reward : 0.00000\n",
      "episode : 152, reward : 1.00000\n",
      "episode : 153, reward : 0.00000\n",
      "episode : 154, reward : 0.00000\n",
      "episode : 155, reward : 0.00000\n",
      "episode : 156, reward : 1.00000\n",
      "episode : 157, reward : 1.00000\n",
      "episode : 158, reward : 0.00000\n",
      "episode : 159, reward : 0.00000\n",
      "episode : 160, reward : 1.00000\n",
      "episode : 161, reward : 0.00000\n",
      "episode : 162, reward : 0.00000\n",
      "episode : 163, reward : 0.00000\n",
      "episode : 164, reward : 1.00000\n",
      "episode : 165, reward : 0.00000\n",
      "episode : 166, reward : 0.00000\n",
      "episode : 167, reward : 0.00000\n",
      "episode : 168, reward : 0.00000\n",
      "episode : 169, reward : 1.00000\n",
      "episode : 170, reward : 0.00000\n",
      "episode : 171, reward : 0.00000\n",
      "episode : 172, reward : 1.00000\n",
      "episode : 173, reward : 0.00000\n",
      "episode : 174, reward : 0.00000\n",
      "episode : 175, reward : 1.00000\n",
      "episode : 176, reward : 0.00000\n",
      "episode : 177, reward : 1.00000\n",
      "episode : 178, reward : 0.00000\n",
      "episode : 179, reward : 1.00000\n",
      "episode : 180, reward : 0.00000\n",
      "episode : 181, reward : 0.00000\n",
      "episode : 182, reward : 0.00000\n",
      "episode : 183, reward : 0.00000\n",
      "episode : 184, reward : 0.00000\n",
      "episode : 185, reward : 0.00000\n",
      "episode : 186, reward : 0.00000\n",
      "episode : 187, reward : 0.00000\n",
      "episode : 188, reward : 1.00000\n",
      "episode : 189, reward : 0.00000\n",
      "episode : 190, reward : 1.00000\n",
      "episode : 191, reward : 0.00000\n",
      "episode : 192, reward : 0.00000\n",
      "episode : 193, reward : 0.00000\n",
      "episode : 194, reward : 1.00000\n",
      "episode : 195, reward : 0.00000\n",
      "episode : 196, reward : 1.00000\n",
      "episode : 197, reward : 0.00000\n",
      "episode : 198, reward : 0.00000\n",
      "episode : 199, reward : 1.00000\n",
      "episode : 200, reward : 0.00000\n",
      "episode : 201, reward : 1.00000\n",
      "episode : 202, reward : 0.00000\n",
      "episode : 203, reward : 0.00000\n",
      "episode : 204, reward : 0.00000\n",
      "episode : 205, reward : 1.00000\n",
      "episode : 206, reward : 1.00000\n",
      "episode : 207, reward : 0.00000\n",
      "episode : 208, reward : 0.00000\n",
      "episode : 209, reward : 0.00000\n",
      "episode : 210, reward : 0.00000\n",
      "episode : 211, reward : 0.00000\n",
      "episode : 212, reward : 0.00000\n",
      "episode : 213, reward : 0.00000\n",
      "episode : 214, reward : 1.00000\n",
      "episode : 215, reward : 0.00000\n",
      "episode : 216, reward : 1.00000\n",
      "episode : 217, reward : 1.00000\n",
      "episode : 218, reward : 1.00000\n",
      "episode : 219, reward : 0.00000\n",
      "episode : 220, reward : 0.00000\n",
      "episode : 221, reward : 1.00000\n",
      "episode : 222, reward : 0.00000\n",
      "episode : 223, reward : 0.00000\n",
      "episode : 224, reward : 1.00000\n",
      "episode : 225, reward : 1.00000\n",
      "episode : 226, reward : 1.00000\n",
      "episode : 227, reward : 1.00000\n",
      "episode : 228, reward : 1.00000\n",
      "episode : 229, reward : 0.00000\n",
      "episode : 230, reward : 0.00000\n",
      "episode : 231, reward : 0.00000\n",
      "episode : 232, reward : 1.00000\n",
      "episode : 233, reward : 1.00000\n",
      "episode : 234, reward : 1.00000\n",
      "episode : 235, reward : 0.00000\n",
      "episode : 236, reward : 0.00000\n",
      "episode : 237, reward : 0.00000\n",
      "episode : 238, reward : 1.00000\n",
      "episode : 239, reward : 1.00000\n",
      "episode : 240, reward : 1.00000\n",
      "episode : 241, reward : 1.00000\n",
      "episode : 242, reward : 1.00000\n",
      "episode : 243, reward : 0.00000\n",
      "episode : 244, reward : 0.00000\n",
      "episode : 245, reward : 0.00000\n",
      "episode : 246, reward : 1.00000\n",
      "episode : 247, reward : 0.00000\n",
      "episode : 248, reward : 0.00000\n",
      "episode : 249, reward : 1.00000\n",
      "episode : 250, reward : 0.00000\n",
      "episode : 251, reward : 0.00000\n",
      "episode : 252, reward : 1.00000\n",
      "episode : 253, reward : 1.00000\n",
      "episode : 254, reward : 0.00000\n",
      "episode : 255, reward : 0.00000\n",
      "episode : 256, reward : 1.00000\n",
      "episode : 257, reward : 0.00000\n",
      "episode : 258, reward : 0.00000\n",
      "episode : 259, reward : 0.00000\n",
      "episode : 260, reward : 0.00000\n",
      "episode : 261, reward : 0.00000\n",
      "episode : 262, reward : 0.00000\n",
      "episode : 263, reward : 0.00000\n",
      "episode : 264, reward : 1.00000\n",
      "episode : 265, reward : 0.00000\n",
      "episode : 266, reward : 0.00000\n",
      "episode : 267, reward : 1.00000\n",
      "episode : 268, reward : 0.00000\n",
      "episode : 269, reward : 0.00000\n",
      "episode : 270, reward : 1.00000\n",
      "episode : 271, reward : 0.00000\n",
      "episode : 272, reward : 0.00000\n",
      "episode : 273, reward : 0.00000\n",
      "episode : 274, reward : 1.00000\n",
      "episode : 275, reward : 0.00000\n",
      "episode : 276, reward : 1.00000\n",
      "episode : 277, reward : 1.00000\n",
      "episode : 278, reward : 0.00000\n",
      "episode : 279, reward : 0.00000\n",
      "episode : 280, reward : 1.00000\n",
      "episode : 281, reward : 0.00000\n",
      "episode : 282, reward : 1.00000\n",
      "episode : 283, reward : 0.00000\n",
      "episode : 284, reward : 0.00000\n",
      "episode : 285, reward : 0.00000\n",
      "episode : 286, reward : 0.00000\n",
      "episode : 287, reward : 0.00000\n",
      "episode : 288, reward : 1.00000\n",
      "episode : 289, reward : 0.00000\n",
      "episode : 290, reward : 0.00000\n",
      "episode : 291, reward : 0.00000\n",
      "episode : 292, reward : 1.00000\n",
      "episode : 293, reward : 1.00000\n",
      "episode : 294, reward : 0.00000\n",
      "episode : 295, reward : 1.00000\n",
      "episode : 296, reward : 0.00000\n",
      "episode : 297, reward : 0.00000\n",
      "episode : 298, reward : 0.00000\n",
      "episode : 299, reward : 0.00000\n",
      "episode : 300, reward : 1.00000\n",
      "episode : 301, reward : 0.00000\n",
      "episode : 302, reward : 0.00000\n",
      "episode : 303, reward : 0.00000\n",
      "episode : 304, reward : 1.00000\n",
      "episode : 305, reward : 0.00000\n",
      "episode : 306, reward : 0.00000\n",
      "episode : 307, reward : 0.00000\n",
      "episode : 308, reward : 1.00000\n",
      "episode : 309, reward : 0.00000\n",
      "episode : 310, reward : 0.00000\n",
      "episode : 311, reward : 1.00000\n",
      "episode : 312, reward : 0.00000\n",
      "episode : 313, reward : 0.00000\n",
      "episode : 314, reward : 1.00000\n",
      "episode : 315, reward : 0.00000\n",
      "episode : 316, reward : 0.00000\n",
      "episode : 317, reward : 1.00000\n",
      "episode : 318, reward : 0.00000\n",
      "episode : 319, reward : 1.00000\n",
      "episode : 320, reward : 0.00000\n",
      "episode : 321, reward : 0.00000\n",
      "episode : 322, reward : 0.00000\n",
      "episode : 323, reward : 0.00000\n",
      "episode : 324, reward : 0.00000\n",
      "episode : 325, reward : 0.00000\n",
      "episode : 326, reward : 0.00000\n",
      "episode : 327, reward : 0.00000\n",
      "episode : 328, reward : 1.00000\n",
      "episode : 329, reward : 1.00000\n",
      "episode : 330, reward : 1.00000\n",
      "episode : 331, reward : 0.00000\n",
      "episode : 332, reward : 0.00000\n",
      "episode : 333, reward : 1.00000\n",
      "episode : 334, reward : 1.00000\n",
      "episode : 335, reward : 1.00000\n",
      "episode : 336, reward : 0.00000\n",
      "episode : 337, reward : 1.00000\n",
      "episode : 338, reward : 0.00000\n",
      "episode : 339, reward : 0.00000\n",
      "episode : 340, reward : 0.00000\n",
      "episode : 341, reward : 0.00000\n",
      "episode : 342, reward : 0.00000\n",
      "episode : 343, reward : 1.00000\n",
      "episode : 344, reward : 1.00000\n",
      "episode : 345, reward : 1.00000\n",
      "episode : 346, reward : 1.00000\n",
      "episode : 347, reward : 0.00000\n",
      "episode : 348, reward : 1.00000\n",
      "episode : 349, reward : 1.00000\n",
      "episode : 350, reward : 0.00000\n",
      "episode : 351, reward : 0.00000\n",
      "episode : 352, reward : 0.00000\n",
      "episode : 353, reward : 1.00000\n",
      "episode : 354, reward : 1.00000\n",
      "episode : 355, reward : 1.00000\n",
      "episode : 356, reward : 1.00000\n",
      "episode : 357, reward : 1.00000\n",
      "episode : 358, reward : 1.00000\n",
      "episode : 359, reward : 1.00000\n",
      "episode : 360, reward : 0.00000\n",
      "episode : 361, reward : 0.00000\n",
      "episode : 362, reward : 1.00000\n",
      "episode : 363, reward : 1.00000\n",
      "episode : 364, reward : 0.00000\n",
      "episode : 365, reward : 1.00000\n",
      "episode : 366, reward : 1.00000\n",
      "episode : 367, reward : 1.00000\n",
      "episode : 368, reward : 0.00000\n",
      "episode : 369, reward : 0.00000\n",
      "episode : 370, reward : 1.00000\n",
      "episode : 371, reward : 0.00000\n",
      "episode : 372, reward : 0.00000\n",
      "episode : 373, reward : 0.00000\n",
      "episode : 374, reward : 1.00000\n",
      "episode : 375, reward : 1.00000\n",
      "episode : 376, reward : 1.00000\n",
      "episode : 377, reward : 0.00000\n",
      "episode : 378, reward : 1.00000\n",
      "episode : 379, reward : 0.00000\n",
      "episode : 380, reward : 0.00000\n",
      "episode : 381, reward : 1.00000\n",
      "episode : 382, reward : 0.00000\n",
      "episode : 383, reward : 1.00000\n",
      "episode : 384, reward : 0.00000\n",
      "episode : 385, reward : 1.00000\n",
      "episode : 386, reward : 1.00000\n",
      "episode : 387, reward : 0.00000\n",
      "episode : 388, reward : 0.00000\n",
      "episode : 389, reward : 1.00000\n",
      "episode : 390, reward : 1.00000\n",
      "episode : 391, reward : 1.00000\n",
      "episode : 392, reward : 1.00000\n",
      "episode : 393, reward : 0.00000\n",
      "episode : 394, reward : 1.00000\n",
      "episode : 395, reward : 0.00000\n",
      "episode : 396, reward : 0.00000\n",
      "episode : 397, reward : 0.00000\n",
      "episode : 398, reward : 1.00000\n",
      "episode : 399, reward : 0.00000\n",
      "episode : 400, reward : 1.00000\n",
      "episode : 401, reward : 0.00000\n",
      "episode : 402, reward : 1.00000\n",
      "episode : 403, reward : 1.00000\n",
      "episode : 404, reward : 1.00000\n",
      "episode : 405, reward : 0.00000\n",
      "episode : 406, reward : 1.00000\n",
      "episode : 407, reward : 0.00000\n",
      "episode : 408, reward : 0.00000\n",
      "episode : 409, reward : 1.00000\n",
      "episode : 410, reward : 0.00000\n",
      "episode : 411, reward : 1.00000\n",
      "episode : 412, reward : 1.00000\n",
      "episode : 413, reward : 0.00000\n",
      "episode : 414, reward : 0.00000\n",
      "episode : 415, reward : 0.00000\n",
      "episode : 416, reward : 0.00000\n",
      "episode : 417, reward : 1.00000\n",
      "episode : 418, reward : 0.00000\n",
      "episode : 419, reward : 0.00000\n",
      "episode : 420, reward : 0.00000\n",
      "episode : 421, reward : 1.00000\n",
      "episode : 422, reward : 0.00000\n",
      "episode : 423, reward : 0.00000\n",
      "episode : 424, reward : 1.00000\n",
      "episode : 425, reward : 0.00000\n",
      "episode : 426, reward : 1.00000\n",
      "episode : 427, reward : 0.00000\n",
      "episode : 428, reward : 0.00000\n",
      "episode : 429, reward : 1.00000\n",
      "episode : 430, reward : 1.00000\n",
      "episode : 431, reward : 1.00000\n",
      "episode : 432, reward : 1.00000\n",
      "episode : 433, reward : 0.00000\n",
      "episode : 434, reward : 1.00000\n",
      "episode : 435, reward : 0.00000\n",
      "episode : 436, reward : 1.00000\n",
      "episode : 437, reward : 1.00000\n",
      "episode : 438, reward : 0.00000\n",
      "episode : 439, reward : 0.00000\n",
      "episode : 440, reward : 0.00000\n",
      "episode : 441, reward : 1.00000\n",
      "episode : 442, reward : 1.00000\n",
      "episode : 443, reward : 0.00000\n",
      "episode : 444, reward : 1.00000\n",
      "episode : 445, reward : 1.00000\n",
      "episode : 446, reward : 0.00000\n",
      "episode : 447, reward : 1.00000\n",
      "episode : 448, reward : 1.00000\n",
      "episode : 449, reward : 0.00000\n",
      "episode : 450, reward : 1.00000\n",
      "episode : 451, reward : 0.00000\n",
      "episode : 452, reward : 1.00000\n",
      "episode : 453, reward : 0.00000\n",
      "episode : 454, reward : 0.00000\n",
      "episode : 455, reward : 1.00000\n",
      "episode : 456, reward : 1.00000\n",
      "episode : 457, reward : 1.00000\n",
      "episode : 458, reward : 0.00000\n",
      "episode : 459, reward : 1.00000\n",
      "episode : 460, reward : 1.00000\n",
      "episode : 461, reward : 0.00000\n",
      "episode : 462, reward : 1.00000\n",
      "episode : 463, reward : 1.00000\n",
      "episode : 464, reward : 0.00000\n",
      "episode : 465, reward : 1.00000\n",
      "episode : 466, reward : 0.00000\n",
      "episode : 467, reward : 1.00000\n",
      "episode : 468, reward : 0.00000\n",
      "episode : 469, reward : 0.00000\n",
      "episode : 470, reward : 1.00000\n",
      "episode : 471, reward : 0.00000\n",
      "episode : 472, reward : 1.00000\n",
      "episode : 473, reward : 0.00000\n",
      "episode : 474, reward : 1.00000\n",
      "episode : 475, reward : 0.00000\n",
      "episode : 476, reward : 0.00000\n",
      "episode : 477, reward : 1.00000\n",
      "episode : 478, reward : 0.00000\n",
      "episode : 479, reward : 1.00000\n",
      "episode : 480, reward : 1.00000\n",
      "episode : 481, reward : 1.00000\n",
      "episode : 482, reward : 1.00000\n",
      "episode : 483, reward : 0.00000\n",
      "episode : 484, reward : 1.00000\n",
      "episode : 485, reward : 0.00000\n",
      "episode : 486, reward : 0.00000\n",
      "episode : 487, reward : 1.00000\n",
      "episode : 488, reward : 1.00000\n",
      "episode : 489, reward : 1.00000\n",
      "episode : 490, reward : 1.00000\n",
      "episode : 491, reward : 0.00000\n",
      "episode : 492, reward : 0.00000\n",
      "episode : 493, reward : 0.00000\n",
      "episode : 494, reward : 0.00000\n",
      "episode : 495, reward : 1.00000\n",
      "episode : 496, reward : 1.00000\n",
      "episode : 497, reward : 1.00000\n",
      "episode : 498, reward : 1.00000\n",
      "episode : 499, reward : 1.00000\n",
      "episode : 500, reward : 1.00000\n",
      "episode : 501, reward : 1.00000\n",
      "episode : 502, reward : 0.00000\n",
      "episode : 503, reward : 1.00000\n",
      "episode : 504, reward : 1.00000\n",
      "episode : 505, reward : 0.00000\n",
      "episode : 506, reward : 0.00000\n",
      "episode : 507, reward : 0.00000\n",
      "episode : 508, reward : 1.00000\n",
      "episode : 509, reward : 0.00000\n",
      "episode : 510, reward : 1.00000\n",
      "episode : 511, reward : 1.00000\n",
      "episode : 512, reward : 0.00000\n",
      "episode : 513, reward : 0.00000\n",
      "episode : 514, reward : 1.00000\n",
      "episode : 515, reward : 0.00000\n",
      "episode : 516, reward : 0.00000\n",
      "episode : 517, reward : 1.00000\n",
      "episode : 518, reward : 1.00000\n",
      "episode : 519, reward : 0.00000\n",
      "episode : 520, reward : 0.00000\n",
      "episode : 521, reward : 1.00000\n",
      "episode : 522, reward : 1.00000\n",
      "episode : 523, reward : 0.00000\n",
      "episode : 524, reward : 0.00000\n",
      "episode : 525, reward : 0.00000\n",
      "episode : 526, reward : 0.00000\n",
      "episode : 527, reward : 0.00000\n",
      "episode : 528, reward : 0.00000\n",
      "episode : 529, reward : 0.00000\n",
      "episode : 530, reward : 1.00000\n",
      "episode : 531, reward : 1.00000\n",
      "episode : 532, reward : 1.00000\n",
      "episode : 533, reward : 0.00000\n",
      "episode : 534, reward : 1.00000\n",
      "episode : 535, reward : 1.00000\n",
      "episode : 536, reward : 1.00000\n",
      "episode : 537, reward : 1.00000\n",
      "episode : 538, reward : 1.00000\n",
      "episode : 539, reward : 0.00000\n",
      "episode : 540, reward : 1.00000\n",
      "episode : 541, reward : 0.00000\n",
      "episode : 542, reward : 0.00000\n",
      "episode : 543, reward : 1.00000\n",
      "episode : 544, reward : 1.00000\n",
      "episode : 545, reward : 0.00000\n",
      "episode : 546, reward : 1.00000\n",
      "episode : 547, reward : 1.00000\n",
      "episode : 548, reward : 1.00000\n",
      "episode : 549, reward : 1.00000\n",
      "episode : 550, reward : 1.00000\n",
      "episode : 551, reward : 0.00000\n",
      "episode : 552, reward : 0.00000\n",
      "episode : 553, reward : 0.00000\n",
      "episode : 554, reward : 0.00000\n",
      "episode : 555, reward : 0.00000\n",
      "episode : 556, reward : 0.00000\n",
      "episode : 557, reward : 1.00000\n",
      "episode : 558, reward : 1.00000\n",
      "episode : 559, reward : 0.00000\n",
      "episode : 560, reward : 0.00000\n",
      "episode : 561, reward : 1.00000\n",
      "episode : 562, reward : 1.00000\n",
      "episode : 563, reward : 0.00000\n",
      "episode : 564, reward : 0.00000\n",
      "episode : 565, reward : 1.00000\n",
      "episode : 566, reward : 1.00000\n",
      "episode : 567, reward : 0.00000\n",
      "episode : 568, reward : 0.00000\n",
      "episode : 569, reward : 1.00000\n",
      "episode : 570, reward : 1.00000\n",
      "episode : 571, reward : 0.00000\n",
      "episode : 572, reward : 1.00000\n",
      "episode : 573, reward : 0.00000\n",
      "episode : 574, reward : 1.00000\n",
      "episode : 575, reward : 0.00000\n",
      "episode : 576, reward : 1.00000\n",
      "episode : 577, reward : 1.00000\n",
      "episode : 578, reward : 0.00000\n",
      "episode : 579, reward : 0.00000\n",
      "episode : 580, reward : 0.00000\n",
      "episode : 581, reward : 0.00000\n",
      "episode : 582, reward : 1.00000\n",
      "episode : 583, reward : 0.00000\n",
      "episode : 584, reward : 0.00000\n",
      "episode : 585, reward : 0.00000\n",
      "episode : 586, reward : 0.00000\n",
      "episode : 587, reward : 0.00000\n",
      "episode : 588, reward : 1.00000\n",
      "episode : 589, reward : 1.00000\n",
      "episode : 590, reward : 1.00000\n",
      "episode : 591, reward : 1.00000\n",
      "episode : 592, reward : 1.00000\n",
      "episode : 593, reward : 1.00000\n",
      "episode : 594, reward : 1.00000\n",
      "episode : 595, reward : 0.00000\n",
      "episode : 596, reward : 1.00000\n",
      "episode : 597, reward : 1.00000\n",
      "episode : 598, reward : 0.00000\n",
      "episode : 599, reward : 1.00000\n",
      "episode : 600, reward : 1.00000\n",
      "episode : 601, reward : 0.00000\n",
      "episode : 602, reward : 0.00000\n",
      "episode : 603, reward : 0.00000\n",
      "episode : 604, reward : 1.00000\n",
      "episode : 605, reward : 1.00000\n",
      "episode : 606, reward : 0.00000\n",
      "episode : 607, reward : 0.00000\n",
      "episode : 608, reward : 1.00000\n",
      "episode : 609, reward : 0.00000\n",
      "episode : 610, reward : 0.00000\n",
      "episode : 611, reward : 0.00000\n",
      "episode : 612, reward : 0.00000\n",
      "episode : 613, reward : 1.00000\n",
      "episode : 614, reward : 1.00000\n",
      "episode : 615, reward : 0.00000\n",
      "episode : 616, reward : 1.00000\n",
      "episode : 617, reward : 1.00000\n",
      "episode : 618, reward : 1.00000\n",
      "episode : 619, reward : 0.00000\n",
      "episode : 620, reward : 1.00000\n",
      "episode : 621, reward : 0.00000\n",
      "episode : 622, reward : 0.00000\n",
      "episode : 623, reward : 1.00000\n",
      "episode : 624, reward : 1.00000\n",
      "episode : 625, reward : 0.00000\n",
      "episode : 626, reward : 0.00000\n",
      "episode : 627, reward : 0.00000\n",
      "episode : 628, reward : 1.00000\n",
      "episode : 629, reward : 0.00000\n",
      "episode : 630, reward : 1.00000\n",
      "episode : 631, reward : 1.00000\n",
      "episode : 632, reward : 0.00000\n",
      "episode : 633, reward : 1.00000\n",
      "episode : 634, reward : 1.00000\n",
      "episode : 635, reward : 0.00000\n",
      "episode : 636, reward : 1.00000\n",
      "episode : 637, reward : 0.00000\n",
      "episode : 638, reward : 0.00000\n",
      "episode : 639, reward : 0.00000\n",
      "episode : 640, reward : 0.00000\n",
      "episode : 641, reward : 1.00000\n",
      "episode : 642, reward : 1.00000\n",
      "episode : 643, reward : 1.00000\n",
      "episode : 644, reward : 0.00000\n",
      "episode : 645, reward : 1.00000\n",
      "episode : 646, reward : 0.00000\n",
      "episode : 647, reward : 1.00000\n",
      "episode : 648, reward : 0.00000\n",
      "episode : 649, reward : 0.00000\n",
      "episode : 650, reward : 0.00000\n",
      "episode : 651, reward : 0.00000\n",
      "episode : 652, reward : 0.00000\n",
      "episode : 653, reward : 0.00000\n",
      "episode : 654, reward : 1.00000\n",
      "episode : 655, reward : 0.00000\n",
      "episode : 656, reward : 1.00000\n",
      "episode : 657, reward : 1.00000\n",
      "episode : 658, reward : 0.00000\n",
      "episode : 659, reward : 0.00000\n",
      "episode : 660, reward : 0.00000\n",
      "episode : 661, reward : 0.00000\n",
      "episode : 662, reward : 0.00000\n",
      "episode : 663, reward : 1.00000\n",
      "episode : 664, reward : 0.00000\n",
      "episode : 665, reward : 1.00000\n",
      "episode : 666, reward : 1.00000\n",
      "episode : 667, reward : 0.00000\n",
      "episode : 668, reward : 0.00000\n",
      "episode : 669, reward : 0.00000\n",
      "episode : 670, reward : 1.00000\n",
      "episode : 671, reward : 0.00000\n",
      "episode : 672, reward : 0.00000\n",
      "episode : 673, reward : 1.00000\n",
      "episode : 674, reward : 0.00000\n",
      "episode : 675, reward : 1.00000\n",
      "episode : 676, reward : 0.00000\n",
      "episode : 677, reward : 1.00000\n",
      "episode : 678, reward : 1.00000\n",
      "episode : 679, reward : 1.00000\n",
      "episode : 680, reward : 1.00000\n",
      "episode : 681, reward : 1.00000\n",
      "episode : 682, reward : 0.00000\n",
      "episode : 683, reward : 0.00000\n",
      "episode : 684, reward : 0.00000\n",
      "episode : 685, reward : 0.00000\n",
      "episode : 686, reward : 0.00000\n",
      "episode : 687, reward : 1.00000\n",
      "episode : 688, reward : 0.00000\n",
      "episode : 689, reward : 1.00000\n",
      "episode : 690, reward : 1.00000\n",
      "episode : 691, reward : 0.00000\n",
      "episode : 692, reward : 1.00000\n",
      "episode : 693, reward : 1.00000\n",
      "episode : 694, reward : 0.00000\n",
      "episode : 695, reward : 0.00000\n",
      "episode : 696, reward : 0.00000\n",
      "episode : 697, reward : 1.00000\n",
      "episode : 698, reward : 0.00000\n",
      "episode : 699, reward : 0.00000\n",
      "episode : 700, reward : 0.00000\n",
      "episode : 701, reward : 1.00000\n",
      "episode : 702, reward : 0.00000\n",
      "episode : 703, reward : 1.00000\n",
      "episode : 704, reward : 1.00000\n",
      "episode : 705, reward : 0.00000\n",
      "episode : 706, reward : 1.00000\n",
      "episode : 707, reward : 1.00000\n",
      "episode : 708, reward : 1.00000\n",
      "episode : 709, reward : 0.00000\n",
      "episode : 710, reward : 0.00000\n",
      "episode : 711, reward : 0.00000\n",
      "episode : 712, reward : 0.00000\n",
      "episode : 713, reward : 0.00000\n",
      "episode : 714, reward : 0.00000\n",
      "episode : 715, reward : 1.00000\n",
      "episode : 716, reward : 1.00000\n",
      "episode : 717, reward : 0.00000\n",
      "episode : 718, reward : 1.00000\n",
      "episode : 719, reward : 1.00000\n",
      "episode : 720, reward : 0.00000\n",
      "episode : 721, reward : 0.00000\n",
      "episode : 722, reward : 0.00000\n",
      "episode : 723, reward : 0.00000\n",
      "episode : 724, reward : 0.00000\n",
      "episode : 725, reward : 1.00000\n",
      "episode : 726, reward : 1.00000\n",
      "episode : 727, reward : 1.00000\n",
      "episode : 728, reward : 0.00000\n",
      "episode : 729, reward : 0.00000\n",
      "episode : 730, reward : 0.00000\n",
      "episode : 731, reward : 0.00000\n",
      "episode : 732, reward : 1.00000\n",
      "episode : 733, reward : 1.00000\n",
      "episode : 734, reward : 1.00000\n",
      "episode : 735, reward : 0.00000\n",
      "episode : 736, reward : 1.00000\n",
      "episode : 737, reward : 1.00000\n",
      "episode : 738, reward : 1.00000\n",
      "episode : 739, reward : 1.00000\n",
      "episode : 740, reward : 1.00000\n",
      "episode : 741, reward : 0.00000\n",
      "episode : 742, reward : 1.00000\n",
      "episode : 743, reward : 0.00000\n",
      "episode : 744, reward : 1.00000\n",
      "episode : 745, reward : 1.00000\n",
      "episode : 746, reward : 1.00000\n",
      "episode : 747, reward : 1.00000\n",
      "episode : 748, reward : 1.00000\n",
      "episode : 749, reward : 0.00000\n",
      "episode : 750, reward : 1.00000\n",
      "episode : 751, reward : 1.00000\n",
      "episode : 752, reward : 1.00000\n",
      "episode : 753, reward : 0.00000\n",
      "episode : 754, reward : 1.00000\n",
      "episode : 755, reward : 0.00000\n",
      "episode : 756, reward : 1.00000\n",
      "episode : 757, reward : 0.00000\n",
      "episode : 758, reward : 1.00000\n",
      "episode : 759, reward : 0.00000\n",
      "episode : 760, reward : 1.00000\n",
      "episode : 761, reward : 0.00000\n",
      "episode : 762, reward : 0.00000\n",
      "episode : 763, reward : 1.00000\n",
      "episode : 764, reward : 1.00000\n",
      "episode : 765, reward : 0.00000\n",
      "episode : 766, reward : 1.00000\n",
      "episode : 767, reward : 1.00000\n",
      "episode : 768, reward : 1.00000\n",
      "episode : 769, reward : 1.00000\n",
      "episode : 770, reward : 1.00000\n",
      "episode : 771, reward : 1.00000\n",
      "episode : 772, reward : 0.00000\n",
      "episode : 773, reward : 0.00000\n",
      "episode : 774, reward : 0.00000\n",
      "episode : 775, reward : 1.00000\n",
      "episode : 776, reward : 1.00000\n",
      "episode : 777, reward : 1.00000\n",
      "episode : 778, reward : 0.00000\n",
      "episode : 779, reward : 0.00000\n",
      "episode : 780, reward : 1.00000\n",
      "episode : 781, reward : 1.00000\n",
      "episode : 782, reward : 0.00000\n",
      "episode : 783, reward : 0.00000\n",
      "episode : 784, reward : 1.00000\n",
      "episode : 785, reward : 0.00000\n",
      "episode : 786, reward : 1.00000\n",
      "episode : 787, reward : 1.00000\n",
      "episode : 788, reward : 1.00000\n",
      "episode : 789, reward : 1.00000\n",
      "episode : 790, reward : 1.00000\n",
      "episode : 791, reward : 1.00000\n",
      "episode : 792, reward : 1.00000\n",
      "episode : 793, reward : 0.00000\n",
      "episode : 794, reward : 1.00000\n",
      "episode : 795, reward : 1.00000\n",
      "episode : 796, reward : 0.00000\n",
      "episode : 797, reward : 1.00000\n",
      "episode : 798, reward : 1.00000\n",
      "episode : 799, reward : 1.00000\n",
      "episode : 800, reward : 1.00000\n",
      "episode : 801, reward : 0.00000\n",
      "episode : 802, reward : 1.00000\n",
      "episode : 803, reward : 1.00000\n",
      "episode : 804, reward : 1.00000\n",
      "episode : 805, reward : 0.00000\n",
      "episode : 806, reward : 0.00000\n",
      "episode : 807, reward : 1.00000\n",
      "episode : 808, reward : 0.00000\n",
      "episode : 809, reward : 0.00000\n",
      "episode : 810, reward : 0.00000\n",
      "episode : 811, reward : 1.00000\n",
      "episode : 812, reward : 1.00000\n",
      "episode : 813, reward : 0.00000\n",
      "episode : 814, reward : 0.00000\n",
      "episode : 815, reward : 0.00000\n",
      "episode : 816, reward : 0.00000\n",
      "episode : 817, reward : 0.00000\n",
      "episode : 818, reward : 0.00000\n",
      "episode : 819, reward : 0.00000\n",
      "episode : 820, reward : 0.00000\n",
      "episode : 821, reward : 0.00000\n",
      "episode : 822, reward : 1.00000\n",
      "episode : 823, reward : 1.00000\n",
      "episode : 824, reward : 1.00000\n",
      "episode : 825, reward : 0.00000\n",
      "episode : 826, reward : 0.00000\n",
      "episode : 827, reward : 1.00000\n",
      "episode : 828, reward : 0.00000\n",
      "episode : 829, reward : 1.00000\n",
      "episode : 830, reward : 0.00000\n",
      "episode : 831, reward : 1.00000\n",
      "episode : 832, reward : 0.00000\n",
      "episode : 833, reward : 1.00000\n",
      "episode : 834, reward : 1.00000\n",
      "episode : 835, reward : 0.00000\n",
      "episode : 836, reward : 0.00000\n",
      "episode : 837, reward : 1.00000\n",
      "episode : 838, reward : 0.00000\n",
      "episode : 839, reward : 0.00000\n",
      "episode : 840, reward : 0.00000\n",
      "episode : 841, reward : 1.00000\n",
      "episode : 842, reward : 0.00000\n",
      "episode : 843, reward : 1.00000\n",
      "episode : 844, reward : 0.00000\n",
      "episode : 845, reward : 1.00000\n",
      "episode : 846, reward : 1.00000\n",
      "episode : 847, reward : 0.00000\n",
      "episode : 848, reward : 1.00000\n",
      "episode : 849, reward : 0.00000\n",
      "episode : 850, reward : 0.00000\n",
      "episode : 851, reward : 0.00000\n",
      "episode : 852, reward : 0.00000\n",
      "episode : 853, reward : 1.00000\n",
      "episode : 854, reward : 1.00000\n",
      "episode : 855, reward : 1.00000\n",
      "episode : 856, reward : 1.00000\n",
      "episode : 857, reward : 0.00000\n",
      "episode : 858, reward : 0.00000\n",
      "episode : 859, reward : 0.00000\n",
      "episode : 860, reward : 0.00000\n",
      "episode : 861, reward : 1.00000\n",
      "episode : 862, reward : 1.00000\n",
      "episode : 863, reward : 0.00000\n",
      "episode : 864, reward : 1.00000\n",
      "episode : 865, reward : 1.00000\n",
      "episode : 866, reward : 0.00000\n",
      "episode : 867, reward : 1.00000\n",
      "episode : 868, reward : 1.00000\n",
      "episode : 869, reward : 0.00000\n",
      "episode : 870, reward : 1.00000\n",
      "episode : 871, reward : 1.00000\n",
      "episode : 872, reward : 0.00000\n",
      "episode : 873, reward : 1.00000\n",
      "episode : 874, reward : 1.00000\n",
      "episode : 875, reward : 1.00000\n",
      "episode : 876, reward : 0.00000\n",
      "episode : 877, reward : 0.00000\n",
      "episode : 878, reward : 1.00000\n",
      "episode : 879, reward : 0.00000\n",
      "episode : 880, reward : 1.00000\n",
      "episode : 881, reward : 0.00000\n",
      "episode : 882, reward : 0.00000\n",
      "episode : 883, reward : 0.00000\n",
      "episode : 884, reward : 1.00000\n",
      "episode : 885, reward : 0.00000\n",
      "episode : 886, reward : 1.00000\n",
      "episode : 887, reward : 0.00000\n",
      "episode : 888, reward : 0.00000\n",
      "episode : 889, reward : 1.00000\n",
      "episode : 890, reward : 0.00000\n",
      "episode : 891, reward : 1.00000\n",
      "episode : 892, reward : 0.00000\n",
      "episode : 893, reward : 1.00000\n",
      "episode : 894, reward : 1.00000\n",
      "episode : 895, reward : 1.00000\n",
      "episode : 896, reward : 0.00000\n",
      "episode : 897, reward : 1.00000\n",
      "episode : 898, reward : 0.00000\n",
      "episode : 899, reward : 0.00000\n",
      "episode : 900, reward : 1.00000\n",
      "episode : 901, reward : 1.00000\n",
      "episode : 902, reward : 1.00000\n",
      "episode : 903, reward : 1.00000\n",
      "episode : 904, reward : 1.00000\n",
      "episode : 905, reward : 0.00000\n",
      "episode : 906, reward : 0.00000\n",
      "episode : 907, reward : 0.00000\n",
      "episode : 908, reward : 1.00000\n",
      "episode : 909, reward : 0.00000\n",
      "episode : 910, reward : 1.00000\n",
      "episode : 911, reward : 1.00000\n",
      "episode : 912, reward : 1.00000\n",
      "episode : 913, reward : 0.00000\n",
      "episode : 914, reward : 1.00000\n",
      "episode : 915, reward : 1.00000\n",
      "episode : 916, reward : 1.00000\n",
      "episode : 917, reward : 0.00000\n",
      "episode : 918, reward : 1.00000\n",
      "episode : 919, reward : 0.00000\n",
      "episode : 920, reward : 0.00000\n",
      "episode : 921, reward : 0.00000\n",
      "episode : 922, reward : 1.00000\n",
      "episode : 923, reward : 1.00000\n",
      "episode : 924, reward : 0.00000\n",
      "episode : 925, reward : 0.00000\n",
      "episode : 926, reward : 0.00000\n",
      "episode : 927, reward : 1.00000\n",
      "episode : 928, reward : 0.00000\n",
      "episode : 929, reward : 1.00000\n",
      "episode : 930, reward : 0.00000\n",
      "episode : 931, reward : 1.00000\n",
      "episode : 932, reward : 0.00000\n",
      "episode : 933, reward : 0.00000\n",
      "episode : 934, reward : 0.00000\n",
      "episode : 935, reward : 0.00000\n",
      "episode : 936, reward : 0.00000\n",
      "episode : 937, reward : 1.00000\n",
      "episode : 938, reward : 1.00000\n",
      "episode : 939, reward : 0.00000\n",
      "episode : 940, reward : 1.00000\n",
      "episode : 941, reward : 1.00000\n",
      "episode : 942, reward : 1.00000\n",
      "episode : 943, reward : 1.00000\n",
      "episode : 944, reward : 0.00000\n",
      "episode : 945, reward : 1.00000\n",
      "episode : 946, reward : 0.00000\n",
      "episode : 947, reward : 1.00000\n",
      "episode : 948, reward : 0.00000\n",
      "episode : 949, reward : 1.00000\n",
      "episode : 950, reward : 0.00000\n",
      "episode : 951, reward : 0.00000\n",
      "episode : 952, reward : 0.00000\n",
      "episode : 953, reward : 1.00000\n",
      "episode : 954, reward : 0.00000\n",
      "episode : 955, reward : 0.00000\n",
      "episode : 956, reward : 0.00000\n",
      "episode : 957, reward : 0.00000\n",
      "episode : 958, reward : 0.00000\n",
      "episode : 959, reward : 0.00000\n",
      "episode : 960, reward : 0.00000\n",
      "episode : 961, reward : 0.00000\n",
      "episode : 962, reward : 1.00000\n",
      "episode : 963, reward : 1.00000\n",
      "episode : 964, reward : 0.00000\n",
      "episode : 965, reward : 0.00000\n",
      "episode : 966, reward : 1.00000\n",
      "episode : 967, reward : 0.00000\n",
      "episode : 968, reward : 1.00000\n",
      "episode : 969, reward : 1.00000\n",
      "episode : 970, reward : 1.00000\n",
      "episode : 971, reward : 0.00000\n",
      "episode : 972, reward : 0.00000\n",
      "episode : 973, reward : 1.00000\n",
      "episode : 974, reward : 1.00000\n",
      "episode : 975, reward : 1.00000\n",
      "episode : 976, reward : 1.00000\n",
      "episode : 977, reward : 1.00000\n",
      "episode : 978, reward : 0.00000\n",
      "episode : 979, reward : 0.00000\n",
      "episode : 980, reward : 1.00000\n",
      "episode : 981, reward : 1.00000\n",
      "episode : 982, reward : 0.00000\n",
      "episode : 983, reward : 1.00000\n",
      "episode : 984, reward : 0.00000\n",
      "episode : 985, reward : 1.00000\n",
      "episode : 986, reward : 1.00000\n",
      "episode : 987, reward : 1.00000\n",
      "episode : 988, reward : 0.00000\n",
      "episode : 989, reward : 1.00000\n",
      "episode : 990, reward : 1.00000\n",
      "episode : 991, reward : 1.00000\n",
      "episode : 992, reward : 1.00000\n",
      "episode : 993, reward : 1.00000\n",
      "episode : 994, reward : 1.00000\n",
      "episode : 995, reward : 1.00000\n",
      "episode : 996, reward : 1.00000\n",
      "episode : 997, reward : 1.00000\n",
      "episode : 998, reward : 0.00000\n",
      "episode : 999, reward : 1.00000\n",
      "episode : 1000, reward : 0.00000\n",
      "episode : 1001, reward : 1.00000\n",
      "episode : 1002, reward : 0.00000\n",
      "episode : 1003, reward : 0.00000\n",
      "episode : 1004, reward : 1.00000\n",
      "episode : 1005, reward : 0.00000\n",
      "episode : 1006, reward : 1.00000\n",
      "episode : 1007, reward : 1.00000\n",
      "episode : 1008, reward : 0.00000\n",
      "episode : 1009, reward : 0.00000\n",
      "episode : 1010, reward : 0.00000\n",
      "episode : 1011, reward : 1.00000\n",
      "episode : 1012, reward : 0.00000\n",
      "episode : 1013, reward : 0.00000\n",
      "episode : 1014, reward : 1.00000\n",
      "episode : 1015, reward : 1.00000\n",
      "episode : 1016, reward : 0.00000\n",
      "episode : 1017, reward : 1.00000\n",
      "episode : 1018, reward : 1.00000\n",
      "episode : 1019, reward : 0.00000\n",
      "episode : 1020, reward : 1.00000\n",
      "episode : 1021, reward : 0.00000\n",
      "episode : 1022, reward : 0.00000\n",
      "episode : 1023, reward : 1.00000\n",
      "episode : 1024, reward : 1.00000\n",
      "episode : 1025, reward : 1.00000\n",
      "episode : 1026, reward : 1.00000\n",
      "episode : 1027, reward : 0.00000\n",
      "episode : 1028, reward : 1.00000\n",
      "episode : 1029, reward : 1.00000\n",
      "episode : 1030, reward : 1.00000\n",
      "episode : 1031, reward : 1.00000\n",
      "episode : 1032, reward : 1.00000\n",
      "episode : 1033, reward : 0.00000\n",
      "episode : 1034, reward : 0.00000\n",
      "episode : 1035, reward : 1.00000\n",
      "episode : 1036, reward : 1.00000\n",
      "episode : 1037, reward : 1.00000\n",
      "episode : 1038, reward : 0.00000\n",
      "episode : 1039, reward : 1.00000\n",
      "episode : 1040, reward : 1.00000\n",
      "episode : 1041, reward : 1.00000\n",
      "episode : 1042, reward : 0.00000\n",
      "episode : 1043, reward : 0.00000\n",
      "episode : 1044, reward : 1.00000\n",
      "episode : 1045, reward : 1.00000\n",
      "episode : 1046, reward : 0.00000\n",
      "episode : 1047, reward : 0.00000\n",
      "episode : 1048, reward : 0.00000\n",
      "episode : 1049, reward : 1.00000\n",
      "episode : 1050, reward : 1.00000\n",
      "episode : 1051, reward : 1.00000\n",
      "episode : 1052, reward : 0.00000\n",
      "episode : 1053, reward : 1.00000\n",
      "episode : 1054, reward : 1.00000\n",
      "episode : 1055, reward : 0.00000\n",
      "episode : 1056, reward : 0.00000\n",
      "episode : 1057, reward : 0.00000\n",
      "episode : 1058, reward : 0.00000\n",
      "episode : 1059, reward : 1.00000\n",
      "episode : 1060, reward : 0.00000\n",
      "episode : 1061, reward : 0.00000\n",
      "episode : 1062, reward : 0.00000\n",
      "episode : 1063, reward : 1.00000\n",
      "episode : 1064, reward : 1.00000\n",
      "episode : 1065, reward : 0.00000\n",
      "episode : 1066, reward : 0.00000\n",
      "episode : 1067, reward : 0.00000\n",
      "episode : 1068, reward : 0.00000\n",
      "episode : 1069, reward : 1.00000\n",
      "episode : 1070, reward : 0.00000\n",
      "episode : 1071, reward : 1.00000\n",
      "episode : 1072, reward : 1.00000\n",
      "episode : 1073, reward : 0.00000\n",
      "episode : 1074, reward : 0.00000\n",
      "episode : 1075, reward : 1.00000\n",
      "episode : 1076, reward : 0.00000\n",
      "episode : 1077, reward : 1.00000\n",
      "episode : 1078, reward : 0.00000\n",
      "episode : 1079, reward : 0.00000\n",
      "episode : 1080, reward : 1.00000\n",
      "episode : 1081, reward : 0.00000\n",
      "episode : 1082, reward : 0.00000\n",
      "episode : 1083, reward : 0.00000\n",
      "episode : 1084, reward : 1.00000\n",
      "episode : 1085, reward : 1.00000\n",
      "episode : 1086, reward : 0.00000\n",
      "episode : 1087, reward : 0.00000\n",
      "episode : 1088, reward : 0.00000\n",
      "episode : 1089, reward : 1.00000\n",
      "episode : 1090, reward : 1.00000\n",
      "episode : 1091, reward : 0.00000\n",
      "episode : 1092, reward : 0.00000\n",
      "episode : 1093, reward : 1.00000\n",
      "episode : 1094, reward : 0.00000\n",
      "episode : 1095, reward : 1.00000\n",
      "episode : 1096, reward : 1.00000\n",
      "episode : 1097, reward : 0.00000\n",
      "episode : 1098, reward : 0.00000\n",
      "episode : 1099, reward : 1.00000\n",
      "episode : 1100, reward : 0.00000\n",
      "episode : 1101, reward : 0.00000\n",
      "episode : 1102, reward : 1.00000\n",
      "episode : 1103, reward : 1.00000\n",
      "episode : 1104, reward : 0.00000\n",
      "episode : 1105, reward : 0.00000\n",
      "episode : 1106, reward : 0.00000\n",
      "episode : 1107, reward : 0.00000\n",
      "episode : 1108, reward : 0.00000\n",
      "episode : 1109, reward : 1.00000\n",
      "episode : 1110, reward : 1.00000\n",
      "episode : 1111, reward : 1.00000\n",
      "episode : 1112, reward : 1.00000\n",
      "episode : 1113, reward : 1.00000\n",
      "episode : 1114, reward : 0.00000\n",
      "episode : 1115, reward : 1.00000\n",
      "episode : 1116, reward : 1.00000\n",
      "episode : 1117, reward : 1.00000\n",
      "episode : 1118, reward : 0.00000\n",
      "episode : 1119, reward : 1.00000\n",
      "episode : 1120, reward : 1.00000\n",
      "episode : 1121, reward : 1.00000\n",
      "episode : 1122, reward : 0.00000\n",
      "episode : 1123, reward : 1.00000\n",
      "episode : 1124, reward : 1.00000\n",
      "episode : 1125, reward : 0.00000\n",
      "episode : 1126, reward : 1.00000\n",
      "episode : 1127, reward : 0.00000\n",
      "episode : 1128, reward : 1.00000\n",
      "episode : 1129, reward : 1.00000\n",
      "episode : 1130, reward : 1.00000\n",
      "episode : 1131, reward : 0.00000\n",
      "episode : 1132, reward : 0.00000\n",
      "episode : 1133, reward : 0.00000\n",
      "episode : 1134, reward : 0.00000\n",
      "episode : 1135, reward : 0.00000\n",
      "episode : 1136, reward : 0.00000\n",
      "episode : 1137, reward : 0.00000\n",
      "episode : 1138, reward : 1.00000\n",
      "episode : 1139, reward : 1.00000\n",
      "episode : 1140, reward : 0.00000\n",
      "episode : 1141, reward : 1.00000\n",
      "episode : 1142, reward : 1.00000\n",
      "episode : 1143, reward : 1.00000\n",
      "episode : 1144, reward : 0.00000\n",
      "episode : 1145, reward : 0.00000\n",
      "episode : 1146, reward : 0.00000\n",
      "episode : 1147, reward : 1.00000\n",
      "episode : 1148, reward : 1.00000\n",
      "episode : 1149, reward : 0.00000\n",
      "episode : 1150, reward : 0.00000\n",
      "episode : 1151, reward : 1.00000\n",
      "episode : 1152, reward : 1.00000\n",
      "episode : 1153, reward : 0.00000\n",
      "episode : 1154, reward : 1.00000\n",
      "episode : 1155, reward : 1.00000\n",
      "episode : 1156, reward : 1.00000\n",
      "episode : 1157, reward : 1.00000\n",
      "episode : 1158, reward : 1.00000\n",
      "episode : 1159, reward : 0.00000\n",
      "episode : 1160, reward : 1.00000\n",
      "episode : 1161, reward : 0.00000\n",
      "episode : 1162, reward : 1.00000\n",
      "episode : 1163, reward : 0.00000\n",
      "episode : 1164, reward : 1.00000\n",
      "episode : 1165, reward : 1.00000\n",
      "episode : 1166, reward : 0.00000\n",
      "episode : 1167, reward : 0.00000\n",
      "episode : 1168, reward : 1.00000\n",
      "episode : 1169, reward : 1.00000\n",
      "episode : 1170, reward : 0.00000\n",
      "episode : 1171, reward : 1.00000\n",
      "episode : 1172, reward : 0.00000\n",
      "episode : 1173, reward : 1.00000\n",
      "episode : 1174, reward : 0.00000\n",
      "episode : 1175, reward : 1.00000\n",
      "episode : 1176, reward : 1.00000\n",
      "episode : 1177, reward : 0.00000\n",
      "episode : 1178, reward : 0.00000\n",
      "episode : 1179, reward : 0.00000\n",
      "episode : 1180, reward : 1.00000\n",
      "episode : 1181, reward : 1.00000\n",
      "episode : 1182, reward : 0.00000\n",
      "episode : 1183, reward : 0.00000\n",
      "episode : 1184, reward : 0.00000\n",
      "episode : 1185, reward : 0.00000\n",
      "episode : 1186, reward : 1.00000\n",
      "episode : 1187, reward : 0.00000\n",
      "episode : 1188, reward : 0.00000\n",
      "episode : 1189, reward : 0.00000\n",
      "episode : 1190, reward : 1.00000\n",
      "episode : 1191, reward : 0.00000\n",
      "episode : 1192, reward : 1.00000\n",
      "episode : 1193, reward : 1.00000\n",
      "episode : 1194, reward : 0.00000\n",
      "episode : 1195, reward : 0.00000\n",
      "episode : 1196, reward : 1.00000\n",
      "episode : 1197, reward : 1.00000\n",
      "episode : 1198, reward : 0.00000\n",
      "episode : 1199, reward : 0.00000\n",
      "episode : 1200, reward : 1.00000\n",
      "episode : 1201, reward : 1.00000\n",
      "episode : 1202, reward : 0.00000\n",
      "episode : 1203, reward : 0.00000\n",
      "episode : 1204, reward : 1.00000\n",
      "episode : 1205, reward : 1.00000\n",
      "episode : 1206, reward : 1.00000\n",
      "episode : 1207, reward : 1.00000\n",
      "episode : 1208, reward : 0.00000\n",
      "episode : 1209, reward : 0.00000\n",
      "episode : 1210, reward : 0.00000\n",
      "episode : 1211, reward : 0.00000\n",
      "episode : 1212, reward : 1.00000\n",
      "episode : 1213, reward : 1.00000\n",
      "episode : 1214, reward : 1.00000\n",
      "episode : 1215, reward : 1.00000\n",
      "episode : 1216, reward : 0.00000\n",
      "episode : 1217, reward : 1.00000\n",
      "episode : 1218, reward : 1.00000\n",
      "episode : 1219, reward : 0.00000\n",
      "episode : 1220, reward : 1.00000\n",
      "episode : 1221, reward : 1.00000\n",
      "episode : 1222, reward : 0.00000\n",
      "episode : 1223, reward : 1.00000\n",
      "episode : 1224, reward : 1.00000\n",
      "episode : 1225, reward : 0.00000\n",
      "episode : 1226, reward : 1.00000\n",
      "episode : 1227, reward : 1.00000\n",
      "episode : 1228, reward : 1.00000\n",
      "episode : 1229, reward : 0.00000\n",
      "episode : 1230, reward : 1.00000\n",
      "episode : 1231, reward : 0.00000\n",
      "episode : 1232, reward : 1.00000\n",
      "episode : 1233, reward : 1.00000\n",
      "episode : 1234, reward : 1.00000\n",
      "episode : 1235, reward : 1.00000\n",
      "episode : 1236, reward : 1.00000\n",
      "episode : 1237, reward : 0.00000\n",
      "episode : 1238, reward : 0.00000\n",
      "episode : 1239, reward : 0.00000\n",
      "episode : 1240, reward : 1.00000\n",
      "episode : 1241, reward : 1.00000\n",
      "episode : 1242, reward : 0.00000\n",
      "episode : 1243, reward : 0.00000\n",
      "episode : 1244, reward : 1.00000\n",
      "episode : 1245, reward : 1.00000\n",
      "episode : 1246, reward : 1.00000\n",
      "episode : 1247, reward : 1.00000\n",
      "episode : 1248, reward : 1.00000\n",
      "episode : 1249, reward : 0.00000\n",
      "episode : 1250, reward : 1.00000\n",
      "episode : 1251, reward : 1.00000\n",
      "episode : 1252, reward : 0.00000\n",
      "episode : 1253, reward : 1.00000\n",
      "episode : 1254, reward : 0.00000\n",
      "episode : 1255, reward : 0.00000\n",
      "episode : 1256, reward : 1.00000\n",
      "episode : 1257, reward : 1.00000\n",
      "episode : 1258, reward : 1.00000\n",
      "episode : 1259, reward : 0.00000\n",
      "episode : 1260, reward : 0.00000\n",
      "episode : 1261, reward : 1.00000\n",
      "episode : 1262, reward : 0.00000\n",
      "episode : 1263, reward : 0.00000\n",
      "episode : 1264, reward : 1.00000\n",
      "episode : 1265, reward : 1.00000\n",
      "episode : 1266, reward : 1.00000\n",
      "episode : 1267, reward : 1.00000\n",
      "episode : 1268, reward : 1.00000\n",
      "episode : 1269, reward : 1.00000\n",
      "episode : 1270, reward : 1.00000\n",
      "episode : 1271, reward : 1.00000\n",
      "episode : 1272, reward : 1.00000\n",
      "episode : 1273, reward : 0.00000\n",
      "episode : 1274, reward : 0.00000\n",
      "episode : 1275, reward : 1.00000\n",
      "episode : 1276, reward : 1.00000\n",
      "episode : 1277, reward : 0.00000\n",
      "episode : 1278, reward : 1.00000\n",
      "episode : 1279, reward : 0.00000\n",
      "episode : 1280, reward : 1.00000\n",
      "episode : 1281, reward : 1.00000\n",
      "episode : 1282, reward : 0.00000\n",
      "episode : 1283, reward : 1.00000\n",
      "episode : 1284, reward : 1.00000\n",
      "episode : 1285, reward : 0.00000\n",
      "episode : 1286, reward : 1.00000\n",
      "episode : 1287, reward : 1.00000\n",
      "episode : 1288, reward : 1.00000\n",
      "episode : 1289, reward : 1.00000\n",
      "episode : 1290, reward : 1.00000\n",
      "episode : 1291, reward : 1.00000\n",
      "episode : 1292, reward : 1.00000\n",
      "episode : 1293, reward : 1.00000\n",
      "episode : 1294, reward : 1.00000\n",
      "episode : 1295, reward : 0.00000\n",
      "episode : 1296, reward : 1.00000\n",
      "episode : 1297, reward : 1.00000\n",
      "episode : 1298, reward : 1.00000\n",
      "episode : 1299, reward : 1.00000\n",
      "episode : 1300, reward : 1.00000\n",
      "episode : 1301, reward : 1.00000\n",
      "episode : 1302, reward : 1.00000\n",
      "episode : 1303, reward : 1.00000\n",
      "episode : 1304, reward : 0.00000\n",
      "episode : 1305, reward : 1.00000\n",
      "episode : 1306, reward : 1.00000\n",
      "episode : 1307, reward : 1.00000\n",
      "episode : 1308, reward : 0.00000\n",
      "episode : 1309, reward : 1.00000\n",
      "episode : 1310, reward : 1.00000\n",
      "episode : 1311, reward : 1.00000\n",
      "episode : 1312, reward : 1.00000\n",
      "episode : 1313, reward : 1.00000\n",
      "episode : 1314, reward : 0.00000\n",
      "episode : 1315, reward : 1.00000\n",
      "episode : 1316, reward : 1.00000\n",
      "episode : 1317, reward : 1.00000\n",
      "episode : 1318, reward : 1.00000\n",
      "episode : 1319, reward : 0.00000\n",
      "episode : 1320, reward : 0.00000\n",
      "episode : 1321, reward : 1.00000\n",
      "episode : 1322, reward : 0.00000\n",
      "episode : 1323, reward : 1.00000\n",
      "episode : 1324, reward : 1.00000\n",
      "episode : 1325, reward : 0.00000\n",
      "episode : 1326, reward : 0.00000\n",
      "episode : 1327, reward : 1.00000\n",
      "episode : 1328, reward : 0.00000\n",
      "episode : 1329, reward : 1.00000\n",
      "episode : 1330, reward : 1.00000\n",
      "episode : 1331, reward : 1.00000\n",
      "episode : 1332, reward : 0.00000\n",
      "episode : 1333, reward : 0.00000\n",
      "episode : 1334, reward : 1.00000\n",
      "episode : 1335, reward : 1.00000\n",
      "episode : 1336, reward : 1.00000\n",
      "episode : 1337, reward : 1.00000\n",
      "episode : 1338, reward : 1.00000\n",
      "episode : 1339, reward : 1.00000\n",
      "episode : 1340, reward : 0.00000\n",
      "episode : 1341, reward : 1.00000\n",
      "episode : 1342, reward : 1.00000\n",
      "episode : 1343, reward : 1.00000\n",
      "episode : 1344, reward : 1.00000\n",
      "episode : 1345, reward : 0.00000\n",
      "episode : 1346, reward : 1.00000\n",
      "episode : 1347, reward : 1.00000\n",
      "episode : 1348, reward : 0.00000\n",
      "episode : 1349, reward : 0.00000\n",
      "episode : 1350, reward : 0.00000\n",
      "episode : 1351, reward : 0.00000\n",
      "episode : 1352, reward : 1.00000\n",
      "episode : 1353, reward : 0.00000\n",
      "episode : 1354, reward : 1.00000\n",
      "episode : 1355, reward : 1.00000\n",
      "episode : 1356, reward : 0.00000\n",
      "episode : 1357, reward : 1.00000\n",
      "episode : 1358, reward : 1.00000\n",
      "episode : 1359, reward : 1.00000\n",
      "episode : 1360, reward : 0.00000\n",
      "episode : 1361, reward : 1.00000\n",
      "episode : 1362, reward : 1.00000\n",
      "episode : 1363, reward : 1.00000\n",
      "episode : 1364, reward : 0.00000\n",
      "episode : 1365, reward : 0.00000\n",
      "episode : 1366, reward : 0.00000\n",
      "episode : 1367, reward : 1.00000\n",
      "episode : 1368, reward : 0.00000\n",
      "episode : 1369, reward : 0.00000\n",
      "episode : 1370, reward : 0.00000\n",
      "episode : 1371, reward : 1.00000\n",
      "episode : 1372, reward : 0.00000\n",
      "episode : 1373, reward : 1.00000\n",
      "episode : 1374, reward : 1.00000\n",
      "episode : 1375, reward : 1.00000\n",
      "episode : 1376, reward : 0.00000\n",
      "episode : 1377, reward : 0.00000\n",
      "episode : 1378, reward : 1.00000\n",
      "episode : 1379, reward : 1.00000\n",
      "episode : 1380, reward : 1.00000\n",
      "episode : 1381, reward : 1.00000\n",
      "episode : 1382, reward : 1.00000\n",
      "episode : 1383, reward : 1.00000\n",
      "episode : 1384, reward : 1.00000\n",
      "episode : 1385, reward : 1.00000\n",
      "episode : 1386, reward : 1.00000\n",
      "episode : 1387, reward : 0.00000\n",
      "episode : 1388, reward : 0.00000\n",
      "episode : 1389, reward : 0.00000\n",
      "episode : 1390, reward : 1.00000\n",
      "episode : 1391, reward : 0.00000\n",
      "episode : 1392, reward : 1.00000\n",
      "episode : 1393, reward : 1.00000\n",
      "episode : 1394, reward : 0.00000\n",
      "episode : 1395, reward : 0.00000\n",
      "episode : 1396, reward : 0.00000\n",
      "episode : 1397, reward : 1.00000\n",
      "episode : 1398, reward : 0.00000\n",
      "episode : 1399, reward : 1.00000\n",
      "episode : 1400, reward : 1.00000\n",
      "episode : 1401, reward : 1.00000\n",
      "episode : 1402, reward : 1.00000\n",
      "episode : 1403, reward : 0.00000\n",
      "episode : 1404, reward : 0.00000\n",
      "episode : 1405, reward : 0.00000\n",
      "episode : 1406, reward : 1.00000\n",
      "episode : 1407, reward : 0.00000\n",
      "episode : 1408, reward : 1.00000\n",
      "episode : 1409, reward : 1.00000\n",
      "episode : 1410, reward : 1.00000\n",
      "episode : 1411, reward : 0.00000\n",
      "episode : 1412, reward : 1.00000\n",
      "episode : 1413, reward : 0.00000\n",
      "episode : 1414, reward : 1.00000\n",
      "episode : 1415, reward : 1.00000\n",
      "episode : 1416, reward : 1.00000\n",
      "episode : 1417, reward : 1.00000\n",
      "episode : 1418, reward : 1.00000\n",
      "episode : 1419, reward : 1.00000\n",
      "episode : 1420, reward : 1.00000\n",
      "episode : 1421, reward : 0.00000\n",
      "episode : 1422, reward : 1.00000\n",
      "episode : 1423, reward : 1.00000\n",
      "episode : 1424, reward : 1.00000\n",
      "episode : 1425, reward : 1.00000\n",
      "episode : 1426, reward : 1.00000\n",
      "episode : 1427, reward : 0.00000\n",
      "episode : 1428, reward : 0.00000\n",
      "episode : 1429, reward : 1.00000\n",
      "episode : 1430, reward : 1.00000\n",
      "episode : 1431, reward : 0.00000\n",
      "episode : 1432, reward : 0.00000\n",
      "episode : 1433, reward : 1.00000\n",
      "episode : 1434, reward : 1.00000\n",
      "episode : 1435, reward : 1.00000\n",
      "episode : 1436, reward : 0.00000\n",
      "episode : 1437, reward : 1.00000\n",
      "episode : 1438, reward : 1.00000\n",
      "episode : 1439, reward : 1.00000\n",
      "episode : 1440, reward : 0.00000\n",
      "episode : 1441, reward : 1.00000\n",
      "episode : 1442, reward : 0.00000\n",
      "episode : 1443, reward : 0.00000\n",
      "episode : 1444, reward : 1.00000\n",
      "episode : 1445, reward : 1.00000\n",
      "episode : 1446, reward : 1.00000\n",
      "episode : 1447, reward : 0.00000\n",
      "episode : 1448, reward : 1.00000\n",
      "episode : 1449, reward : 0.00000\n",
      "episode : 1450, reward : 1.00000\n",
      "episode : 1451, reward : 1.00000\n",
      "episode : 1452, reward : 0.00000\n",
      "episode : 1453, reward : 1.00000\n",
      "episode : 1454, reward : 1.00000\n",
      "episode : 1455, reward : 1.00000\n",
      "episode : 1456, reward : 1.00000\n",
      "episode : 1457, reward : 0.00000\n",
      "episode : 1458, reward : 1.00000\n",
      "episode : 1459, reward : 1.00000\n",
      "episode : 1460, reward : 1.00000\n",
      "episode : 1461, reward : 0.00000\n",
      "episode : 1462, reward : 1.00000\n",
      "episode : 1463, reward : 1.00000\n",
      "episode : 1464, reward : 0.00000\n",
      "episode : 1465, reward : 1.00000\n",
      "episode : 1466, reward : 1.00000\n",
      "episode : 1467, reward : 1.00000\n",
      "episode : 1468, reward : 0.00000\n",
      "episode : 1469, reward : 1.00000\n",
      "episode : 1470, reward : 1.00000\n",
      "episode : 1471, reward : 0.00000\n",
      "episode : 1472, reward : 1.00000\n",
      "episode : 1473, reward : 0.00000\n",
      "episode : 1474, reward : 1.00000\n",
      "episode : 1475, reward : 1.00000\n",
      "episode : 1476, reward : 1.00000\n",
      "episode : 1477, reward : 1.00000\n",
      "episode : 1478, reward : 1.00000\n",
      "episode : 1479, reward : 1.00000\n",
      "episode : 1480, reward : 1.00000\n",
      "episode : 1481, reward : 1.00000\n",
      "episode : 1482, reward : 1.00000\n",
      "episode : 1483, reward : 1.00000\n",
      "episode : 1484, reward : 0.00000\n",
      "episode : 1485, reward : 1.00000\n",
      "episode : 1486, reward : 1.00000\n",
      "episode : 1487, reward : 0.00000\n",
      "episode : 1488, reward : 0.00000\n",
      "episode : 1489, reward : 1.00000\n",
      "episode : 1490, reward : 1.00000\n",
      "episode : 1491, reward : 0.00000\n",
      "episode : 1492, reward : 1.00000\n",
      "episode : 1493, reward : 1.00000\n",
      "episode : 1494, reward : 1.00000\n",
      "episode : 1495, reward : 1.00000\n",
      "episode : 1496, reward : 1.00000\n",
      "episode : 1497, reward : 0.00000\n",
      "episode : 1498, reward : 1.00000\n",
      "episode : 1499, reward : 1.00000\n",
      "episode : 1500, reward : 1.00000\n",
      "episode : 1501, reward : 1.00000\n",
      "episode : 1502, reward : 1.00000\n",
      "episode : 1503, reward : 1.00000\n",
      "episode : 1504, reward : 0.00000\n",
      "episode : 1505, reward : 1.00000\n",
      "episode : 1506, reward : 1.00000\n",
      "episode : 1507, reward : 0.00000\n",
      "episode : 1508, reward : 1.00000\n",
      "episode : 1509, reward : 1.00000\n",
      "episode : 1510, reward : 1.00000\n",
      "episode : 1511, reward : 0.00000\n",
      "episode : 1512, reward : 1.00000\n",
      "episode : 1513, reward : 0.00000\n",
      "episode : 1514, reward : 0.00000\n",
      "episode : 1515, reward : 0.00000\n",
      "episode : 1516, reward : 0.00000\n",
      "episode : 1517, reward : 1.00000\n",
      "episode : 1518, reward : 1.00000\n",
      "episode : 1519, reward : 0.00000\n",
      "episode : 1520, reward : 0.00000\n",
      "episode : 1521, reward : 1.00000\n",
      "episode : 1522, reward : 1.00000\n",
      "episode : 1523, reward : 1.00000\n",
      "episode : 1524, reward : 0.00000\n",
      "episode : 1525, reward : 1.00000\n",
      "episode : 1526, reward : 0.00000\n",
      "episode : 1527, reward : 1.00000\n",
      "episode : 1528, reward : 1.00000\n",
      "episode : 1529, reward : 1.00000\n",
      "episode : 1530, reward : 1.00000\n",
      "episode : 1531, reward : 0.00000\n",
      "episode : 1532, reward : 1.00000\n",
      "episode : 1533, reward : 1.00000\n",
      "episode : 1534, reward : 0.00000\n",
      "episode : 1535, reward : 1.00000\n",
      "episode : 1536, reward : 1.00000\n",
      "episode : 1537, reward : 1.00000\n",
      "episode : 1538, reward : 1.00000\n",
      "episode : 1539, reward : 0.00000\n",
      "episode : 1540, reward : 1.00000\n",
      "episode : 1541, reward : 1.00000\n",
      "episode : 1542, reward : 1.00000\n",
      "episode : 1543, reward : 1.00000\n",
      "episode : 1544, reward : 1.00000\n",
      "episode : 1545, reward : 0.00000\n",
      "episode : 1546, reward : 1.00000\n",
      "episode : 1547, reward : 0.00000\n",
      "episode : 1548, reward : 1.00000\n",
      "episode : 1549, reward : 0.00000\n",
      "episode : 1550, reward : 0.00000\n",
      "episode : 1551, reward : 0.00000\n",
      "episode : 1552, reward : 0.00000\n",
      "episode : 1553, reward : 0.00000\n",
      "episode : 1554, reward : 1.00000\n",
      "episode : 1555, reward : 0.00000\n",
      "episode : 1556, reward : 0.00000\n",
      "episode : 1557, reward : 0.00000\n",
      "episode : 1558, reward : 0.00000\n",
      "episode : 1559, reward : 1.00000\n",
      "episode : 1560, reward : 0.00000\n",
      "episode : 1561, reward : 1.00000\n",
      "episode : 1562, reward : 1.00000\n",
      "episode : 1563, reward : 1.00000\n",
      "episode : 1564, reward : 0.00000\n",
      "episode : 1565, reward : 0.00000\n",
      "episode : 1566, reward : 1.00000\n",
      "episode : 1567, reward : 1.00000\n",
      "episode : 1568, reward : 1.00000\n",
      "episode : 1569, reward : 1.00000\n",
      "episode : 1570, reward : 0.00000\n",
      "episode : 1571, reward : 1.00000\n",
      "episode : 1572, reward : 0.00000\n",
      "episode : 1573, reward : 0.00000\n",
      "episode : 1574, reward : 0.00000\n",
      "episode : 1575, reward : 1.00000\n",
      "episode : 1576, reward : 1.00000\n",
      "episode : 1577, reward : 1.00000\n",
      "episode : 1578, reward : 0.00000\n",
      "episode : 1579, reward : 1.00000\n",
      "episode : 1580, reward : 0.00000\n",
      "episode : 1581, reward : 0.00000\n",
      "episode : 1582, reward : 1.00000\n",
      "episode : 1583, reward : 0.00000\n",
      "episode : 1584, reward : 0.00000\n",
      "episode : 1585, reward : 1.00000\n",
      "episode : 1586, reward : 1.00000\n",
      "episode : 1587, reward : 0.00000\n",
      "episode : 1588, reward : 0.00000\n",
      "episode : 1589, reward : 1.00000\n",
      "episode : 1590, reward : 1.00000\n",
      "episode : 1591, reward : 1.00000\n",
      "episode : 1592, reward : 0.00000\n",
      "episode : 1593, reward : 0.00000\n",
      "episode : 1594, reward : 1.00000\n",
      "episode : 1595, reward : 1.00000\n",
      "episode : 1596, reward : 0.00000\n",
      "episode : 1597, reward : 1.00000\n",
      "episode : 1598, reward : 1.00000\n",
      "episode : 1599, reward : 0.00000\n",
      "episode : 1600, reward : 1.00000\n",
      "episode : 1601, reward : 1.00000\n",
      "episode : 1602, reward : 1.00000\n",
      "episode : 1603, reward : 1.00000\n",
      "episode : 1604, reward : 1.00000\n",
      "episode : 1605, reward : 1.00000\n",
      "episode : 1606, reward : 1.00000\n",
      "episode : 1607, reward : 0.00000\n",
      "episode : 1608, reward : 0.00000\n",
      "episode : 1609, reward : 1.00000\n",
      "episode : 1610, reward : 0.00000\n",
      "episode : 1611, reward : 1.00000\n",
      "episode : 1612, reward : 1.00000\n",
      "episode : 1613, reward : 0.00000\n",
      "episode : 1614, reward : 1.00000\n",
      "episode : 1615, reward : 0.00000\n",
      "episode : 1616, reward : 1.00000\n",
      "episode : 1617, reward : 0.00000\n",
      "episode : 1618, reward : 1.00000\n",
      "episode : 1619, reward : 1.00000\n",
      "episode : 1620, reward : 1.00000\n",
      "episode : 1621, reward : 1.00000\n",
      "episode : 1622, reward : 1.00000\n",
      "episode : 1623, reward : 0.00000\n",
      "episode : 1624, reward : 1.00000\n",
      "episode : 1625, reward : 1.00000\n",
      "episode : 1626, reward : 1.00000\n",
      "episode : 1627, reward : 1.00000\n",
      "episode : 1628, reward : 1.00000\n",
      "episode : 1629, reward : 1.00000\n",
      "episode : 1630, reward : 0.00000\n",
      "episode : 1631, reward : 1.00000\n",
      "episode : 1632, reward : 1.00000\n",
      "episode : 1633, reward : 1.00000\n",
      "episode : 1634, reward : 1.00000\n",
      "episode : 1635, reward : 1.00000\n",
      "episode : 1636, reward : 0.00000\n",
      "episode : 1637, reward : 0.00000\n",
      "episode : 1638, reward : 1.00000\n",
      "episode : 1639, reward : 1.00000\n",
      "episode : 1640, reward : 0.00000\n",
      "episode : 1641, reward : 1.00000\n",
      "episode : 1642, reward : 1.00000\n",
      "episode : 1643, reward : 0.00000\n",
      "episode : 1644, reward : 1.00000\n",
      "episode : 1645, reward : 0.00000\n",
      "episode : 1646, reward : 0.00000\n",
      "episode : 1647, reward : 0.00000\n",
      "episode : 1648, reward : 1.00000\n",
      "episode : 1649, reward : 1.00000\n",
      "episode : 1650, reward : 1.00000\n",
      "episode : 1651, reward : 1.00000\n",
      "episode : 1652, reward : 1.00000\n",
      "episode : 1653, reward : 1.00000\n",
      "episode : 1654, reward : 1.00000\n",
      "episode : 1655, reward : 1.00000\n",
      "episode : 1656, reward : 1.00000\n",
      "episode : 1657, reward : 0.00000\n",
      "episode : 1658, reward : 1.00000\n",
      "episode : 1659, reward : 0.00000\n",
      "episode : 1660, reward : 0.00000\n",
      "episode : 1661, reward : 1.00000\n",
      "episode : 1662, reward : 0.00000\n",
      "episode : 1663, reward : 0.00000\n",
      "episode : 1664, reward : 0.00000\n",
      "episode : 1665, reward : 1.00000\n",
      "episode : 1666, reward : 1.00000\n",
      "episode : 1667, reward : 1.00000\n",
      "episode : 1668, reward : 0.00000\n",
      "episode : 1669, reward : 0.00000\n",
      "episode : 1670, reward : 1.00000\n",
      "episode : 1671, reward : 1.00000\n",
      "episode : 1672, reward : 0.00000\n",
      "episode : 1673, reward : 1.00000\n",
      "episode : 1674, reward : 0.00000\n",
      "episode : 1675, reward : 0.00000\n",
      "episode : 1676, reward : 1.00000\n",
      "episode : 1677, reward : 0.00000\n",
      "episode : 1678, reward : 1.00000\n",
      "episode : 1679, reward : 1.00000\n",
      "episode : 1680, reward : 1.00000\n",
      "episode : 1681, reward : 0.00000\n",
      "episode : 1682, reward : 0.00000\n",
      "episode : 1683, reward : 0.00000\n",
      "episode : 1684, reward : 0.00000\n",
      "episode : 1685, reward : 0.00000\n",
      "episode : 1686, reward : 1.00000\n",
      "episode : 1687, reward : 0.00000\n",
      "episode : 1688, reward : 1.00000\n",
      "episode : 1689, reward : 1.00000\n",
      "episode : 1690, reward : 1.00000\n",
      "episode : 1691, reward : 1.00000\n",
      "episode : 1692, reward : 0.00000\n",
      "episode : 1693, reward : 1.00000\n",
      "episode : 1694, reward : 0.00000\n",
      "episode : 1695, reward : 0.00000\n",
      "episode : 1696, reward : 1.00000\n",
      "episode : 1697, reward : 1.00000\n",
      "episode : 1698, reward : 1.00000\n",
      "episode : 1699, reward : 1.00000\n",
      "episode : 1700, reward : 1.00000\n",
      "episode : 1701, reward : 1.00000\n",
      "episode : 1702, reward : 1.00000\n",
      "episode : 1703, reward : 1.00000\n",
      "episode : 1704, reward : 0.00000\n",
      "episode : 1705, reward : 1.00000\n",
      "episode : 1706, reward : 0.00000\n",
      "episode : 1707, reward : 1.00000\n",
      "episode : 1708, reward : 1.00000\n",
      "episode : 1709, reward : 1.00000\n",
      "episode : 1710, reward : 1.00000\n",
      "episode : 1711, reward : 1.00000\n",
      "episode : 1712, reward : 0.00000\n",
      "episode : 1713, reward : 1.00000\n",
      "episode : 1714, reward : 1.00000\n",
      "episode : 1715, reward : 1.00000\n",
      "episode : 1716, reward : 1.00000\n",
      "episode : 1717, reward : 1.00000\n",
      "episode : 1718, reward : 1.00000\n",
      "episode : 1719, reward : 1.00000\n",
      "episode : 1720, reward : 1.00000\n",
      "episode : 1721, reward : 1.00000\n",
      "episode : 1722, reward : 0.00000\n",
      "episode : 1723, reward : 1.00000\n",
      "episode : 1724, reward : 0.00000\n",
      "episode : 1725, reward : 0.00000\n",
      "episode : 1726, reward : 1.00000\n",
      "episode : 1727, reward : 0.00000\n",
      "episode : 1728, reward : 0.00000\n",
      "episode : 1729, reward : 1.00000\n",
      "episode : 1730, reward : 0.00000\n",
      "episode : 1731, reward : 1.00000\n",
      "episode : 1732, reward : 1.00000\n",
      "episode : 1733, reward : 1.00000\n",
      "episode : 1734, reward : 1.00000\n",
      "episode : 1735, reward : 1.00000\n",
      "episode : 1736, reward : 0.00000\n",
      "episode : 1737, reward : 1.00000\n",
      "episode : 1738, reward : 0.00000\n",
      "episode : 1739, reward : 1.00000\n",
      "episode : 1740, reward : 1.00000\n",
      "episode : 1741, reward : 1.00000\n",
      "episode : 1742, reward : 1.00000\n",
      "episode : 1743, reward : 0.00000\n",
      "episode : 1744, reward : 0.00000\n",
      "episode : 1745, reward : 0.00000\n",
      "episode : 1746, reward : 1.00000\n",
      "episode : 1747, reward : 1.00000\n",
      "episode : 1748, reward : 0.00000\n",
      "episode : 1749, reward : 1.00000\n",
      "episode : 1750, reward : 1.00000\n",
      "episode : 1751, reward : 1.00000\n",
      "episode : 1752, reward : 1.00000\n",
      "episode : 1753, reward : 0.00000\n",
      "episode : 1754, reward : 1.00000\n",
      "episode : 1755, reward : 1.00000\n",
      "episode : 1756, reward : 0.00000\n",
      "episode : 1757, reward : 0.00000\n",
      "episode : 1758, reward : 1.00000\n",
      "episode : 1759, reward : 0.00000\n",
      "episode : 1760, reward : 1.00000\n",
      "episode : 1761, reward : 0.00000\n",
      "episode : 1762, reward : 0.00000\n",
      "episode : 1763, reward : 0.00000\n",
      "episode : 1764, reward : 0.00000\n",
      "episode : 1765, reward : 0.00000\n",
      "episode : 1766, reward : 1.00000\n",
      "episode : 1767, reward : 1.00000\n",
      "episode : 1768, reward : 0.00000\n",
      "episode : 1769, reward : 0.00000\n",
      "episode : 1770, reward : 0.00000\n",
      "episode : 1771, reward : 1.00000\n",
      "episode : 1772, reward : 0.00000\n",
      "episode : 1773, reward : 0.00000\n",
      "episode : 1774, reward : 0.00000\n",
      "episode : 1775, reward : 1.00000\n",
      "episode : 1776, reward : 0.00000\n",
      "episode : 1777, reward : 1.00000\n",
      "episode : 1778, reward : 0.00000\n",
      "episode : 1779, reward : 0.00000\n",
      "episode : 1780, reward : 1.00000\n",
      "episode : 1781, reward : 0.00000\n",
      "episode : 1782, reward : 1.00000\n",
      "episode : 1783, reward : 1.00000\n",
      "episode : 1784, reward : 1.00000\n",
      "episode : 1785, reward : 1.00000\n",
      "episode : 1786, reward : 0.00000\n",
      "episode : 1787, reward : 1.00000\n",
      "episode : 1788, reward : 0.00000\n",
      "episode : 1789, reward : 1.00000\n",
      "episode : 1790, reward : 1.00000\n",
      "episode : 1791, reward : 0.00000\n",
      "episode : 1792, reward : 1.00000\n",
      "episode : 1793, reward : 1.00000\n",
      "episode : 1794, reward : 0.00000\n",
      "episode : 1795, reward : 1.00000\n",
      "episode : 1796, reward : 0.00000\n",
      "episode : 1797, reward : 1.00000\n",
      "episode : 1798, reward : 1.00000\n",
      "episode : 1799, reward : 0.00000\n",
      "episode : 1800, reward : 1.00000\n",
      "episode : 1801, reward : 0.00000\n",
      "episode : 1802, reward : 1.00000\n",
      "episode : 1803, reward : 1.00000\n",
      "episode : 1804, reward : 1.00000\n",
      "episode : 1805, reward : 1.00000\n",
      "episode : 1806, reward : 0.00000\n",
      "episode : 1807, reward : 1.00000\n",
      "episode : 1808, reward : 1.00000\n",
      "episode : 1809, reward : 0.00000\n",
      "episode : 1810, reward : 0.00000\n",
      "episode : 1811, reward : 1.00000\n",
      "episode : 1812, reward : 0.00000\n",
      "episode : 1813, reward : 0.00000\n",
      "episode : 1814, reward : 1.00000\n",
      "episode : 1815, reward : 1.00000\n",
      "episode : 1816, reward : 1.00000\n",
      "episode : 1817, reward : 0.00000\n",
      "episode : 1818, reward : 0.00000\n",
      "episode : 1819, reward : 1.00000\n",
      "episode : 1820, reward : 1.00000\n",
      "episode : 1821, reward : 0.00000\n",
      "episode : 1822, reward : 1.00000\n",
      "episode : 1823, reward : 1.00000\n",
      "episode : 1824, reward : 0.00000\n",
      "episode : 1825, reward : 0.00000\n",
      "episode : 1826, reward : 1.00000\n",
      "episode : 1827, reward : 1.00000\n",
      "episode : 1828, reward : 1.00000\n",
      "episode : 1829, reward : 1.00000\n",
      "episode : 1830, reward : 1.00000\n",
      "episode : 1831, reward : 1.00000\n",
      "episode : 1832, reward : 0.00000\n",
      "episode : 1833, reward : 0.00000\n",
      "episode : 1834, reward : 1.00000\n",
      "episode : 1835, reward : 0.00000\n",
      "episode : 1836, reward : 1.00000\n",
      "episode : 1837, reward : 0.00000\n",
      "episode : 1838, reward : 1.00000\n",
      "episode : 1839, reward : 0.00000\n",
      "episode : 1840, reward : 1.00000\n",
      "episode : 1841, reward : 1.00000\n",
      "episode : 1842, reward : 1.00000\n",
      "episode : 1843, reward : 1.00000\n",
      "episode : 1844, reward : 1.00000\n",
      "episode : 1845, reward : 1.00000\n",
      "episode : 1846, reward : 0.00000\n",
      "episode : 1847, reward : 0.00000\n",
      "episode : 1848, reward : 1.00000\n",
      "episode : 1849, reward : 1.00000\n",
      "episode : 1850, reward : 1.00000\n",
      "episode : 1851, reward : 1.00000\n",
      "episode : 1852, reward : 1.00000\n",
      "episode : 1853, reward : 1.00000\n",
      "episode : 1854, reward : 1.00000\n",
      "episode : 1855, reward : 1.00000\n",
      "episode : 1856, reward : 0.00000\n",
      "episode : 1857, reward : 1.00000\n",
      "episode : 1858, reward : 1.00000\n",
      "episode : 1859, reward : 1.00000\n",
      "episode : 1860, reward : 1.00000\n",
      "episode : 1861, reward : 1.00000\n",
      "episode : 1862, reward : 0.00000\n",
      "episode : 1863, reward : 0.00000\n",
      "episode : 1864, reward : 0.00000\n",
      "episode : 1865, reward : 0.00000\n",
      "episode : 1866, reward : 1.00000\n",
      "episode : 1867, reward : 0.00000\n",
      "episode : 1868, reward : 1.00000\n",
      "episode : 1869, reward : 1.00000\n",
      "episode : 1870, reward : 0.00000\n",
      "episode : 1871, reward : 1.00000\n",
      "episode : 1872, reward : 0.00000\n",
      "episode : 1873, reward : 1.00000\n",
      "episode : 1874, reward : 1.00000\n",
      "episode : 1875, reward : 1.00000\n",
      "episode : 1876, reward : 1.00000\n",
      "episode : 1877, reward : 0.00000\n",
      "episode : 1878, reward : 1.00000\n",
      "episode : 1879, reward : 1.00000\n",
      "episode : 1880, reward : 1.00000\n",
      "episode : 1881, reward : 1.00000\n",
      "episode : 1882, reward : 0.00000\n",
      "episode : 1883, reward : 0.00000\n",
      "episode : 1884, reward : 0.00000\n",
      "episode : 1885, reward : 1.00000\n",
      "episode : 1886, reward : 1.00000\n",
      "episode : 1887, reward : 1.00000\n",
      "episode : 1888, reward : 0.00000\n",
      "episode : 1889, reward : 0.00000\n",
      "episode : 1890, reward : 1.00000\n",
      "episode : 1891, reward : 1.00000\n",
      "episode : 1892, reward : 1.00000\n",
      "episode : 1893, reward : 1.00000\n",
      "episode : 1894, reward : 1.00000\n",
      "episode : 1895, reward : 1.00000\n",
      "episode : 1896, reward : 0.00000\n",
      "episode : 1897, reward : 1.00000\n",
      "episode : 1898, reward : 0.00000\n",
      "episode : 1899, reward : 1.00000\n",
      "episode : 1900, reward : 1.00000\n",
      "episode : 1901, reward : 1.00000\n",
      "episode : 1902, reward : 0.00000\n",
      "episode : 1903, reward : 1.00000\n",
      "episode : 1904, reward : 0.00000\n",
      "episode : 1905, reward : 1.00000\n",
      "episode : 1906, reward : 1.00000\n",
      "episode : 1907, reward : 0.00000\n",
      "episode : 1908, reward : 1.00000\n",
      "episode : 1909, reward : 1.00000\n",
      "episode : 1910, reward : 1.00000\n",
      "episode : 1911, reward : 1.00000\n",
      "episode : 1912, reward : 0.00000\n",
      "episode : 1913, reward : 1.00000\n",
      "episode : 1914, reward : 1.00000\n",
      "episode : 1915, reward : 0.00000\n",
      "episode : 1916, reward : 1.00000\n",
      "episode : 1917, reward : 0.00000\n",
      "episode : 1918, reward : 1.00000\n",
      "episode : 1919, reward : 1.00000\n",
      "episode : 1920, reward : 1.00000\n",
      "episode : 1921, reward : 1.00000\n",
      "episode : 1922, reward : 1.00000\n",
      "episode : 1923, reward : 1.00000\n",
      "episode : 1924, reward : 0.00000\n",
      "episode : 1925, reward : 1.00000\n",
      "episode : 1926, reward : 1.00000\n",
      "episode : 1927, reward : 0.00000\n",
      "episode : 1928, reward : 1.00000\n",
      "episode : 1929, reward : 0.00000\n",
      "episode : 1930, reward : 1.00000\n",
      "episode : 1931, reward : 1.00000\n",
      "episode : 1932, reward : 0.00000\n",
      "episode : 1933, reward : 1.00000\n",
      "episode : 1934, reward : 1.00000\n",
      "episode : 1935, reward : 1.00000\n",
      "episode : 1936, reward : 0.00000\n",
      "episode : 1937, reward : 0.00000\n",
      "episode : 1938, reward : 0.00000\n",
      "episode : 1939, reward : 1.00000\n",
      "episode : 1940, reward : 0.00000\n",
      "episode : 1941, reward : 1.00000\n",
      "episode : 1942, reward : 0.00000\n",
      "episode : 1943, reward : 1.00000\n",
      "episode : 1944, reward : 1.00000\n",
      "episode : 1945, reward : 1.00000\n",
      "episode : 1946, reward : 1.00000\n",
      "episode : 1947, reward : 0.00000\n",
      "episode : 1948, reward : 0.00000\n",
      "episode : 1949, reward : 1.00000\n",
      "episode : 1950, reward : 1.00000\n",
      "episode : 1951, reward : 0.00000\n",
      "episode : 1952, reward : 1.00000\n",
      "episode : 1953, reward : 1.00000\n",
      "episode : 1954, reward : 1.00000\n",
      "episode : 1955, reward : 1.00000\n",
      "episode : 1956, reward : 0.00000\n",
      "episode : 1957, reward : 1.00000\n",
      "episode : 1958, reward : 1.00000\n",
      "episode : 1959, reward : 1.00000\n",
      "episode : 1960, reward : 0.00000\n",
      "episode : 1961, reward : 1.00000\n",
      "episode : 1962, reward : 1.00000\n",
      "episode : 1963, reward : 1.00000\n",
      "episode : 1964, reward : 0.00000\n",
      "episode : 1965, reward : 1.00000\n",
      "episode : 1966, reward : 0.00000\n",
      "episode : 1967, reward : 1.00000\n",
      "episode : 1968, reward : 1.00000\n",
      "episode : 1969, reward : 1.00000\n",
      "episode : 1970, reward : 1.00000\n",
      "episode : 1971, reward : 1.00000\n",
      "episode : 1972, reward : 1.00000\n",
      "episode : 1973, reward : 1.00000\n",
      "episode : 1974, reward : 1.00000\n",
      "episode : 1975, reward : 1.00000\n",
      "episode : 1976, reward : 1.00000\n",
      "episode : 1977, reward : 0.00000\n",
      "episode : 1978, reward : 0.00000\n",
      "episode : 1979, reward : 1.00000\n",
      "episode : 1980, reward : 0.00000\n",
      "episode : 1981, reward : 1.00000\n",
      "episode : 1982, reward : 1.00000\n",
      "episode : 1983, reward : 0.00000\n",
      "episode : 1984, reward : 1.00000\n",
      "episode : 1985, reward : 1.00000\n",
      "episode : 1986, reward : 1.00000\n",
      "episode : 1987, reward : 0.00000\n",
      "episode : 1988, reward : 1.00000\n",
      "episode : 1989, reward : 0.00000\n",
      "episode : 1990, reward : 1.00000\n",
      "episode : 1991, reward : 1.00000\n",
      "episode : 1992, reward : 0.00000\n",
      "episode : 1993, reward : 1.00000\n",
      "episode : 1994, reward : 1.00000\n",
      "episode : 1995, reward : 1.00000\n",
      "episode : 1996, reward : 1.00000\n",
      "episode : 1997, reward : 1.00000\n",
      "episode : 1998, reward : 1.00000\n",
      "episode : 1999, reward : 1.00000\n",
      "acc : 53.2%\n",
      "In time : 64.96329832077026 s\n"
     ]
    }
   ],
   "source": [
    "def QLEARNING():\n",
    "    \"\"\" 20 mins \"\"\"\n",
    "    import time\n",
    "\n",
    "    import numpy as np\n",
    "    import tensorflow as tf\n",
    "    import gym\n",
    "\n",
    "    # HYPER PARAMS\n",
    "    env = gym.make(\"FrozenLake-v0\")\n",
    "    learning_rate = 0.4\n",
    "\n",
    "    input_size = env.observation_space.n # 16\n",
    "    output_size = env.action_space.n # 4\n",
    "    max_episode = 2000\n",
    "    dis = 0.99\n",
    "\n",
    "    # PARAMS\n",
    "    X = tf.placeholder(tf.float32, [1, input_size]) # 1x16\n",
    "    W = tf.Variable(tf.random_uniform([input_size, output_size], 0, 0.01)) # 16x4\n",
    "\n",
    "    Qpred = tf.matmul(X, W) # 1x4\n",
    "    Y = tf.placeholder(tf.float32, [1, output_size])\n",
    "\n",
    "    loss = tf.reduce_mean(tf.square(Y - Qpred))\n",
    "    optimizer = tf.train.GradientDescentOptimizer(learning_rate=learning_rate).minimize(loss)\n",
    "\n",
    "    rList = []\n",
    "\n",
    "    def one_hot(x, inputsize):\n",
    "        return np.identity(input_size)[x:x+1]\n",
    "\n",
    "    start = time.time()\n",
    "    with tf.Session() as sess:\n",
    "        sess.run(tf.global_variables_initializer())\n",
    "        for episode in range(max_episode):\n",
    "            # e-greedy\n",
    "            e = 1. / ((episode / 50.) + 10.)\n",
    "            s = env.reset() # 1\n",
    "            rewardAll = 0\n",
    "            done = False\n",
    "            step = 1\n",
    "            while not done:\n",
    "                Qs = sess.run(Qpred, feed_dict={X: one_hot(s, input_size)})\n",
    "                if np.random.rand(1) < e:\n",
    "                    action = env.action_space.sample()\n",
    "                else:\n",
    "                    action = np.argmax(Qs)\n",
    "\n",
    "                sn, reward, done, info = env.step(action)\n",
    "\n",
    "                if done:\n",
    "                    Qs[0, action] = reward\n",
    "                else:\n",
    "                    Qsn = sess.run(Qpred, feed_dict={X: one_hot(sn, input_size)})\n",
    "                    Qs[0, action] = reward + dis*np.max(Qsn)\n",
    "                sess.run(optimizer, feed_dict={X: one_hot(s, input_size), Y: Qs})\n",
    "                rewardAll += reward\n",
    "                s = sn\n",
    "                step += 1\n",
    "            print(\"episode : {:d}, reward : {:.5f}\".format(episode, rewardAll))\n",
    "            rList.append(rewardAll)\n",
    "        end = time.time()\n",
    "        print(\"acc : \" + str(sum(rList)/max_episode * 100) + \"%\")\n",
    "        print(\"In time : {} s\".format(end-start))\n",
    "    return None\n",
    "\n",
    "QLEARNING()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='Policy Based Agent tensorflow'></a>\n",
    "## Policy Based Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2017-03-15 22:18:10,838] Making new env: CartPole-v0\n",
      "/usr/local/lib/python3.5/dist-packages/tensorflow/python/ops/gradients_impl.py:91: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n"
     ]
    },
    {
     "ename": "InvalidArgumentError",
     "evalue": "You must feed a value for placeholder tensor 'agent/Placeholder' with dtype float\n\t [[Node: agent/Placeholder = Placeholder[dtype=DT_FLOAT, shape=[], _device=\"/job:localhost/replica:0/task:0/cpu:0\"]()]]\n\nCaused by op 'agent/Placeholder', defined at:\n  File \"/usr/lib/python3.5/runpy.py\", line 184, in _run_module_as_main\n    \"__main__\", mod_spec)\n  File \"/usr/lib/python3.5/runpy.py\", line 85, in _run_code\n    exec(code, run_globals)\n  File \"/usr/local/lib/python3.5/dist-packages/ipykernel/__main__.py\", line 3, in <module>\n    app.launch_new_instance()\n  File \"/usr/local/lib/python3.5/dist-packages/traitlets/config/application.py\", line 658, in launch_instance\n    app.start()\n  File \"/usr/local/lib/python3.5/dist-packages/ipykernel/kernelapp.py\", line 474, in start\n    ioloop.IOLoop.instance().start()\n  File \"/usr/local/lib/python3.5/dist-packages/zmq/eventloop/ioloop.py\", line 177, in start\n    super(ZMQIOLoop, self).start()\n  File \"/usr/local/lib/python3.5/dist-packages/tornado/ioloop.py\", line 887, in start\n    handler_func(fd_obj, events)\n  File \"/usr/local/lib/python3.5/dist-packages/tornado/stack_context.py\", line 275, in null_wrapper\n    return fn(*args, **kwargs)\n  File \"/usr/local/lib/python3.5/dist-packages/zmq/eventloop/zmqstream.py\", line 440, in _handle_events\n    self._handle_recv()\n  File \"/usr/local/lib/python3.5/dist-packages/zmq/eventloop/zmqstream.py\", line 472, in _handle_recv\n    self._run_callback(callback, msg)\n  File \"/usr/local/lib/python3.5/dist-packages/zmq/eventloop/zmqstream.py\", line 414, in _run_callback\n    callback(*args, **kwargs)\n  File \"/usr/local/lib/python3.5/dist-packages/tornado/stack_context.py\", line 275, in null_wrapper\n    return fn(*args, **kwargs)\n  File \"/usr/local/lib/python3.5/dist-packages/ipykernel/kernelbase.py\", line 276, in dispatcher\n    return self.dispatch_shell(stream, msg)\n  File \"/usr/local/lib/python3.5/dist-packages/ipykernel/kernelbase.py\", line 228, in dispatch_shell\n    handler(stream, idents, msg)\n  File \"/usr/local/lib/python3.5/dist-packages/ipykernel/kernelbase.py\", line 390, in execute_request\n    user_expressions, allow_stdin)\n  File \"/usr/local/lib/python3.5/dist-packages/ipykernel/ipkernel.py\", line 196, in do_execute\n    res = shell.run_cell(code, store_history=store_history, silent=silent)\n  File \"/usr/local/lib/python3.5/dist-packages/ipykernel/zmqshell.py\", line 501, in run_cell\n    return super(ZMQInteractiveShell, self).run_cell(*args, **kwargs)\n  File \"/usr/local/lib/python3.5/dist-packages/IPython/core/interactiveshell.py\", line 2717, in run_cell\n    interactivity=interactivity, compiler=compiler, result=result)\n  File \"/usr/local/lib/python3.5/dist-packages/IPython/core/interactiveshell.py\", line 2827, in run_ast_nodes\n    if self.run_code(code, result):\n  File \"/usr/local/lib/python3.5/dist-packages/IPython/core/interactiveshell.py\", line 2881, in run_code\n    exec(code_obj, self.user_global_ns, self.user_ns)\n  File \"<ipython-input-4-bbea01d50998>\", line 237, in <module>\n    RL_PBA()\n  File \"<ipython-input-4-bbea01d50998>\", line 179, in RL_PBA\n    Agent = agent(lr=1e-2, s_size=4, a_size=2, h_size=8, alpha=0.01)\n  File \"<ipython-input-4-bbea01d50998>\", line 153, in __init__\n    self.state_in = tf.placeholder(tf.float32, [None, s_size]) #\n  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/python/ops/array_ops.py\", line 1587, in placeholder\n    name=name)\n  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/python/ops/gen_array_ops.py\", line 2043, in _placeholder\n    name=name)\n  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/python/framework/op_def_library.py\", line 759, in apply_op\n    op_def=op_def)\n  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/python/framework/ops.py\", line 2240, in create_op\n    original_op=self._default_original_op, op_def=op_def)\n  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/python/framework/ops.py\", line 1128, in __init__\n    self._traceback = _extract_stack()\n\nInvalidArgumentError (see above for traceback): You must feed a value for placeholder tensor 'agent/Placeholder' with dtype float\n\t [[Node: agent/Placeholder = Placeholder[dtype=DT_FLOAT, shape=[], _device=\"/job:localhost/replica:0/task:0/cpu:0\"]()]]\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mInvalidArgumentError\u001b[0m                      Traceback (most recent call last)",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1020\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1021\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1022\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[0;34m(session, feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[1;32m   1002\u001b[0m                                  \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1003\u001b[0;31m                                  status, run_metadata)\n\u001b[0m\u001b[1;32m   1004\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3.5/contextlib.py\u001b[0m in \u001b[0;36m__exit__\u001b[0;34m(self, type, value, traceback)\u001b[0m\n\u001b[1;32m     65\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 66\u001b[0;31m                 \u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgen\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     67\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mStopIteration\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/tensorflow/python/framework/errors_impl.py\u001b[0m in \u001b[0;36mraise_exception_on_not_ok_status\u001b[0;34m()\u001b[0m\n\u001b[1;32m    468\u001b[0m           \u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpywrap_tensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_Message\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstatus\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 469\u001b[0;31m           pywrap_tensorflow.TF_GetCode(status))\n\u001b[0m\u001b[1;32m    470\u001b[0m   \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mInvalidArgumentError\u001b[0m: You must feed a value for placeholder tensor 'agent/Placeholder' with dtype float\n\t [[Node: agent/Placeholder = Placeholder[dtype=DT_FLOAT, shape=[], _device=\"/job:localhost/replica:0/task:0/cpu:0\"]()]]",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mInvalidArgumentError\u001b[0m                      Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-4-bbea01d50998>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    235\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    236\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 237\u001b[0;31m \u001b[0mRL_PBA\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-4-bbea01d50998>\u001b[0m in \u001b[0;36mRL_PBA\u001b[0;34m()\u001b[0m\n\u001b[1;32m    213\u001b[0m                             Agent.state_in: np.vstack(ep_buffer[:, 0])}\n\u001b[1;32m    214\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 215\u001b[0;31m                         \u001b[0mgrads\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mAgent\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgradients\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    216\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    217\u001b[0m                         \u001b[0;32mfor\u001b[0m \u001b[0midx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgrads\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    764\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    765\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 766\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    767\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    768\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    962\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    963\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[0;32m--> 964\u001b[0;31m                              feed_dict_string, options, run_metadata)\n\u001b[0m\u001b[1;32m    965\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    966\u001b[0m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_run\u001b[0;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1012\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1013\u001b[0m       return self._do_call(_run_fn, self._session, feed_dict, fetch_list,\n\u001b[0;32m-> 1014\u001b[0;31m                            target_list, options, run_metadata)\n\u001b[0m\u001b[1;32m   1015\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1016\u001b[0m       return self._do_call(_prun_fn, self._session, handle, feed_dict,\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1032\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1033\u001b[0m           \u001b[0;32mpass\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1034\u001b[0;31m       \u001b[0;32mraise\u001b[0m \u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnode_def\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mop\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1035\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1036\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_extend_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mInvalidArgumentError\u001b[0m: You must feed a value for placeholder tensor 'agent/Placeholder' with dtype float\n\t [[Node: agent/Placeholder = Placeholder[dtype=DT_FLOAT, shape=[], _device=\"/job:localhost/replica:0/task:0/cpu:0\"]()]]\n\nCaused by op 'agent/Placeholder', defined at:\n  File \"/usr/lib/python3.5/runpy.py\", line 184, in _run_module_as_main\n    \"__main__\", mod_spec)\n  File \"/usr/lib/python3.5/runpy.py\", line 85, in _run_code\n    exec(code, run_globals)\n  File \"/usr/local/lib/python3.5/dist-packages/ipykernel/__main__.py\", line 3, in <module>\n    app.launch_new_instance()\n  File \"/usr/local/lib/python3.5/dist-packages/traitlets/config/application.py\", line 658, in launch_instance\n    app.start()\n  File \"/usr/local/lib/python3.5/dist-packages/ipykernel/kernelapp.py\", line 474, in start\n    ioloop.IOLoop.instance().start()\n  File \"/usr/local/lib/python3.5/dist-packages/zmq/eventloop/ioloop.py\", line 177, in start\n    super(ZMQIOLoop, self).start()\n  File \"/usr/local/lib/python3.5/dist-packages/tornado/ioloop.py\", line 887, in start\n    handler_func(fd_obj, events)\n  File \"/usr/local/lib/python3.5/dist-packages/tornado/stack_context.py\", line 275, in null_wrapper\n    return fn(*args, **kwargs)\n  File \"/usr/local/lib/python3.5/dist-packages/zmq/eventloop/zmqstream.py\", line 440, in _handle_events\n    self._handle_recv()\n  File \"/usr/local/lib/python3.5/dist-packages/zmq/eventloop/zmqstream.py\", line 472, in _handle_recv\n    self._run_callback(callback, msg)\n  File \"/usr/local/lib/python3.5/dist-packages/zmq/eventloop/zmqstream.py\", line 414, in _run_callback\n    callback(*args, **kwargs)\n  File \"/usr/local/lib/python3.5/dist-packages/tornado/stack_context.py\", line 275, in null_wrapper\n    return fn(*args, **kwargs)\n  File \"/usr/local/lib/python3.5/dist-packages/ipykernel/kernelbase.py\", line 276, in dispatcher\n    return self.dispatch_shell(stream, msg)\n  File \"/usr/local/lib/python3.5/dist-packages/ipykernel/kernelbase.py\", line 228, in dispatch_shell\n    handler(stream, idents, msg)\n  File \"/usr/local/lib/python3.5/dist-packages/ipykernel/kernelbase.py\", line 390, in execute_request\n    user_expressions, allow_stdin)\n  File \"/usr/local/lib/python3.5/dist-packages/ipykernel/ipkernel.py\", line 196, in do_execute\n    res = shell.run_cell(code, store_history=store_history, silent=silent)\n  File \"/usr/local/lib/python3.5/dist-packages/ipykernel/zmqshell.py\", line 501, in run_cell\n    return super(ZMQInteractiveShell, self).run_cell(*args, **kwargs)\n  File \"/usr/local/lib/python3.5/dist-packages/IPython/core/interactiveshell.py\", line 2717, in run_cell\n    interactivity=interactivity, compiler=compiler, result=result)\n  File \"/usr/local/lib/python3.5/dist-packages/IPython/core/interactiveshell.py\", line 2827, in run_ast_nodes\n    if self.run_code(code, result):\n  File \"/usr/local/lib/python3.5/dist-packages/IPython/core/interactiveshell.py\", line 2881, in run_code\n    exec(code_obj, self.user_global_ns, self.user_ns)\n  File \"<ipython-input-4-bbea01d50998>\", line 237, in <module>\n    RL_PBA()\n  File \"<ipython-input-4-bbea01d50998>\", line 179, in RL_PBA\n    Agent = agent(lr=1e-2, s_size=4, a_size=2, h_size=8, alpha=0.01)\n  File \"<ipython-input-4-bbea01d50998>\", line 153, in __init__\n    self.state_in = tf.placeholder(tf.float32, [None, s_size]) #\n  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/python/ops/array_ops.py\", line 1587, in placeholder\n    name=name)\n  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/python/ops/gen_array_ops.py\", line 2043, in _placeholder\n    name=name)\n  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/python/framework/op_def_library.py\", line 759, in apply_op\n    op_def=op_def)\n  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/python/framework/ops.py\", line 2240, in create_op\n    original_op=self._default_original_op, op_def=op_def)\n  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/python/framework/ops.py\", line 1128, in __init__\n    self._traceback = _extract_stack()\n\nInvalidArgumentError (see above for traceback): You must feed a value for placeholder tensor 'agent/Placeholder' with dtype float\n\t [[Node: agent/Placeholder = Placeholder[dtype=DT_FLOAT, shape=[], _device=\"/job:localhost/replica:0/task:0/cpu:0\"]()]]\n"
     ]
    }
   ],
   "source": [
    "def RL_PBA():\n",
    "    \"\"\" 30 mins \"\"\"\n",
    "    import matplotlib.pyplot as plt\n",
    "    import math\n",
    "\n",
    "    import numpy as np\n",
    "    import tensorflow as tf\n",
    "\n",
    "    import tensorflow.contrib.slim as slim\n",
    "\n",
    "    import gym\n",
    "\n",
    "    env = gym.make(\"CartPole-v0\")\n",
    "    s = env.reset()\n",
    "\n",
    "    agent_level = True\n",
    "    gamma = 0.99\n",
    "    # HELPER FUNCTION\n",
    "    def discount_rewards(reward):\n",
    "            discounted_r = np.zeros_like(reward)\n",
    "            running_add = 0\n",
    "            for t in reversed(range(reward.size)):\n",
    "                running_add = running_add * gamma + reward[t]\n",
    "                discounted_r[t] = running_add\n",
    "            return discounted_r\n",
    "\n",
    "    if not agent_level:\n",
    "        # HYPER PARAMS\n",
    "        random_episodes = 0\n",
    "        episode_number = 1\n",
    "        total_episodes = 2000\n",
    "\n",
    "        batch_size = 3\n",
    "        learning_rate = 5e-2\n",
    "        gamma = .99\n",
    "\n",
    "        input_dim = 4\n",
    "\n",
    "        n_hidden1 = 40\n",
    "        n_hidden2 = 40\n",
    "\n",
    "        # PARAMS\n",
    "        tf.reset_default_graph()\n",
    "        reward_sum = 0\n",
    "        running_reward = None\n",
    "\n",
    "        observations = tf.placeholder(tf.float32, [None, input_dim], name=\"input_x\")\n",
    "        input_y = tf.placeholder(tf.float32, [None, 1], name=\"input_y\")\n",
    "        advantages = tf.placeholder(tf.float32, name=\"reward\")\n",
    "\n",
    "        def pol_net(observations):\n",
    "            w1 = tf.get_variable(\"w1\", shape=[input_dim, n_hidden1], \n",
    "                                initializer=tf.contrib.layers.xavier_initializer())\n",
    "            h1 = tf.nn.elu(tf.matmul(observations, w1))\n",
    "            w2 = tf.get_variable(\"w2\", shape=[n_hidden1, n_hidden2], \n",
    "                                initializer=tf.contrib.layers.xavier_initializer())\n",
    "            h2 = tf.nn.elu(tf.matmul(h1, w2))\n",
    "            w3 = tf.get_variable(\"w3\", shape=[n_hidden2, 1], \n",
    "                                initializer=tf.contrib.layers.xavier_initializer())\n",
    "            return tf.nn.sigmoid(tf.matmul(h2, w3))\n",
    "\n",
    "        pred = pol_net(observations)\n",
    "\n",
    "        tvars = tf.trainable_variables()\n",
    "\n",
    "        loglikelihood = tf.log(input_y*(input_y - pred) + (1-input_y)*(input_y + pred))\n",
    "        policy_cost = -tf.reduce_mean(loglikelihood*advantages)\n",
    "\n",
    "        newGrad = tf.gradients(policy_cost, tvars)\n",
    "        optimizer = tf.train.AdamOptimizer(learning_rate)\n",
    "\n",
    "        w1grad = tf.placeholder(tf.float32, name=\"batch_grad1\")\n",
    "        w2grad = tf.placeholder(tf.float32, name=\"batch_grad2\")\n",
    "\n",
    "        batchGrad = [w1grad, w2grad]\n",
    "        updateGrad = optimizer.apply_gradients(zip(batchGrad, tvars))\n",
    "\n",
    "        with tf.Session() as sess:\n",
    "            rendering = False\n",
    "            sess.run(tf.global_variables_initializer())\n",
    "            s = env.reset()\n",
    "\n",
    "            gradBuffer = sess.run(tvars)\n",
    "            for ix, grad in enumerate(gradBuffer):\n",
    "                gradBuffer[ix] = grad*0\n",
    "\n",
    "            xs, hs, dlogps, drs, ys, tfps = [], [], [], [], [], []\n",
    "\n",
    "            while episode_number <= total_episodes:\n",
    "                if reward_sum/batch_size > 100 or rendering == True:\n",
    "                    env.render()\n",
    "                    rendering = True\n",
    "                s = np.reshape(s, [1, input_dim])\n",
    "                tfprob = sess.run(pred, feed_dict={observations: s})\n",
    "\n",
    "                action = 1 if np.random.uniform() < tfprob else 0\n",
    "                xs.append(s)\n",
    "\n",
    "                y = 1 if action == 0 else 0\n",
    "                ys.append(y)\n",
    "\n",
    "                sn, reward, done, info = env.step(action)\n",
    "\n",
    "                reward_sum += reward\n",
    "                drs.append(reward)\n",
    "                s = sn\n",
    "\n",
    "                if done:\n",
    "                    episode_number += 1\n",
    "                    epx = np.vstack(xs)\n",
    "                    epy = np.vstack(ys)\n",
    "                    epr = np.vstack(drs)\n",
    "\n",
    "                    tfp = tfps\n",
    "                    xs, hs, dlogps, drs, ys, tfps = [], [], [], [], [], []\n",
    "\n",
    "                    discounted_epr = discount_rewards(epr)\n",
    "                    discounted_epr -= np.mean(discounted_epr)\n",
    "                    discounted_epr /= np.std(discounted_epr)\n",
    "\n",
    "                    tGrad = sess.run(newGrad, feed_dict={\n",
    "                            observations: epx,\n",
    "                            input_y: epy,\n",
    "                            advantages: discounted_epr,\n",
    "                        })\n",
    "                    for ix, grad in enumerate(tGrad):\n",
    "                        gradBuffer[ix] += grad\n",
    "\n",
    "                    if episode_number % batch_size == 0:\n",
    "                        sess.run(updateGrad, feed_dict={w1grad: gradBuffer[0], w2grad: gradBuffer[1]})\n",
    "\n",
    "                        for ix, grad in enumerate(gradBuffer):\n",
    "                            gradBuffer[ix] = grad*0\n",
    "\n",
    "                        running_reward = reward_sum if running_reward is None else running_reward*0.99 + reward_sum*0.01\n",
    "                        print(\"{:d} episode : Average reward for episode {:.2f}. Total average reward {:.2f}\".format(\n",
    "                                    episode_number,\n",
    "                                    reward_sum/batch_size,\n",
    "                                    running_reward/batch_size\n",
    "                            ))\n",
    "                    if reward_sum/batch_size > 200:\n",
    "                        print(\"Task solved in \", episode_number, \" episodes.\")\n",
    "                        break\n",
    "                    reward_sum = 0\n",
    "                    s = env.reset()\n",
    "\n",
    "    else:\n",
    "        # DEFINE POLICY AGENT\n",
    "        class agent():\n",
    "            def __init__(self, lr, s_size, a_size, h_size, alpha):\n",
    "                with tf.variable_scope(\"agent\"):\n",
    "                    # POLICY NET\n",
    "                    self.state_in = tf.placeholder(tf.float32, [None, s_size]) #\n",
    "                    hidden = slim.fully_connected(self.state_in, h_size, \n",
    "                                biases_initializer=None, activation_fn=tf.nn.relu)\n",
    "                    self.output = slim.fully_connected(hidden, a_size, \n",
    "                                biases_initializer=None, activation_fn=tf.nn.softmax) #\n",
    "\n",
    "                    self.action = tf.argmax(self.output, 1)\n",
    "\n",
    "                    self.reward_holder = tf.placeholder(tf.float32, [None]) #\n",
    "                    self.action_holder = tf.placeholder(tf.int32, [None]) # \n",
    "                    self.indexes = tf.range(0, tf.shape(self.output)[0])*tf.shape(self.output)[1] + self.action_holder\n",
    "                    self.responsible_outputs = tf.gather(tf.reshape(self.output, [-1]), self.indexes)\n",
    "\n",
    "                    self.loss = -tf.reduce_mean(tf.log(self.responsible_outputs)*self.reward_holder)\n",
    "\n",
    "                    tvars = tf.trainable_variables()\n",
    "                    self.gradient_holders = [] #\n",
    "                    for idx, var in enumerate(tvars):\n",
    "                        placeholder = tf.placeholder(tf.float32, name=str(idx)+\"_holder\")\n",
    "                        self.gradient_holders.append(placeholder)\n",
    "\n",
    "                    self.gradients = tf.gradients(self.loss, tvars) #\n",
    "                    optimizer = tf.train.AdamOptimizer(learning_rate=lr)\n",
    "                    self.update_batch = optimizer.apply_gradients(zip(self.gradient_holders, tvars)) #\n",
    "\n",
    "        tf.reset_default_graph()\n",
    "        Agent = agent(lr=1e-2, s_size=4, a_size=2, h_size=8, alpha=0.01)\n",
    "        total_episodes = 5000\n",
    "        max_ep = 999\n",
    "        update_frequency = 5\n",
    "\n",
    "        with tf.Session() as sess:\n",
    "            sess.run(tf.global_variables_initializer())\n",
    "            i = 0\n",
    "            total_reward = []\n",
    "            total_length = []\n",
    "\n",
    "            gradBuffer = sess.run(tf.trainable_variables())\n",
    "            for ix, grad in enumerate(gradBuffer):\n",
    "                gradBuffer[ix] = grad*0\n",
    "\n",
    "            while i < total_episodes:\n",
    "                s = env.reset()\n",
    "                running_reward = 0\n",
    "                ep_buffer = []\n",
    "                for j in range(max_ep):\n",
    "                    a_dist = sess.run(Agent.output, feed_dict={Agent.state_in : [s]})\n",
    "                    a = np.random.choice(a_dist[0], p=a_dist[0])\n",
    "                    a = np.argmax(a_dist == a)\n",
    "\n",
    "                    sn, r, d, _ = env.step(a)\n",
    "                    ep_buffer.append([s,a,r,sn])\n",
    "                    s = sn\n",
    "                    running_reward += r\n",
    "\n",
    "                    if d:\n",
    "                        ep_buffer = np.array(ep_buffer)\n",
    "                        ep_buffer[:, 2] = discount_rewards(ep_buffer[:, 2])\n",
    "                        feed_dict={Agent.reward_holder: ep_buffer[:, 2],\n",
    "                            Agent.action_holder: ep_buffer[:, 1],\n",
    "                            Agent.state_in: np.vstack(ep_buffer[:, 0])}\n",
    "\n",
    "                        grads = sess.run(Agent.gradients)\n",
    "\n",
    "                        for idx, grad in enumerate(grads):\n",
    "                            gradBuffer[idx] += grad\n",
    "\n",
    "                        if i % update_frequency == 0 and i != 0:\n",
    "                            feed_dict=dict(zip(Agent.gradient_holders, gradBuffer))\n",
    "                            _ = sess.run(Agent.update_batch, feed_dict=feed_dict)\n",
    "                            for ix, grad in enumerate(gradBuffer):\n",
    "                                gradBuffer[ix] = grad * 0\n",
    "                        total_reward.append(running_reward)\n",
    "                        total_length.append(j)\n",
    "                        break\n",
    "                if i % 100 ==0:\n",
    "                    print(\"mean reward : \",np.mean(total_reward[-100:]))\n",
    "                if np.mean(total_reward[-100:]) > 190:\n",
    "                    env.render()\n",
    "                    break\n",
    "                i += 1\n",
    "\n",
    "    return None\n",
    "\n",
    "RL_PBA()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='Model Based Agent tensorflow'></a>\n",
    "## Model Based Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def RL_MBA():\n",
    "    import math\n",
    "    import matplotlib.pyplot as plt\n",
    "\n",
    "    import numpy as np\n",
    "    import tensorflow as tf\t\n",
    "\n",
    "    from tensorflow.python.framework import dtypes\n",
    "    from tensorflow.python.framework import ops\n",
    "    from tensorflow.python.ops import (\n",
    "                array_ops,\n",
    "                control_flow_ops,\n",
    "                math_ops,\n",
    "                nn_ops,\n",
    "                rnn, rnn_cell,\n",
    "                variable_scope\n",
    "        )\n",
    "    from tensorflow.contrib.layers import xavier_initializer\n",
    "\n",
    "    import gym\n",
    "\n",
    "    env = gym.make(\"CartPole-v0\")\n",
    "\n",
    "    # HYPER PARAMS\t\n",
    "    max_episode_num = 5000\n",
    "\n",
    "    learning_rate = 1e-2\n",
    "    dis = .99\n",
    "    decay_rate = .99\n",
    "    resume = False\n",
    "\n",
    "    model_batch_size = 3\n",
    "    real_batch_size = 3\n",
    "\n",
    "    input_dim = 4\n",
    "    n_hidden = 4\t\n",
    "\n",
    "    # POLICY NETWORK\n",
    "    tf.reset_default_graph()\n",
    "    class Policy_Network():\n",
    "        def __init__(self, session, input_size, output_size, structure_dict=None, name=\"pol_main\")\n",
    "            self.session = session\n",
    "            self.input_size = input_size\n",
    "            self.output_size = output_size \n",
    "            self._build_network()\n",
    "\n",
    "        def _build_network(self, lr=1e-2):\n",
    "            #TODO: structure dict contains layer structure\n",
    "            with tf.variable_scope(\"policy_network\"):\n",
    "                self._obs = tf.placeholder(tf.float32, [None, 4], name=\"input_x\")\n",
    "                self._y = tf.placeholder(tf.float32, [None, 1], name=\"input_y\")\n",
    "\n",
    "                w1 = tf.get_variable(\"w1\", shape=[4, n_hidden], initializer=xavier_initializer())\n",
    "                w2 = tf.get_variable(\"w2\", shape=[n_hidden, 1], initializer=xavier_initializer())\n",
    "\n",
    "                h1 = tf.nn.relu(tf.matmul(obs, w1))\n",
    "            self._pred = tf.nn.sigmoid(tf.matmul(h1, w2))\n",
    "\n",
    "            tvars = tf.trainable_variables()\n",
    "\n",
    "            self._advantages = tf.placeholder(tf.float32, name=\"reward_signal\")\n",
    "            self._loglik = tf.log(input_y*(input_y - pred) + (1-input_y)*(input_y+pred))\n",
    "            self._loss = -tf.reduce_mean(loglik*advantages)\n",
    "            self._optm = tf.train.AdamOptimizer(learning_rate=lr).minimize(loss)\n",
    "\n",
    "        def predict(self, state):\n",
    "            x = np.reshape(state, [1, self.input_size])\n",
    "            return self.session.run(self._pred, feed_dict={self._obs: x})\n",
    "\n",
    "        def update(self, x_stack, y_stack):\n",
    "            return self.session.run([self._loss, self._optm], \n",
    "                        feed_dict={self._obs: x_stack, self._y: y_stack})\n",
    "\n",
    "\n",
    "    class Model_Network():\n",
    "        def __init__(self, session, input_size, output_size, \n",
    "                        sdict={'m_hidden':256}, name=\"model_main\"):\n",
    "            \"\"\"\n",
    "                input_size : [env.observation_space.shape, env.action_space.m]\n",
    "            \"\"\"\n",
    "            self.input_size = input_size\n",
    "            self.s_size = np.prod(input_size[0])\n",
    "            self.a_size = input_size[1]\n",
    "            self.output_size = output_size\n",
    "            self.m_hidden = sdict['m_hidden']\n",
    "\n",
    "            print_network_info()\n",
    "            _build_network()\n",
    "\n",
    "        def _build_network(self, rl=1e-3):#TODO: change to structure dict\n",
    "            with tf.variable_scope(\"model_network\"):\n",
    "                m_w1 = tf.get_variable(\"m_w1\", shape=[self.input_size, self.m_hidden],\n",
    "                                                initializer=xavier_initializer())\n",
    "                m_b1 = tf.Variable(tf.zeros([self.m_hidden, self.m_hidden]), name=\"m_b1\")\n",
    "\n",
    "                m_w2 = tf.get_variable(\"m_w2\", shape=[self.m_hidden, self.m_hidden], \n",
    "                                                initializer=xavier_initializer())\n",
    "                m_b2 = tf.Variable(tf.zeros([self.m_hidden, self.output_size]))\n",
    "\n",
    "                m_w_obs = tf.get_variable(\"m_w_obs\", shape=[self.m_hidden, a_size],\n",
    "                                                initializer=xavier_initializer())\n",
    "                m_w_reward = tf.get_variable(\"m_w_reward\", shape=[self.m_hidden, 1])\n",
    "                m_w_done = tf.get_variable(\"m_w_done\", shape=[self.m,_hidden 1])\n",
    "\n",
    "                m_b_obs = tf.Variable(tf.zeros([s_size]), name=\"m_b_obs\")\n",
    "                m_b_reward = tf.Variable(tf.zeros([1]), name=\"m_b_reward\")\n",
    "                m_b_done = tf.Variable(tf.zeros([1]), name=\"m_b_done\")\n",
    "\n",
    "                self.prev_state = tf.placeholder(tf.float32, [None, self.input_size])\n",
    "                m_h1 = tf.nn.relu(tf.matmul(prev_state, m_w1) + m_b1)\n",
    "                m_out = tf.nn.relu(tf.matmul(m_h1, m_w2) + m_b2)\n",
    "\n",
    "            pred_obs = tf.matmul(m_out, m_w_obs, name=\"pred_obs\") + m_b_obs\n",
    "            pred_reward = tf.matmul(m_out, m_w_bos, name=\"pred_reward\") + m_b_reward\n",
    "            pred_done = tf.sigmoid(tf.matmul(m_out, m_w_done, name=\"pred_done\") + m_b_done)\n",
    "\n",
    "            self.pred_state = tf.concat(1, [pred_obs, pred_reward, pred_done])\t\n",
    "\n",
    "            true_obs = tf.placeholder(tf.float32, [None, s_size], name=\"true_observation\")\n",
    "            true_reward = tf.placeholder(tf.float32, [None, 1],name=\"true_reward\")\n",
    "            true_done = tf.placeholder(tf.float32, [None, 1])\n",
    "\n",
    "        def predict(self, state):\n",
    "            x = np.reshape(state, [1, self.input_size])\n",
    "            return self.session.run(self._pred, feed_dict={self._obs: x})\n",
    "\n",
    "        def update(self, x_stack, y_stack):\n",
    "            return self.session.run([self._loss, self._optm], feed_dict={self._obs: x_stack, self._y: y_stack})\n",
    "\n",
    "        def stepModel(sess, xs, action):\n",
    "            feed_dict = {self.prev_state: np.reshape(np.hstack([xs[-1][0], np.array(action)]),\n",
    "                        [1, 5])}\n",
    "            myPredict = sess.run([self.pred_state], feed_dict=feed_dict)\n",
    "            reward = myPredict[0][:4]\n",
    "            obs = myPredict[0][:, 0:4]\n",
    "            obs[:, 0] = np.clip(obs[:, 0], -2.4, 2.4)\n",
    "            obs[:, 2] = np.clip(obs[:, 2], -0.4, 0.4)\n",
    "\n",
    "            doneP = np.clip(myPredict[0][:, 5], 0, 1)\n",
    "            if doneP > 0.1 or len(xs) >= 300:\n",
    "                done = True\n",
    "            else:\n",
    "                done = False\n",
    "            return obs, reward, done \n",
    "\n",
    "        def print_network_info(self):#TODO: infos\n",
    "            print(\"Building Network..\" + \"\\n\" +\\\n",
    "                \"=\"*10 + name +\"=\"*10 + \"\\n\" +\\\n",
    "            )\n",
    "\n",
    "\n",
    "    # PARAMS\n",
    "\n",
    "    xs, drs, ys, ds = [], [], [], []\n",
    "\n",
    "    drawFromModel = Flase\n",
    "    trainTheModel = True\n",
    "    trainThePolicy = False\n",
    "\n",
    "    with tf.Session() as sess:\n",
    "        rendering = False\n",
    "        sess.run(tf.global_variables_initializer())\n",
    "\n",
    "        policy_net = Policy_NetWork(sess, input_size, output_size)\n",
    "        model_net = Model_NetWork(sess, input_sizes, output_size)\n",
    "        print(\"SET NETWORK\")\n",
    "\n",
    "        obs = env.reset()\n",
    "        x = obs\n",
    "        print(\"SET ENVIRONMENT\")\n",
    "\n",
    "        while episode <= max_episode_num:\n",
    "            if episode % 50 == 0:\n",
    "                print(\"in \" + str(episode) + \"th episode...\")\n",
    "            if (reward_sum/batch_size) > 140 and drawFromModel == False or rendering == True:\n",
    "                    env.render()\n",
    "                    rendering = True\n",
    "\n",
    "            x = np.reshape(obs, [1, 4])\n",
    "            tfprob = sess.run(pred, feed_dict={obs: x})\n",
    "\n",
    "            if np.random.uniform() < tfprob:\n",
    "                action = 1\n",
    "            else:\n",
    "                action = env.action_space.sample()\n",
    "\n",
    "            xs.append(x)\n",
    "\n",
    "            if drawFromModel == False:\n",
    "                obs, reward, done, info = env.step(action)\n",
    "            else:\n",
    "                obs, reward, done = stepModel(sess, xs, action)\n",
    "\n",
    "            reward_sum += reward\n",
    "\n",
    "            ds.append(done*1)\n",
    "            drs.append(reward)\n",
    "\n",
    "            if done:\n",
    "                if drawFromModel == False:\n",
    "                    real_episodes += 1\n",
    "                episode_number += 1\n",
    "\n",
    "                epx = np.vstack(xs)\n",
    "                epy = np.vstack(ys)\n",
    "                epr = np.vstack(drs)\n",
    "                epd = np.vstack(ds)\n",
    "                xs, drs, ys, ds = [], [], [], []\n",
    "\n",
    "            if trainTheModel == True:\n",
    "                print(\"trainPolicy in \" + str(episode_number))\n",
    "                discounted_epr = discounted_rewards(epr).astype('float32')\n",
    "                discounted_epr -= np.mean(discounted_epr)\n",
    "                discounted_epr /= np.std(discounted_epr)\n",
    "\n",
    "                sess.run(optimizer, feed_dict={\n",
    "                            obs: epx,\n",
    "                            input_y: epy,\n",
    "                            advantages: discounted_epr\n",
    "                        })\n",
    "\n",
    "            if switch_point + batch_size == episode_number:\n",
    "                switch_point = episode_number\n",
    "\n",
    "                if trainThePolicy == True:\n",
    "                    print(\"trainThePoilcy in \"+str(episode_number))\n",
    "                    discounted_epr = discounted_rewards(epr).astype('float32')\n",
    "                    discounted_epr -= np.mean(discounted_epr)\n",
    "                    discounted_epr /= np.std(discounted_epr)\n",
    "\n",
    "                running_reward = reward_sum if running_reward is None else running_reward*.99 + reward_sum*.01\n",
    "                if drawFromModel == False:\n",
    "                    print(\"World Perf: Episode {:d}, Reward {:.5f}, Action {:.5f}\"+\n",
    "                            \"Mean Reward {}\".format(real_episodes, \n",
    "                                reward_sum/real_batch_size,\n",
    "                                action,\n",
    "                                running_reward/real_batch_size))\n",
    "                    if reward_sum/batch_size > 200:\n",
    "                        break\n",
    "                reward_sum = 0\n",
    "                if episode_number > 100:\n",
    "                    drawFromModel = not drawFromModel\n",
    "                    trainTheModel = not trainTheModel\n",
    "                    trainThePolicy = not trainThePolicy\n",
    "\n",
    "            if drawFromModel == True:\n",
    "                observation = np.random.uniform(-0.1, 0.1, [4])\n",
    "                batch_size = model_batch_size\n",
    "            else:\n",
    "                observation = env.reset()\n",
    "                batch_size = real_batch_size\n",
    "\n",
    "\n",
    "    plt.figure(figsize=(8, 12))\t\n",
    "    for i in range(6):\n",
    "        plt.subplot(6, 2, 2*i+1)\n",
    "        plt.plot(pstate[:, i])\n",
    "        plt.subplot(6, 2, 2*i+1)\n",
    "        plt.plot(state_nextsAll[:, i])\t\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "    return None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='Deep Q-Learning Network tensorflow'></a>\n",
    "## Deep Q-Learning Network tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2017-03-15 22:24:27,295] Making new env: CartPole-v0\n",
      "[2017-03-15 22:24:27,722] You are calling 'step()' even though this environment has already returned done = True. You should always call 'reset()' once you receive 'done = True' -- any further steps are undefined behavior.\n",
      "[2017-03-15 22:24:27,885] You are calling 'step()' even though this environment has already returned done = True. You should always call 'reset()' once you receive 'done = True' -- any further steps are undefined behavior.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode : 0, step : 10001\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2017-03-15 22:24:28,457] You are calling 'step()' even though this environment has already returned done = True. You should always call 'reset()' once you receive 'done = True' -- any further steps are undefined behavior.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode : 1, step : 10001\n",
      "loss :  0.00232896\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2017-03-15 22:24:29,070] You are calling 'step()' even though this environment has already returned done = True. You should always call 'reset()' once you receive 'done = True' -- any further steps are undefined behavior.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode : 2, step : 10001\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2017-03-15 22:24:29,792] You are calling 'step()' even though this environment has already returned done = True. You should always call 'reset()' once you receive 'done = True' -- any further steps are undefined behavior.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode : 3, step : 10001\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2017-03-15 22:24:30,697] You are calling 'step()' even though this environment has already returned done = True. You should always call 'reset()' once you receive 'done = True' -- any further steps are undefined behavior.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode : 4, step : 10001\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2017-03-15 22:24:31,639] You are calling 'step()' even though this environment has already returned done = True. You should always call 'reset()' once you receive 'done = True' -- any further steps are undefined behavior.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode : 5, step : 10001\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2017-03-15 22:24:32,664] You are calling 'step()' even though this environment has already returned done = True. You should always call 'reset()' once you receive 'done = True' -- any further steps are undefined behavior.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode : 6, step : 10001\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2017-03-15 22:24:33,770] You are calling 'step()' even though this environment has already returned done = True. You should always call 'reset()' once you receive 'done = True' -- any further steps are undefined behavior.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode : 7, step : 10001\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2017-03-15 22:24:34,948] You are calling 'step()' even though this environment has already returned done = True. You should always call 'reset()' once you receive 'done = True' -- any further steps are undefined behavior.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode : 8, step : 10001\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2017-03-15 22:24:36,137] You are calling 'step()' even though this environment has already returned done = True. You should always call 'reset()' once you receive 'done = True' -- any further steps are undefined behavior.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode : 9, step : 10001\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2017-03-15 22:24:37,515] You are calling 'step()' even though this environment has already returned done = True. You should always call 'reset()' once you receive 'done = True' -- any further steps are undefined behavior.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode : 10, step : 10001\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2017-03-15 22:24:39,069] You are calling 'step()' even though this environment has already returned done = True. You should always call 'reset()' once you receive 'done = True' -- any further steps are undefined behavior.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode : 11, step : 10001\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2017-03-15 22:24:40,666] You are calling 'step()' even though this environment has already returned done = True. You should always call 'reset()' once you receive 'done = True' -- any further steps are undefined behavior.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode : 12, step : 10001\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2017-03-15 22:24:42,141] You are calling 'step()' even though this environment has already returned done = True. You should always call 'reset()' once you receive 'done = True' -- any further steps are undefined behavior.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode : 13, step : 10001\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2017-03-15 22:24:43,661] You are calling 'step()' even though this environment has already returned done = True. You should always call 'reset()' once you receive 'done = True' -- any further steps are undefined behavior.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode : 14, step : 10001\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-6-f122142d1906>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    125\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    126\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 127\u001b[0;31m \u001b[0mprint_runtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mDQN\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-2-c1bbb8ae935c>\u001b[0m in \u001b[0;36mprint_runtime\u001b[0;34m(func)\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mprint_runtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0mstart\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m     \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m     \u001b[0mend\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"=\"\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0;36m20\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-6-f122142d1906>\u001b[0m in \u001b[0;36mDQN\u001b[0;34m()\u001b[0m\n\u001b[1;32m     98\u001b[0m                     \u001b[0ma\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maction_space\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msample\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     99\u001b[0m                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 100\u001b[0;31m                     \u001b[0ma\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmaindqn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    101\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    102\u001b[0m                 \u001b[0msn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0md\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minfo\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-6-f122142d1906>\u001b[0m in \u001b[0;36mpredict\u001b[0;34m(self, state)\u001b[0m\n\u001b[1;32m     39\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     40\u001b[0m             \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minput_size\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 41\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msession\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_Qpred\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_X\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     42\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     43\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx_stack\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_stack\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    764\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    765\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 766\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    767\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    768\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    962\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    963\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[0;32m--> 964\u001b[0;31m                              feed_dict_string, options, run_metadata)\n\u001b[0m\u001b[1;32m    965\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    966\u001b[0m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_run\u001b[0;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1012\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1013\u001b[0m       return self._do_call(_run_fn, self._session, feed_dict, fetch_list,\n\u001b[0;32m-> 1014\u001b[0;31m                            target_list, options, run_metadata)\n\u001b[0m\u001b[1;32m   1015\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1016\u001b[0m       return self._do_call(_prun_fn, self._session, handle, feed_dict,\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1019\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1020\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1021\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1022\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1023\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[0;34m(session, feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[1;32m   1001\u001b[0m         return tf_session.TF_Run(session, options,\n\u001b[1;32m   1002\u001b[0m                                  \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1003\u001b[0;31m                                  status, run_metadata)\n\u001b[0m\u001b[1;32m   1004\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1005\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msession\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "def DQN():\n",
    "    \"\"\" 30 mins \"\"\"\n",
    "    import random\n",
    "    from collections import deque\n",
    "    import numpy as np\n",
    "\n",
    "    import tensorflow as tf\n",
    "    import gym\n",
    "\n",
    "    env = gym.make(\"CartPole-v0\")\n",
    "    input_size = env.observation_space.shape[0]\n",
    "    output_size = env.action_space.n\n",
    "    dis = 0.99\n",
    "    REPLAY_BUFFER = 4000\n",
    "\n",
    "    class dqn():\n",
    "        def __init__(self, session, input_size, output_size, name=\"main\"):\n",
    "            self.session = session\n",
    "            self.input_size = input_size\n",
    "            self.output_size = output_size\n",
    "            self.net_name = name\n",
    "\n",
    "            self._build_network()\n",
    "\n",
    "        def _build_network(self, h_size=10, l_rate=1e-1):\n",
    "            with tf.variable_scope(self.net_name):\n",
    "                self._X = tf.placeholder(tf.float32, [None, self.input_size], name=\"input_x\")\n",
    "                W1 = tf.get_variable(\"w1\", shape=[self.input_size, h_size], \n",
    "                            initializer=tf.contrib.layers.xavier_initializer())\n",
    "                W2 = tf.get_variable(\"w2\", shape=[h_size, self.output_size], \n",
    "                            initializer=tf.contrib.layers.xavier_initializer())\n",
    "                h1 = tf.nn.tanh(tf.matmul(self._X, W1))\n",
    "\n",
    "                self._Qpred = tf.matmul(h1, W2)\n",
    "            self._Y = tf.placeholder(tf.float32, [None, self.output_size])\n",
    "            self._loss = tf.reduce_mean(tf.square(self._Y - self._Qpred))\n",
    "            self._train = tf.train.AdamOptimizer(l_rate).minimize(self._loss)\n",
    "\n",
    "        def predict(self, state):\n",
    "            x = np.reshape(state, [1, self.input_size])\n",
    "            return self.session.run(self._Qpred, feed_dict={self._X: x})\n",
    "\n",
    "        def update(self, x_stack, y_stack):\n",
    "            return self.session.run([self._loss, self._train], feed_dict={self._X: x_stack, self._Y: y_stack})\n",
    "\n",
    "    def simple_replay_train(maindqn, target, train_batch):\n",
    "        x_stack = np.empty(0).reshape(0, maindqn.input_size)\n",
    "        y_stack = np.empty(0).reshape(0, maindqn.output_size)\n",
    "\n",
    "        for state, action, reward, next_state, done in train_batch:\n",
    "            q = maindqn.predict(state)\n",
    "            if done:\n",
    "                q[0, action] = reward\n",
    "            else:\n",
    "                q[0, action] = reward + dis*np.max(targetdqn.predict(next_state))\n",
    "            y_stack = np.vstack([y_stack, q])\n",
    "            x_stack = np.vstack([x_stack, state])\n",
    "        return maindqn.update(x_stack, y_stack)\n",
    "\n",
    "    def bot_play(maindqn):\n",
    "        s = env.reset()\n",
    "        reward_sum = 0\n",
    "        while True:\n",
    "            env.render()\n",
    "            a = np.argmax(maindqn.predict(s))\n",
    "            s, reward, done, info = env.step(a)\n",
    "            reward_sum += reward\n",
    "            if done:\n",
    "                print(\"Total reward : {}\".format(reward_sum))\n",
    "                break\n",
    "\n",
    "    def get_copy_var_ops(*, from_scope=\"main\", to_scope=\"target\"):\n",
    "        op_holder = []\n",
    "        from_vars = tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES, from_scope)\n",
    "        to_vars = tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES, to_scope)\n",
    "        for from_var, to_var in zip(from_vars, to_vars):\n",
    "            op_holder.append(to_var.assign(from_var))\n",
    "        return op_holder\n",
    "\n",
    "    max_episodes = 300\n",
    "    replay_buffer = deque()\n",
    "    with tf.Session() as sess:\n",
    "        maindqn = dqn(sess, input_size, output_size, name=\"main\")\n",
    "        targetdqn = dqn(sess, input_size, output_size, name=\"target\")\n",
    "        sess.run(tf.global_variables_initializer())\n",
    "\n",
    "        copy_ops = get_copy_var_ops(from_scope=\"main\", to_scope=\"target\")\n",
    "        sess.run(copy_ops)\n",
    "\n",
    "        for episode in range(max_episodes):\n",
    "            e = 1. / ((episode / 10.) + 1.)\n",
    "            done = False\n",
    "            step_count = 0\n",
    "\n",
    "            s = env.reset()\n",
    "            while not done:\n",
    "                if np.random.rand(1) < e:\n",
    "                    a = env.action_space.sample()\n",
    "                else:\n",
    "                    a = np.argmax(maindqn.predict(s))\n",
    "\n",
    "                sn, r, d, info = env.step(a)\n",
    "                if done:\n",
    "                    r = -100\n",
    "                replay_buffer.append((s, a, r, sn, d))\n",
    "                if len(replay_buffer) > REPLAY_BUFFER:\n",
    "                    replay_buffer.popleft()\n",
    "\n",
    "                s = sn\n",
    "                step_count += 1\n",
    "                if step_count > 10000:\n",
    "                    break\n",
    "            print(\"episode : {}, step : {}\".format(episode, step_count))\n",
    "            #if step_count > 10000:\n",
    "            #\tbreak\n",
    "\n",
    "            if episode % 10 == 1:\n",
    "                for _ in range(50):\n",
    "                    minibatch = random.sample(replay_buffer, 10)\n",
    "                    loss, _ = simple_replay_train(maindqn, targetdqn, minibatch)\n",
    "                if episode % 50 == 1:\n",
    "                    print(\"loss : \", loss)\n",
    "                sess.run(copy_ops)\n",
    "        bot_play(maindqn)\n",
    "    return None\n",
    "\n",
    "print_runtime(DQN)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='Double-Dualing Deep Q-Learning Network tensorflow'></a>\n",
    "## Doubling-Dualing Deep Q-learning Network tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def DDQN():\n",
    "    \"\"\" 120 mins  \"\"\"\n",
    "    import os\n",
    "    import functools\n",
    "    import random\n",
    "    import matplotlib.pyplot as plt\n",
    "    import scipy.misc\n",
    "\n",
    "    import numpy as np\n",
    "    import tensorflow as tf\n",
    "    import tensorflow.contrib.slim as slim\n",
    "\n",
    "    import gym\n",
    "\n",
    "    env = gym.make(\"Breakout-v0\")\n",
    "\n",
    "    #\tinput_state_n = functools.reduce(lambda x,y : x*y, env.observation_space.shape)\n",
    "    input_state_n = np.prod(env.observation_space.shape)\n",
    "    input_obs_shape = list(env.observation_space.shape)\n",
    "    output_action_n = env.action_space.n\n",
    "\n",
    "    max_episode_num = 1000\n",
    "    max_step_num = 500\n",
    "\n",
    "    pre_train_step_num = 100\n",
    "\n",
    "    starte = 1\n",
    "    ende = 0.1\n",
    "    explore_num = min(max_step_num, 300)\n",
    "    ed = (starte - ende)/explore_num\n",
    "    e = starte\n",
    "\n",
    "    replay_buffer_size = 100\n",
    "    update_freq = 5\n",
    "    h_size = 512\n",
    "\n",
    "    batch_size = 32\n",
    "    tau = 0.001\n",
    "\n",
    "    dis = .99\n",
    "\n",
    "    load_model = False\n",
    "    model_path = \"./ddqn_model\"\n",
    "\n",
    "    class qnetwork():\n",
    "        def __init__(self, h_size):\n",
    "            self.scalarInput = tf.placeholder(tf.float32, [None, input_state_n])\n",
    "            self.imageIn = tf.reshape(self.scalarInput, shape=[-1]+input_obs_shape)\n",
    "            self.conv1 = slim.convolution2d(\n",
    "                            inputs=self.imageIn,\n",
    "                            num_outputs=32,\n",
    "                            kernel_size=[8, 8],\n",
    "                            stride=[4, 4],\n",
    "                            padding='VALID',\n",
    "                            activation_fn=tf.nn.relu,\n",
    "                            biases_initializer=None\n",
    "                        )\n",
    "            self.conv2 = slim.convolution2d(\n",
    "                            inputs=self.conv1,\n",
    "                            num_outputs=64,\n",
    "                            kernel_size=[3, 3],\n",
    "                            stride=[1, 1],\n",
    "                            padding='VALID',\n",
    "                            activation_fn=tf.nn.relu,\n",
    "                            biases_initializer=None\n",
    "                        )\n",
    "            self.conv3 = slim.convolution2d(\n",
    "                            inputs=self.conv2,\n",
    "                            num_outputs=512,\n",
    "                            kernel_size=[7, 7],\n",
    "                            stride=[1, 1],\n",
    "                            padding='VALID',\n",
    "                            activation_fn=tf.nn.relu,\n",
    "                            biases_initializer=None\n",
    "                        )\n",
    "\n",
    "            # Dueling\n",
    "            self.streamAC, self.streamVC = tf.split(3, 2, self.conv3)\n",
    "            self.streamA = slim.flatten(self.streamAC)\n",
    "            self.streamV = slim.flatten(self.streamVC)\n",
    "\n",
    "            self.streamA = slim.fully_connected(self.streamA, 256)\n",
    "            self.streamV = slim.fully_connected(self.streamV, 256)\n",
    "\n",
    "            self.AW = tf.Variable(tf.random_normal([h_size//2, output_action_n]))\n",
    "            self.VW = tf.Variable(tf.random_normal([h_size//2, 1]))\n",
    "\n",
    "            self.Advantage = tf.matmul(self.streamA, self.AW)\n",
    "            self.Value = tf.matmul(self.streamV, self.VW)\n",
    "\n",
    "            self.Qout = self.Value + tf.sub(self.Advantage, tf.reduce_mean(self.Advantage, reduction_indices=1, keep_dims=True))\n",
    "            self.predict = tf.argmax(self.Qout, 1)\n",
    "\n",
    "            self.targetQ = tf.placeholder(tf.float32, [None])\n",
    "            self.actions = tf.placeholder(tf.int32, [None])\n",
    "            self.actions_onehot = tf.one_hot(self.actions, output_action_n, dtype=tf.float32)\n",
    "\n",
    "            self.Q = tf.reduce_sum(tf.mul(self.Qout, self.actions_onehot), reduction_indices=1)\n",
    "\n",
    "            self.td_error = tf.square(self.targetQ - self.Q)\n",
    "            self.loss = tf.reduce_mean(self.td_error)\n",
    "            self.trainer = tf.train.AdamOptimizer(learning_rate=1e-3)\n",
    "            self.updateModel = self.trainer.minimize(self.loss)\n",
    "\n",
    "    class experience_buffer():\n",
    "        \"\"\" s, a, r, sn, d \"\"\"\n",
    "        def __init__(self, buffer_size=50000):\n",
    "            self.buffer = []\n",
    "            self.buffer_size = buffer_size\n",
    "\n",
    "        def add(self, experience):\n",
    "            if len(self.buffer) + len(experience) >= self.buffer_size:\n",
    "                self.buffer[0:(len(experience) + len(self.buffer))] = []\n",
    "            self.buffer.extend(experience)\n",
    "\n",
    "        def sample(self, size):\n",
    "            return np.reshape(np.array(random.sample(self.buffer, size)), [size, 5])\n",
    "\n",
    "    def updateTargetGraph(tfVars, tau):\n",
    "        total_vars = len(tfVars)\n",
    "        op_holder = []\n",
    "        for idx, var in enumerate(tfVars[0:total_vars//2]):\n",
    "            op_holder.append(tfVars[idx+total_vars//2].assign((var.value())*tau) + (1-tau)*tfVars[idx+total_vars//2].value())\n",
    "        return op_holder\n",
    "\n",
    "    def updateTarget(op_holder, sess):\n",
    "        for op in op_holder:\n",
    "            sess.run(op)\n",
    "\n",
    "    def processState(states):\n",
    "        return np.reshape(states, input_state_n)\n",
    "\n",
    "    tf.reset_default_graph()\n",
    "    mainqn = qnetwork(h_size)\n",
    "    targetqn = qnetwork(h_size)\n",
    "\n",
    "    init = tf.global_variables_initializer()\n",
    "    saver = tf.train.Saver()\n",
    "    tvars = tf.trainable_variables()\n",
    "\n",
    "    targetOps = updateTargetGraph(tvars, tau)\n",
    "    mybuffer = experience_buffer(replay_buffer_size)\n",
    "\n",
    "    stepList = []\n",
    "    rList = []\n",
    "    total_steps = 0\n",
    "\n",
    "    if not os.path.exists(model_path):\n",
    "        os.makedirs(model_path)\n",
    "\n",
    "    with tf.Session() as sess:\n",
    "        if load_model == True:\n",
    "            print(\"Loading Model..\")\n",
    "            ckpt = tf.train.get_checkpoint_state(model_path)\n",
    "            saver.restore(sess, ckpt.model_checkpoint_path)\n",
    "        sess.run(init)\n",
    "        updateTarget(targetOps)\n",
    "\n",
    "        for episode in range(max_episode_num):\n",
    "            episode_buffer = experience_buffer(replay_buffer_size)\n",
    "            s = env.reset()\n",
    "            s = processState(s)\n",
    "            d = False\n",
    "            rAll = 0\n",
    "            step = 0\n",
    "\n",
    "            while step < max_step_num:\n",
    "                #e = 1. / ((episode + 50.) + 10.)\n",
    "                if np.random.rand(1) < e or total_steps < pre_train_step_num:\n",
    "                    a = np.random.randint(0, 4)\n",
    "                else:\n",
    "                    a = sess.run(mainqn.predict, feed_dict={mainqn.scalarInput[s]})[0]\n",
    "\n",
    "                sn, r, d, _ = env.step(a)\n",
    "                sn = processState(sn)\n",
    "\n",
    "                total_steps += 1\n",
    "                step+=1\n",
    "                episode_buffer.add(np.reshape(np.array([s, a, r, sn, d]), [1, 5]))\n",
    "\n",
    "                if total_steps > pre_train_step_num:\n",
    "                    if e > ende:\n",
    "                        e -= ed\n",
    "\n",
    "                    if total_steps % update_freq == 0:\n",
    "                        trainBatch = mybuffer.sample(batch_size)\n",
    "                        # Doubling\n",
    "                        Q1 = sess.run(mainqn.predict, feed_dict={mainqn.scalarInput: np.vstack(trainBatch[:, 3])})\n",
    "                        Q2 = sess.run(targetqn.Qout, feed_dict={targetqn.scalarInput: np.vstack(trainBatch[:, 3])})\n",
    "                        end_multiplier = -(trainBatch[:, 4] - 1) # filp done\n",
    "                        doubleQ = Q2[range(batch_size), Q1]\n",
    "                        targetQ = trainBatch[:, 2] + (dis*doubleQ*end_multiplier) # reward + gamma * (collect not done q)\n",
    "                        _ = sess.run(mainqn.updateModel,\n",
    "                                feed_dict={\n",
    "                                    mainqn.scalarInput: np.vstack(trianBatch[:, 0]),\n",
    "                                    mainqn.targetQ: targetQ,\n",
    "                                    mainqn.actions: trainBatch[:, 1]\n",
    "                                })\n",
    "                        updateTarget(targetOps, sess)\n",
    "                rAll += r\n",
    "                s = sn\n",
    "                if d == True:\n",
    "                    break\n",
    "            mybuffer.add(episode_buffer.buffer)\n",
    "            stepList.append(step)\n",
    "            rList.append(rAll)\n",
    "            if i % 1000 == 0:\n",
    "                svaer.save(sess, model_path + \"/model-\"+str(episode)+\".ckpt\")\n",
    "                print(\"Saved Model..\")\n",
    "            if len(rList) % 10 == 0:\n",
    "                print(total_steps, np.mean(rList[-10:]), e)\n",
    "        saver.save(sess, path+\"/model-\"+str(episode)+\".ckpt\")\n",
    "        bot_play(mainqn)\n",
    "\n",
    "    print(\"Percent of succesful episodes : \" + str(sum(rList)/num_episodes * 100) + \"%\")\n",
    "\n",
    "    return None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='Asynchronous Advantages Actor-Critic Model tensorflow'></a>\n",
    "## Asynchronous Advantages Actor-Critic Model tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2017-03-21 02:21:27,930] Making new env: Breakout-v0\n",
      "[2017-03-21 02:21:28,101] Making new env: Breakout-v0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100800\n",
      "Starting Worker 0\n",
      "s in worker.work\n",
      "<class 'numpy.ndarray'> float32 (1, 100800)\n",
      "worker work! :  float32 float32\n",
      "[-1.] float64\n",
      "worker train! : \n",
      "float64 float64\n",
      "[-1.]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Exception in thread Thread-16:\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/python/client/session.py\", line 1021, in _do_call\n",
      "    return fn(*args)\n",
      "  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/python/client/session.py\", line 1003, in _run_fn\n",
      "    status, run_metadata)\n",
      "  File \"/usr/lib/python3.5/contextlib.py\", line 66, in __exit__\n",
      "    next(self.gen)\n",
      "  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/python/framework/errors_impl.py\", line 469, in raise_exception_on_not_ok_status\n",
      "    pywrap_tensorflow.TF_GetCode(status))\n",
      "tensorflow.python.framework.errors_impl.InvalidArgumentError: You must feed a value for placeholder tensor 'worker0/Placeholder' with dtype float\n",
      "\t [[Node: worker0/Placeholder = Placeholder[dtype=DT_FLOAT, shape=[], _device=\"/job:localhost/replica:0/task:0/cpu:0\"]()]]\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/lib/python3.5/threading.py\", line 914, in _bootstrap_inner\n",
      "    self.run()\n",
      "  File \"/usr/lib/python3.5/threading.py\", line 862, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"<ipython-input-2-1ca326d0e4e7>\", line 312, in <lambda>\n",
      "    worker_work = lambda: worker.work(sess, coor, saver, max_episode_nums, dis)# NOTE:\n",
      "  File \"<ipython-input-2-1ca326d0e4e7>\", line 267, in work\n",
      "    v_l, p_l, e_l, g_n, v_n = self.train(sess, episode_buffer, 0.0, gamma)\n",
      "  File \"<ipython-input-2-1ca326d0e4e7>\", line 188, in train\n",
      "    ], feed_dict=feed_dict\n",
      "  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/python/client/session.py\", line 766, in run\n",
      "    run_metadata_ptr)\n",
      "  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/python/client/session.py\", line 964, in _run\n",
      "    feed_dict_string, options, run_metadata)\n",
      "  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/python/client/session.py\", line 1014, in _do_run\n",
      "    target_list, options, run_metadata)\n",
      "  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/python/client/session.py\", line 1034, in _do_call\n",
      "    raise type(e)(node_def, op, message)\n",
      "tensorflow.python.framework.errors_impl.InvalidArgumentError: You must feed a value for placeholder tensor 'worker0/Placeholder' with dtype float\n",
      "\t [[Node: worker0/Placeholder = Placeholder[dtype=DT_FLOAT, shape=[], _device=\"/job:localhost/replica:0/task:0/cpu:0\"]()]]\n",
      "\n",
      "Caused by op 'worker0/Placeholder', defined at:\n",
      "  File \"/usr/lib/python3.5/runpy.py\", line 184, in _run_module_as_main\n",
      "    \"__main__\", mod_spec)\n",
      "  File \"/usr/lib/python3.5/runpy.py\", line 85, in _run_code\n",
      "    exec(code, run_globals)\n",
      "  File \"/usr/local/lib/python3.5/dist-packages/ipykernel/__main__.py\", line 3, in <module>\n",
      "    app.launch_new_instance()\n",
      "  File \"/usr/local/lib/python3.5/dist-packages/traitlets/config/application.py\", line 658, in launch_instance\n",
      "    app.start()\n",
      "  File \"/usr/local/lib/python3.5/dist-packages/ipykernel/kernelapp.py\", line 474, in start\n",
      "    ioloop.IOLoop.instance().start()\n",
      "  File \"/usr/local/lib/python3.5/dist-packages/zmq/eventloop/ioloop.py\", line 177, in start\n",
      "    super(ZMQIOLoop, self).start()\n",
      "  File \"/usr/local/lib/python3.5/dist-packages/tornado/ioloop.py\", line 887, in start\n",
      "    handler_func(fd_obj, events)\n",
      "  File \"/usr/local/lib/python3.5/dist-packages/tornado/stack_context.py\", line 275, in null_wrapper\n",
      "    return fn(*args, **kwargs)\n",
      "  File \"/usr/local/lib/python3.5/dist-packages/zmq/eventloop/zmqstream.py\", line 440, in _handle_events\n",
      "    self._handle_recv()\n",
      "  File \"/usr/local/lib/python3.5/dist-packages/zmq/eventloop/zmqstream.py\", line 472, in _handle_recv\n",
      "    self._run_callback(callback, msg)\n",
      "  File \"/usr/local/lib/python3.5/dist-packages/zmq/eventloop/zmqstream.py\", line 414, in _run_callback\n",
      "    callback(*args, **kwargs)\n",
      "  File \"/usr/local/lib/python3.5/dist-packages/tornado/stack_context.py\", line 275, in null_wrapper\n",
      "    return fn(*args, **kwargs)\n",
      "  File \"/usr/local/lib/python3.5/dist-packages/ipykernel/kernelbase.py\", line 276, in dispatcher\n",
      "    return self.dispatch_shell(stream, msg)\n",
      "  File \"/usr/local/lib/python3.5/dist-packages/ipykernel/kernelbase.py\", line 228, in dispatch_shell\n",
      "    handler(stream, idents, msg)\n",
      "  File \"/usr/local/lib/python3.5/dist-packages/ipykernel/kernelbase.py\", line 390, in execute_request\n",
      "    user_expressions, allow_stdin)\n",
      "  File \"/usr/local/lib/python3.5/dist-packages/ipykernel/ipkernel.py\", line 196, in do_execute\n",
      "    res = shell.run_cell(code, store_history=store_history, silent=silent)\n",
      "  File \"/usr/local/lib/python3.5/dist-packages/ipykernel/zmqshell.py\", line 501, in run_cell\n",
      "    return super(ZMQInteractiveShell, self).run_cell(*args, **kwargs)\n",
      "  File \"/usr/local/lib/python3.5/dist-packages/IPython/core/interactiveshell.py\", line 2717, in run_cell\n",
      "    interactivity=interactivity, compiler=compiler, result=result)\n",
      "  File \"/usr/local/lib/python3.5/dist-packages/IPython/core/interactiveshell.py\", line 2827, in run_ast_nodes\n",
      "    if self.run_code(code, result):\n",
      "  File \"/usr/local/lib/python3.5/dist-packages/IPython/core/interactiveshell.py\", line 2881, in run_code\n",
      "    exec(code_obj, self.user_global_ns, self.user_ns)\n",
      "  File \"<ipython-input-2-1ca326d0e4e7>\", line 320, in <module>\n",
      "    A3C()\n",
      "  File \"<ipython-input-2-1ca326d0e4e7>\", line 298, in A3C\n",
      "    workers.append(Worker(gym.make('Breakout-v0'), [210, 160, 3], i, s_size, a_size, global_optimizer, global_episode))\n",
      "  File \"<ipython-input-2-1ca326d0e4e7>\", line 146, in __init__\n",
      "    self.local_AC = AC_Network(obs_shape, s_size, a_size, trainer, self.name)\n",
      "  File \"<ipython-input-2-1ca326d0e4e7>\", line 64, in __init__\n",
      "    self.inputs = tf.placeholder(tf.float32, [None, s_size])\n",
      "  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/python/ops/array_ops.py\", line 1587, in placeholder\n",
      "    name=name)\n",
      "  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/python/ops/gen_array_ops.py\", line 2043, in _placeholder\n",
      "    name=name)\n",
      "  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/python/framework/op_def_library.py\", line 759, in apply_op\n",
      "    op_def=op_def)\n",
      "  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/python/framework/ops.py\", line 2240, in create_op\n",
      "    original_op=self._default_original_op, op_def=op_def)\n",
      "  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/python/framework/ops.py\", line 1128, in __init__\n",
      "    self._traceback = _extract_stack()\n",
      "\n",
      "InvalidArgumentError (see above for traceback): You must feed a value for placeholder tensor 'worker0/Placeholder' with dtype float\n",
      "\t [[Node: worker0/Placeholder = Placeholder[dtype=DT_FLOAT, shape=[], _device=\"/job:localhost/replica:0/task:0/cpu:0\"]()]]\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def A3C():\n",
    "    import os\n",
    "    import multiprocessing\n",
    "    import threading\n",
    "    import functools\n",
    "    import scipy.signal\n",
    "    import scipy.misc\n",
    "\n",
    "    import numpy as np\n",
    "    import tensorflow as tf\n",
    "    import tensorflow.contrib.slim as slim\n",
    "    from tensorflow.python.ops import rnn_cell, rnn\n",
    "\n",
    "    import gym\n",
    "\n",
    "    replay_buffer_size = 30\n",
    "\n",
    "    # HELPER FUNCTIONS\n",
    "    def update_target_ops(from_scope, to_scope):\n",
    "        from_vars = tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES, from_scope)\n",
    "        to_vars = tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES, to_scope)\n",
    "        op_holder=[]\n",
    "        for from_var, to_var in zip(from_vars, to_vars):\n",
    "            op_holder.append(to_var.assign(from_var))\n",
    "        return op_holder\n",
    "\n",
    "    def discount(x, gamma):\n",
    "        return scipy.signal.lfilter([1], [1, -gamma], x[::-1], axis=0)[::-1]\n",
    "\n",
    "    def normalized_columns_initializer(stddev=0.1):#TODO:\n",
    "        def _initializer(shape, dtype=None, partition_info=None):\n",
    "            \"\"\"\n",
    "                >>> a = np.array([[1,2,3],[2,3,4]])\n",
    "                >>> a.sum(axis=0, keepdims=True)\n",
    "                array([[3, 5, 7]])\n",
    "                >>> a\n",
    "                array([[1, 2, 3],\n",
    "                       [2, 3, 4]])\n",
    "                >>> a.sum(axis=0)\n",
    "                array([3, 5, 7])\n",
    "            \"\"\"\n",
    "            out = np.random.randn(*shape).astype(np.float32)\n",
    "            out *= stddev / np.sqrt(np.square(out).sum(axis=0, keepdims=True))\n",
    "            return tf.constant(out)\n",
    "        return _initializer\n",
    "\n",
    "    def preprocessState(state):\n",
    "        \"\"\" Flatten \"\"\"\n",
    "        state = state.astype(np.float32) / 255.0\n",
    "        return np.reshape(state, [1,-1])\n",
    "\n",
    "    def process_State(frame):\n",
    "        s = frame[10:-10,30:-30]\n",
    "        s = scipy.misc.imresize(s,[84,84])\n",
    "        s = np.reshape(s,[np.prod(s.shape)]) / 255.0\n",
    "        return s\n",
    "\n",
    "\n",
    "    # NETWORK\n",
    "    class AC_Network():\n",
    "        def __init__(self, obs_shape, s_size, a_size, trainer, scope):\n",
    "            with tf.variable_scope(scope):\n",
    "                # INPUT LAYERS\n",
    "                self.inputs = tf.placeholder(tf.float32, [None, s_size])\n",
    "                self.imageIn = tf.reshape(self.inputs, [-1]+obs_shape)\n",
    "                self.conv1 = slim.conv2d(inputs=self.imageIn, num_outputs=16,\n",
    "                                         kernel_size=[8, 8], stride=[4, 4], \n",
    "                                         padding='VALID', activation_fn=tf.nn.elu)\n",
    "                self.conv2 = slim.conv2d(inputs=self.conv1, num_outputs=32, \n",
    "                                         kernel_size=[8, 8], stride=[4, 4], \n",
    "                                         padding='VALID', activation_fn=tf.nn.elu)\n",
    "\n",
    "                self.hidden = slim.fully_connected(slim.flatten(self.conv2), 256,\n",
    "                                                   activation_fn=tf.nn.elu)\n",
    "\n",
    "                # RNN FOR TEMPORAL DEPENDENCY\n",
    "                lstm_cell = rnn_cell.BasicLSTMCell(256, state_is_tuple=True)\n",
    "                c_size, h_size = lstm_cell.state_size.c, lstm_cell.state_size.h\n",
    "\n",
    "                c_init = np.zeros((1, c_size), np.float32)\n",
    "                h_init = np.zeros((1, h_size), np.float32)\n",
    "                self.state_init = [c_init, h_init]# TODO: check state_init \n",
    "\n",
    "                c_in = tf.placeholder(tf.float32, [1, c_size])\n",
    "                h_in = tf.placeholder(tf.float32, [1, h_size])\n",
    "                self.state_in = (c_in, h_in)\n",
    "\n",
    "                rnn_in = tf.expand_dims(self.hidden, [0]) # [?, 256]\n",
    "                step_size = tf.shape(self.imageIn)[:1] # 4\n",
    "                state_in = rnn_cell.LSTMStateTuple(c_in, h_in) # [1, 256]\n",
    "                # rnn_in, state_in\n",
    "\n",
    "                lstm_output, lstm_state = tf.nn.dynamic_rnn(lstm_cell, rnn_in, \n",
    "                        initial_state=state_in,\n",
    "                        sequence_length=[256], time_major=False\n",
    "                    )\n",
    "                #TODO:\n",
    "                # rnn_out -> OUTPUT LAYERS, state_out -> worker.work\n",
    "                rnn_out = tf.reshape(lstm_output, [-1, 256])\n",
    "                self.state_out = (lstm_state[0][:1, :], lstm_state[1][:1, :])\n",
    "\n",
    "                # OUTPUT LAYERS\n",
    "                self.policy = slim.fully_connected(rnn_out, a_size, activation_fn=tf.nn.softmax,\n",
    "                                    weights_initializer=normalized_columns_initializer(0.01),\n",
    "                                    biases_initializer=None)#NOTE:\n",
    "                self.value = slim.fully_connected(rnn_out, 1, activation_fn=tf.nn.softmax,\n",
    "                                    weights_initializer=normalized_columns_initializer(1),\n",
    "                                    biases_initializer=None)#NOTE:\n",
    "\n",
    "                if scope != 'global':\n",
    "                    # 3. LOSS FUNCTION\n",
    "                    self.actions = tf.placeholder(tf.int32, [None])\n",
    "                    self.actions_onehot = tf.one_hot(self.actions, a_size, dtype=tf.float32)\n",
    "                    self.target_v = tf.placeholder(tf.float32, [None])\n",
    "                    self.advantages = tf.placeholder(tf.float32, [None]) \n",
    "                    self.responsible_outputs = -tf.reduce_sum(self.policy * self.actions_onehot, [1])\n",
    "\n",
    "                    self.value_loss = 0.5*tf.reduce_sum(tf.square(self.target_v - self.value))\n",
    "                    self.entropy = -tf.reduce_sum(tf.log(self.policy)*self.policy)\n",
    "                    self.policy_loss = tf.log(self.responsible_outputs) * self.advantages\n",
    "                    self.loss = self.policy_loss + 0.5*self.value_loss - 0.01*self.entropy\n",
    "\n",
    "                    # 4. TRAIN LOCAL NETWORK\n",
    "                    local_vars = tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES, scope)\n",
    "                    self.gradients = tf.gradients(self.loss, local_vars)\n",
    "                    self.var_norms = tf.global_norm(local_vars)\n",
    "                    grads, self.grad_norms = tf.clip_by_global_norm(self.gradients, 40.0)\n",
    "\n",
    "                    # 5. APPLY GRADIENT TO GLOBAL NETWORK\n",
    "                    global_vars = tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES, 'global')\n",
    "                    self.apply_grads = trainer.apply_gradients(zip(grads, global_vars))\n",
    "\n",
    "    class Worker():\n",
    "        def __init__(self, env, obs_shape, idx, s_size, a_size, trainer, global_episode):\n",
    "            self.name = 'worker'+str(idx)\n",
    "            self.number = idx\n",
    "            self.trainer = trainer\n",
    "            self.global_episode = global_episode\n",
    "            self.increment = self.global_episode.assign_add(1)\n",
    "            self.episode_rewards = []\n",
    "            self.episode_lengths = []\n",
    "            self.episode_mean_values = []\n",
    "            self.summary_writer = tf.summary.FileWriter('train_'+str(self.number)) \n",
    "            self.env = env\t\n",
    "\n",
    "            self.local_AC = AC_Network(obs_shape, s_size, a_size, trainer, self.name)\n",
    "            self.update_local_ops = update_target_ops('global', self.name)\n",
    "\n",
    "        def train(self, sess, rollout, bootstrap_value, gamma):\n",
    "            # 5. APPLY GRADIENT TO GLOBAL NETWORK\n",
    "            rl = len(rollout)\n",
    "            rollout = np.array(rollout)\n",
    "            observations = rollout[:, 0]\n",
    "            actions = rollout[:, 1]\n",
    "            rewards = rollout[:, 2]\n",
    "            next_observation = rollout[:, 3]\n",
    "            values = rollout[:, 5]\n",
    "\n",
    "            # Advantage and Discounted returns\n",
    "            self.reward_plus = np.asarray(rewards.tolist() + [bootstrap_value])\n",
    "            discounted_rewards = discount(self.reward_plus, gamma)[:-1]\n",
    "            self.value_plus = np.asarray(values.tolist() + [bootstrap_value]) \n",
    "            advantage = rewards + gamma * self.value_plus[1:] - self.value_plus[:-1]\n",
    "            advantage = discount(advantage, gamma)[0]\n",
    "            \n",
    "            advantage = np.array([advantage])\n",
    "            print(advantage, advantage.dtype)\n",
    "            \n",
    "            rnn_state = self.local_AC.state_init\n",
    "            feed_dict = {\n",
    "                self.local_AC.target_v: discounted_rewards,\n",
    "                self.local_AC.actions: actions,\n",
    "                self.local_AC.advantages: advantage,\n",
    "                self.local_AC.state_in[0]: rnn_state[0],\n",
    "                self.local_AC.state_in[1]: rnn_state[1]\n",
    "            }\n",
    "\n",
    "            print(\"worker train! : \")\n",
    "            print(discounted_rewards.dtype, advantage.dtype)\n",
    "            print(advantage)\n",
    "            \n",
    "            v_l, p_l, e_l, g_n, v_n = sess.run([\n",
    "                self.local_AC.value_loss,\n",
    "                self.local_AC.policy_loss,\n",
    "                self.local_AC.entropy,\n",
    "                self.local_AC.grad_norms,\n",
    "                self.local_AC.var_norms,\n",
    "                ], feed_dict=feed_dict\n",
    "            )\n",
    "\n",
    "            return v_l/rl, p_l/rl, e_l/rl, g_n, v_n\n",
    "\n",
    "\n",
    "        def work(self, sess, coor, saver, max_episode_length, gamma):#TODO:\n",
    "            episode_count = sess.run(self.global_episode)\n",
    "            total_steps = 0\n",
    "            print(\"Starting Worker \" + str(self.number))\n",
    "            with sess.as_default(), sess.graph.as_default():\n",
    "                while not coor.should_stop():\n",
    "                    # 1. INTIALIZE LOCAL NETWORK FROM GLOBAL NETWORK\n",
    "                    sess.run(self.update_local_ops) \n",
    "\n",
    "                    episode_buffer = []\n",
    "                    episode_values =  []\n",
    "                    episode_frames = []\n",
    "                    episode_reward = 0\n",
    "                    episode_step_count = 0\n",
    "                    done = False\n",
    "\n",
    "                    s = self.env.reset()\n",
    "                    s = preprocessState(s)\n",
    "                    #s = process_State(s)\n",
    "                    print(\"s in worker.work\")\n",
    "                    print(type(s), s.dtype, s.shape)\n",
    "                    \n",
    "                    \n",
    "                    rnn_state = self.local_AC.state_init\n",
    "                    \n",
    "                    print('worker work! : ', rnn_state[0].dtype,  rnn_state[1].dtype)\n",
    "                    while not done:\n",
    "                        # 2. INTERACTS WITH ITS OWN ENV ; TAKE ACTION\n",
    "                        a_dist, v, rnn_state = sess.run([\n",
    "                            self.local_AC.policy, self.local_AC.value, self.local_AC.state_out\n",
    "                            ], feed_dict={\n",
    "                                self.local_AC.inputs: s,\n",
    "                                self.local_AC.state_in[0]: rnn_state[0],\n",
    "                                self.local_AC.state_in[1]: rnn_state[1]\n",
    "                        }) \n",
    "\n",
    "                        a = np.random.choice(a_dist[0], p=a_dist[0])\n",
    "                        a = np.argmax(a_dist == a)\n",
    "\n",
    "                        sn, r, d, _ = self.env.step(a)\n",
    "\n",
    "                        if d == False:\n",
    "                            sn = preprocessState(sn)\n",
    "                            #sn = process_State(sn)\n",
    "                            \n",
    "\n",
    "                        episode_buffer.append([s, a, r, sn, done, v[0, 0]])\n",
    "                        episode_values.append(v[0, 0])\n",
    "                        episode_reward += r\n",
    "                        s = sn\n",
    "                        total_steps += 1\n",
    "                        episode_step_count += 1\n",
    "\n",
    "                        # 2. INTERACTS WITH ITS OWN ENV ; UPDATE ROLLOUT\n",
    "                        # buffer clean\n",
    "                        if len(episode_buffer) == replay_buffer_size and done != True and episode_step_count != max_episode_length-1:\n",
    "                            vn = sess.run(self.local_AC.value,\n",
    "                                    feed_dict={\n",
    "                                        self.local_AC.inputs: s,\n",
    "                                        self.local_AC.state_in[0]: rnn_state[0],\n",
    "                                        self.local_AC.state_in[1]: rnn_state[1],\n",
    "                                })[0, 0]\n",
    "                            v_l, p_l, e_l, g_n, v_n = self.train(sess, episode_buffer, vn, gamma)\n",
    "                            episode_buffer = []\n",
    "                            sess.run(self.update_local_ops)\n",
    "\n",
    "                        if d == True:\n",
    "                            break\n",
    "                        self.episode_rewards.append(episode_reward)\n",
    "                        self.episode_lengths.append(episode_step_count)\n",
    "                        self.episode_mean_values.append(np.mean(episode_values))\n",
    "                        # buffer update\n",
    "                        if len(episode_buffer) != 0:\n",
    "                            v_l, p_l, e_l, g_n, v_n = self.train(sess, episode_buffer, 0.0, gamma)\n",
    "\n",
    "                    episode_count += 1\n",
    "            return \n",
    "\n",
    "    env = gym.make('Breakout-v0')\n",
    "    input_obs_n = list(env.observation_space.shape)# [210,160,3]\n",
    "    output_action_n = env.action_space.n # 6 : \n",
    "\n",
    "    #s_size = functools.reduce(lambda x,y : x*y, input_obs_n)\n",
    "    s_size = np.prod(input_obs_n)\n",
    "    print(s_size)\n",
    "    a_size = output_action_n\n",
    "\n",
    "    max_episode_nums = 400\n",
    "    dis = .99\n",
    "\n",
    "    load_model = False\n",
    "    model_path = SAVER_DIR+'/a3c_model'\n",
    "\n",
    "    if not os.path.exists(model_path):\n",
    "        os.makedirs(model_path)\n",
    "    # GENERATE NETWORK\n",
    "    with tf.device(\"/cpu:0\"):\n",
    "        global_episode = tf.Variable(0, dtype=tf.int32, name='global_episodes', trainable=False)\n",
    "        global_optimizer = tf.train.AdamOptimizer(learning_rate=1e-3)\n",
    "\n",
    "        master_network = AC_Network([210, 160, 3], s_size, a_size, None, 'global')\n",
    "        worker_num = 1 #multiprocessing.cpu_count()\n",
    "        workers = []\n",
    "        for i in range(worker_num):\n",
    "            workers.append(Worker(gym.make('Breakout-v0'), [210, 160, 3], i, s_size, a_size, global_optimizer, global_episode))\n",
    "            saver = tf.train.Saver(max_to_keep=5)\n",
    "\n",
    "    # THREADING\n",
    "    with tf.Session() as sess:\n",
    "        coor = tf.train.Coordinator()\n",
    "        if load_model :\n",
    "            print(\"Loading Model..\")\n",
    "            ckpt = tf.train.get_checkpoint_state(model_path)# NOTE:\n",
    "            saver.restore(sess, ckpt.model_checkpoint_path)# NOTE:\n",
    "        else:\n",
    "            sess.run(tf.global_variables_initializer())\n",
    "        worker_threads = []\n",
    "        for worker in workers:\n",
    "            worker_work = lambda: worker.work(sess, coor, saver, max_episode_nums, dis)# NOTE:\n",
    "            t = threading.Thread(target=worker_work)\n",
    "            t.start()\n",
    "            worker_threads.append(t)\n",
    "        coor.join(worker_threads)\n",
    "\n",
    "    return None\n",
    "\n",
    "A3C()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "DATASET_DIR = './dataset/'\n",
    "PROJECT_DIR = './projects/RL_collections/'\n",
    "SUMMARY_DIR = PROJECT_DIR+'summaries/'\n",
    "SAVER_DIR = PROJECT_DIR+'models/'\n",
    "CHECKPOINT_DIR = PROJECT_DIR+'checkpoints/'\n",
    "RESULT_DIR = PROJECT_DIR+'results/'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "<a id='Meta_RL'></a>\n",
    "## Meta_RL\n",
    "\n",
    "#### from https://github.com/awjuliani/Meta-RL, https://hackernoon.com/learning-policies-for-learning-policies-meta-reinforcement-learning-rl%C2%B2-in-tensorflow-b15b592a2ddf#.2ck6nb2jp\n",
    "\n",
    "x(t)\n",
    "r(t-1)\n",
    "a(t-1)\n",
    "\n",
    "[IDEA] how could one cluster the tasks with similarities... for meta rl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "import scipy.misc\n",
    "import os\n",
    "import csv\n",
    "import itertools\n",
    "import tensorflow.contrib.slim as slim\n",
    "from PIL import Image\n",
    "from PIL import ImageDraw \n",
    "from PIL import ImageFont\n",
    "\n",
    "\n",
    "# Copies one set of variables to another.\n",
    "# Used to set worker network parameters to those of global network.\n",
    "def update_target_graph(from_scope,to_scope):\n",
    "    from_vars = tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES, from_scope)\n",
    "    to_vars = tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES, to_scope)\n",
    "\n",
    "    op_holder = []\n",
    "    for from_var,to_var in zip(from_vars,to_vars):\n",
    "        op_holder.append(to_var.assign(from_var))\n",
    "    return op_holder\n",
    "\n",
    "# Discounting function used to calculate discounted returns.\n",
    "def discount(x, gamma):\n",
    "    return scipy.signal.lfilter([1], [1, -gamma], x[::-1], axis=0)[::-1]\n",
    "\n",
    "#Used to initialize weights for policy and value output layers\n",
    "def normalized_columns_initializer(std=1.0):\n",
    "    def _initializer(shape, dtype=None, partition_info=None):\n",
    "        out = np.random.randn(*shape).astype(np.float32)\n",
    "        out *= std / np.sqrt(np.square(out).sum(axis=0, keepdims=True))\n",
    "        return tf.constant(out)\n",
    "    return _initializer\n",
    "\n",
    "\n",
    "#This code allows gifs to be saved of the training episode for use in the Control Center.\n",
    "def make_gif(images, fname, duration=2, true_image=False):\n",
    "    import moviepy.editor as mpy\n",
    "\n",
    "    def make_frame(t):\n",
    "        try:\n",
    "            x = images[int(len(images)/duration*t)]\n",
    "        except:\n",
    "            x = images[-1]\n",
    "\n",
    "    if true_image:\n",
    "        return x.astype(np.uint8)\n",
    "    else:\n",
    "        return ((x+1)/2*255).astype(np.uint8)\n",
    "\n",
    "    clip = mpy.VideoClip(make_frame, duration=duration)\n",
    "    clip.write_gif(fname, fps = len(images) / duration,verbose=False)\n",
    "\n",
    "def set_image_bandit(values,probs,selection,trial):\n",
    "    bandit_image = Image.open('./resources/bandit.png')\n",
    "    draw = ImageDraw.Draw(bandit_image)\n",
    "    font = ImageFont.truetype(\"./resources/FreeSans.ttf\", 24)\n",
    "    draw.text((40, 10),str(float(\"{0:.2f}\".format(probs[0]))),(0,0,0),font=font)\n",
    "    draw.text((130, 10),str(float(\"{0:.2f}\".format(probs[1]))),(0,0,0),font=font)\n",
    "    draw.text((60, 370),'Trial: ' + str(trial),(0,0,0),font=font)\n",
    "    bandit_image = np.array(bandit_image)\n",
    "    bandit_image[115:115+floor(values[0]*2.5),20:75,:] = [0,255.0,0] \n",
    "    bandit_image[115:115+floor(values[1]*2.5),120:175,:] = [0,255.0,0]    \n",
    "    bandit_image[101:107,10+(selection*95):10+(selection*95)+80,:] = [80.0,80.0,225.0]\n",
    "    return bandit_image\n",
    "    \n",
    "    \n",
    "def set_image_context(correct, observation,values,selection,trial):\n",
    "    obs = observation * 225.0\n",
    "    obs_a = obs[:,0:1,:]\n",
    "    obs_b = obs[:,1:2,:]\n",
    "    cor = correct * 225.0\n",
    "    obs_a = scipy.misc.imresize(obs_a,[100,100],interp='nearest')\n",
    "    obs_b = scipy.misc.imresize(obs_b,[100,100],interp='nearest')\n",
    "    cor = scipy.misc.imresize(cor,[100,100],interp='nearest')\n",
    "    bandit_image = Image.open('./resources/c_bandit.png')\n",
    "    draw = ImageDraw.Draw(bandit_image)\n",
    "    font = ImageFont.truetype(\"./resources/FreeSans.ttf\", 24)\n",
    "    draw.text((50, 360),'Trial: ' + str(trial),(0,0,0),font=font)\n",
    "    draw.text((50, 330),'Reward: ' + str(values),(0,0,0),font=font)\n",
    "    bandit_image = np.array(bandit_image)\n",
    "    bandit_image[120:220,0:100,:] = obs_a\n",
    "    bandit_image[120:220,100:200,:] = obs_b\n",
    "    bandit_image[0:100,50:150,:] = cor\n",
    "    bandit_image[291:297,10+(selection*95):10+(selection*95)+80,:] = [80.0,80.0,225.0]\n",
    "    return bandit_image\n",
    "\n",
    "\n",
    "def set_image_gridworld(frame,color,reward,step):\n",
    "    a = scipy.misc.imresize(frame,[200,200],interp='nearest')\n",
    "    b = np.ones([400,200,3]) * 255.0\n",
    "    b[0:200,0:200,:] = a \n",
    "    b[200:210,0:200,:] = np.array(color) * 255.0\n",
    "    b = Image.fromarray(b.astype('uint8'))\n",
    "    draw = ImageDraw.Draw(b)\n",
    "    font = ImageFont.truetype(\"./resources/FreeSans.ttf\", 24)\n",
    "    draw.text((40, 280),'Step: ' + str(step),(0,0,0),font=font)\n",
    "    draw.text((40, 330),'Reward: ' + str(reward),(0,0,0),font=font)\n",
    "    c = np.array(b)\n",
    "    return c\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "Expected int32, got list containing Tensors of type '_Message' instead.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-2-f49c322fb23d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    280\u001b[0m     \u001b[0mglobal_episodes\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mVariable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mint32\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'global_episodes'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtrainable\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    281\u001b[0m     \u001b[0mtrainer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mAdamOptimizer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlearning_rate\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1e-3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 282\u001b[0;31m     \u001b[0mmaster_network\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mAC_Network\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma_size\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'global'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# Generate global network\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    283\u001b[0m     \u001b[0;31m#num_workers = multiprocessing.cpu_count() # Set workers ot number of available CPU threads\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    284\u001b[0m     \u001b[0mnum_workers\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-2-f49c322fb23d>\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, a_size, scope, trainer)\u001b[0m\n\u001b[1;32m     66\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprev_actions_onehot\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mone_hot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprev_actions\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0ma_size\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat32\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     67\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 68\u001b[0;31m             \u001b[0mhidden\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconcat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprev_rewards\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprev_actions_onehot\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtimestep\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     69\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     70\u001b[0m             \u001b[0;31m#Recurrent network for temporal dependencies\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/tensorflow/python/ops/array_ops.py\u001b[0m in \u001b[0;36mconcat\u001b[0;34m(concat_dim, values, name)\u001b[0m\n\u001b[1;32m   1073\u001b[0m       ops.convert_to_tensor(concat_dim,\n\u001b[1;32m   1074\u001b[0m                             \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"concat_dim\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1075\u001b[0;31m                             \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdtypes\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mint32\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_shape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1076\u001b[0m                             ).assert_is_compatible_with(tensor_shape.scalar())\n\u001b[1;32m   1077\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0midentity\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mscope\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/tensorflow/python/framework/ops.py\u001b[0m in \u001b[0;36mconvert_to_tensor\u001b[0;34m(value, dtype, name, as_ref, preferred_dtype)\u001b[0m\n\u001b[1;32m    667\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    668\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mret\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 669\u001b[0;31m           \u001b[0mret\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconversion_func\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mas_ref\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mas_ref\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    670\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    671\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mret\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0mNotImplemented\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/tensorflow/python/framework/constant_op.py\u001b[0m in \u001b[0;36m_constant_tensor_conversion_function\u001b[0;34m(v, dtype, name, as_ref)\u001b[0m\n\u001b[1;32m    174\u001b[0m                                          as_ref=False):\n\u001b[1;32m    175\u001b[0m   \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mas_ref\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 176\u001b[0;31m   \u001b[0;32mreturn\u001b[0m \u001b[0mconstant\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mv\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    177\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    178\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/tensorflow/python/framework/constant_op.py\u001b[0m in \u001b[0;36mconstant\u001b[0;34m(value, dtype, shape, name, verify_shape)\u001b[0m\n\u001b[1;32m    163\u001b[0m   \u001b[0mtensor_value\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mattr_value_pb2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mAttrValue\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    164\u001b[0m   tensor_value.tensor.CopyFrom(\n\u001b[0;32m--> 165\u001b[0;31m       tensor_util.make_tensor_proto(value, dtype=dtype, shape=shape, verify_shape=verify_shape))\n\u001b[0m\u001b[1;32m    166\u001b[0m   \u001b[0mdtype_value\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mattr_value_pb2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mAttrValue\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtensor_value\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    167\u001b[0m   const_tensor = g.create_op(\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/tensorflow/python/framework/tensor_util.py\u001b[0m in \u001b[0;36mmake_tensor_proto\u001b[0;34m(values, dtype, shape, verify_shape)\u001b[0m\n\u001b[1;32m    365\u001b[0m       \u001b[0mnparray\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mempty\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnp_dt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    366\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 367\u001b[0;31m       \u001b[0m_AssertCompatible\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    368\u001b[0m       \u001b[0mnparray\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnp_dt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    369\u001b[0m       \u001b[0;31m# check to them.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/tensorflow/python/framework/tensor_util.py\u001b[0m in \u001b[0;36m_AssertCompatible\u001b[0;34m(values, dtype)\u001b[0m\n\u001b[1;32m    300\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    301\u001b[0m       raise TypeError(\"Expected %s, got %s of type '%s' instead.\" %\n\u001b[0;32m--> 302\u001b[0;31m                       (dtype.name, repr(mismatch), type(mismatch).__name__))\n\u001b[0m\u001b[1;32m    303\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    304\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: Expected int32, got list containing Tensors of type '_Message' instead."
     ]
    }
   ],
   "source": [
    "import threading\n",
    "import multiprocessing\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "import tensorflow.contrib.slim as slim\n",
    "import scipy.signal\n",
    "from PIL import Image\n",
    "from PIL import ImageDraw \n",
    "from PIL import ImageFont\n",
    "%matplotlib inline\n",
    "\n",
    "from random import choice\n",
    "from time import sleep\n",
    "from time import time\n",
    "\n",
    "class dependent_bandit():\n",
    "    def __init__(self,difficulty):\n",
    "        self.num_actions = 2\n",
    "        self.difficulty = difficulty\n",
    "        self.reset()\n",
    "        \n",
    "    def set_restless_prob(self):\n",
    "        self.bandit = np.array([self.restless_list[self.timestep],1 - self.restless_list[self.timestep]])\n",
    "        \n",
    "    def reset(self):\n",
    "        self.timestep = 0\n",
    "        if self.difficulty == 'restless': \n",
    "            variance = np.random.uniform(0,.5)\n",
    "            self.restless_list = np.cumsum(np.random.uniform(-variance,variance,(150,1)))\n",
    "            self.restless_list = (self.restless_list - np.min(self.restless_list)) / (np.max(self.restless_list - np.min(self.restless_list))) \n",
    "            self.set_restless_prob()\n",
    "        if self.difficulty == 'easy': bandit_prob = np.random.choice([0.9,0.1])\n",
    "        if self.difficulty == 'medium': bandit_prob = np.random.choice([0.75,0.25])\n",
    "        if self.difficulty == 'hard': bandit_prob = np.random.choice([0.6,0.4])\n",
    "        if self.difficulty == 'uniform': bandit_prob = np.random.uniform()\n",
    "        if self.difficulty != 'independent' and self.difficulty != 'restless':\n",
    "            self.bandit = np.array([bandit_prob,1 - bandit_prob])\n",
    "        else:\n",
    "            self.bandit = np.random.uniform(size=2)\n",
    "        \n",
    "    def pullArm(self,action):\n",
    "        #Get a random number.\n",
    "        if self.difficulty == 'restless': self.set_restless_prob()\n",
    "        self.timestep += 1\n",
    "        bandit = self.bandit[action]\n",
    "        result = np.random.uniform()\n",
    "        if result < bandit:\n",
    "            #return a positive reward.\n",
    "            reward = 1\n",
    "        else:\n",
    "            #return a negative reward.\n",
    "            reward = 0\n",
    "        if self.timestep > 99: \n",
    "            done = True\n",
    "        else: done = False\n",
    "        return reward,done,self.timestep\n",
    "    \n",
    "class AC_Network():\n",
    "    def __init__(self,a_size,scope,trainer):\n",
    "        with tf.variable_scope(scope):\n",
    "            #Input and visual encoding layers\n",
    "            self.prev_rewards = tf.placeholder(shape=[None,1],dtype=tf.float32)\n",
    "            self.prev_actions = tf.placeholder(shape=[None],dtype=tf.int32)\n",
    "            self.timestep = tf.placeholder(shape=[None,1],dtype=tf.float32)\n",
    "            self.prev_actions_onehot = tf.one_hot(self.prev_actions,a_size,dtype=tf.float32)\n",
    "\n",
    "            hidden = tf.concat([self.prev_rewards,self.prev_actions_onehot,self.timestep],1)\n",
    "            \n",
    "            #Recurrent network for temporal dependencies\n",
    "            lstm_cell = tf.contrib.rnn.BasicLSTMCell(48,state_is_tuple=True)\n",
    "            c_init = np.zeros((1, lstm_cell.state_size.c), np.float32)\n",
    "            h_init = np.zeros((1, lstm_cell.state_size.h), np.float32)\n",
    "            self.state_init = [c_init, h_init]\n",
    "            c_in = tf.placeholder(tf.float32, [1, lstm_cell.state_size.c])\n",
    "            h_in = tf.placeholder(tf.float32, [1, lstm_cell.state_size.h])\n",
    "            self.state_in = (c_in, h_in)\n",
    "            rnn_in = tf.expand_dims(hidden, [0])\n",
    "            step_size = tf.shape(self.prev_rewards)[:1]\n",
    "            state_in = tf.contrib.rnn.LSTMStateTuple(c_in, h_in)\n",
    "            lstm_outputs, lstm_state = tf.nn.dynamic_rnn(\n",
    "                lstm_cell, rnn_in, initial_state=state_in, sequence_length=step_size,\n",
    "                time_major=False)\n",
    "            lstm_c, lstm_h = lstm_state\n",
    "            self.state_out = (lstm_c[:1, :], lstm_h[:1, :])\n",
    "            rnn_out = tf.reshape(lstm_outputs, [-1, 48])\n",
    "            \n",
    "            self.actions = tf.placeholder(shape=[None],dtype=tf.int32)\n",
    "            self.actions_onehot = tf.one_hot(self.actions,a_size,dtype=tf.float32)\n",
    "                        \n",
    "            #Output layers for policy and value estimations\n",
    "            self.policy = slim.fully_connected(rnn_out,a_size,\n",
    "                activation_fn=tf.nn.softmax,\n",
    "                weights_initializer=normalized_columns_initializer(0.01),\n",
    "                biases_initializer=None)\n",
    "            self.value = slim.fully_connected(rnn_out,1,\n",
    "                activation_fn=None,\n",
    "                weights_initializer=normalized_columns_initializer(1.0),\n",
    "                biases_initializer=None)\n",
    "            \n",
    "            #Only the worker network need ops for loss functions and gradient updating.\n",
    "            if scope != 'global':\n",
    "                self.target_v = tf.placeholder(shape=[None],dtype=tf.float32)\n",
    "                self.advantages = tf.placeholder(shape=[None],dtype=tf.float32)\n",
    "                \n",
    "                self.responsible_outputs = tf.reduce_sum(self.policy * self.actions_onehot, [1])\n",
    "\n",
    "                #Loss functions\n",
    "                self.value_loss = 0.5 * tf.reduce_sum(tf.square(self.target_v - tf.reshape(self.value,[-1])))\n",
    "                self.entropy = - tf.reduce_sum(self.policy * tf.log(self.policy + 1e-7))\n",
    "                self.policy_loss = -tf.reduce_sum(tf.log(self.responsible_outputs + 1e-7)*self.advantages)\n",
    "                self.loss = 0.5 *self.value_loss + self.policy_loss - self.entropy * 0.05\n",
    "\n",
    "                #Get gradients from local network using local losses\n",
    "                local_vars = tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES, scope)\n",
    "                self.gradients = tf.gradients(self.loss,local_vars)\n",
    "                self.var_norms = tf.global_norm(local_vars)\n",
    "                grads,self.grad_norms = tf.clip_by_global_norm(self.gradients,50.0)\n",
    "                \n",
    "                #Apply local gradients to global network\n",
    "                global_vars = tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES, 'global')\n",
    "                self.apply_grads = trainer.apply_gradients(zip(grads,global_vars))\n",
    "                \n",
    "class Worker():\n",
    "    def __init__(self,game,name,a_size,trainer,model_path,global_episodes):\n",
    "        self.name = \"worker_\" + str(name)\n",
    "        self.number = name        \n",
    "        self.model_path = model_path\n",
    "        self.trainer = trainer\n",
    "        self.global_episodes = global_episodes\n",
    "        self.increment = self.global_episodes.assign_add(1)\n",
    "        self.episode_rewards = []\n",
    "        self.episode_lengths = []\n",
    "        self.episode_mean_values = []\n",
    "        self.summary_writer = tf.summary.FileWriter(\"train_\"+str(self.number))\n",
    "\n",
    "        #Create the local copy of the network and the tensorflow op to copy global paramters to local network\n",
    "        self.local_AC = AC_Network(a_size,self.name,trainer)\n",
    "        self.update_local_ops = update_target_graph('global',self.name)        \n",
    "        self.env = game\n",
    "        \n",
    "    def train(self,rollout,sess,gamma,bootstrap_value):\n",
    "        rollout = np.array(rollout)\n",
    "        actions = rollout[:,0]\n",
    "        rewards = rollout[:,1]\n",
    "        timesteps = rollout[:,2]\n",
    "        prev_rewards = [0] + rewards[:-1].tolist()\n",
    "        prev_actions = [0] + actions[:-1].tolist()\n",
    "        values = rollout[:,4]\n",
    "        \n",
    "        self.pr = prev_rewards\n",
    "        self.pa = prev_actions\n",
    "        # Here we take the rewards and values from the rollout, and use them to \n",
    "        # generate the advantage and discounted returns. \n",
    "        # The advantage function uses \"Generalized Advantage Estimation\"\n",
    "        self.rewards_plus = np.asarray(rewards.tolist() + [bootstrap_value])\n",
    "        discounted_rewards = discount(self.rewards_plus,gamma)[:-1]\n",
    "        self.value_plus = np.asarray(values.tolist() + [bootstrap_value])\n",
    "        advantages = rewards + gamma * self.value_plus[1:] - self.value_plus[:-1]\n",
    "        advantages = discount(advantages,gamma)\n",
    "\n",
    "        # Update the global network using gradients from loss\n",
    "        # Generate network statistics to periodically save\n",
    "        rnn_state = self.local_AC.state_init\n",
    "        feed_dict = {self.local_AC.target_v:discounted_rewards,\n",
    "            self.local_AC.prev_rewards:np.vstack(prev_rewards),\n",
    "            self.local_AC.prev_actions:prev_actions,\n",
    "            self.local_AC.actions:actions,\n",
    "            self.local_AC.timestep:np.vstack(timesteps),\n",
    "            self.local_AC.advantages:advantages,\n",
    "            self.local_AC.state_in[0]:rnn_state[0],\n",
    "            self.local_AC.state_in[1]:rnn_state[1]}\n",
    "        v_l,p_l,e_l,g_n,v_n,_ = sess.run([self.local_AC.value_loss,\n",
    "            self.local_AC.policy_loss,\n",
    "            self.local_AC.entropy,\n",
    "            self.local_AC.grad_norms,\n",
    "            self.local_AC.var_norms,\n",
    "            self.local_AC.apply_grads],\n",
    "            feed_dict=feed_dict)\n",
    "        return v_l / len(rollout),p_l / len(rollout),e_l / len(rollout), g_n,v_n\n",
    "        \n",
    "    def work(self,gamma,sess,coord,saver,train):\n",
    "        episode_count = sess.run(self.global_episodes)\n",
    "        total_steps = 0\n",
    "        print(\"Starting worker \" + str(self.number))\n",
    "        with sess.as_default(), sess.graph.as_default():                 \n",
    "            while not coord.should_stop():\n",
    "                sess.run(self.update_local_ops)\n",
    "                episode_buffer = []\n",
    "                episode_values = []\n",
    "                episode_frames = []\n",
    "                episode_reward = [0,0]\n",
    "                episode_step_count = 0\n",
    "                d = False\n",
    "                r = 0\n",
    "                a = 0\n",
    "                t = 0\n",
    "                self.env.reset()\n",
    "                rnn_state = self.local_AC.state_init\n",
    "                \n",
    "                while d == False:\n",
    "                    #Take an action using probabilities from policy network output.\n",
    "                    a_dist,v,rnn_state_new = sess.run([self.local_AC.policy,self.local_AC.value,self.local_AC.state_out], \n",
    "                        feed_dict={\n",
    "                        self.local_AC.prev_rewards:[[r]],\n",
    "                        self.local_AC.timestep:[[t]],\n",
    "                        self.local_AC.prev_actions:[a],\n",
    "                        self.local_AC.state_in[0]:rnn_state[0],\n",
    "                        self.local_AC.state_in[1]:rnn_state[1]})\n",
    "                    a = np.random.choice(a_dist[0],p=a_dist[0])\n",
    "                    a = np.argmax(a_dist == a)\n",
    "                    \n",
    "                    rnn_state = rnn_state_new\n",
    "                    r,d,t = self.env.pullArm(a)                        \n",
    "                    episode_buffer.append([a,r,t,d,v[0,0]])\n",
    "                    episode_values.append(v[0,0])\n",
    "                    episode_frames.append(set_image_bandit(episode_reward,self.env.bandit,a,t))\n",
    "                    episode_reward[a] += r\n",
    "                    total_steps += 1\n",
    "                    episode_step_count += 1\n",
    "                                            \n",
    "                self.episode_rewards.append(np.sum(episode_reward))\n",
    "                self.episode_lengths.append(episode_step_count)\n",
    "                self.episode_mean_values.append(np.mean(episode_values))\n",
    "                \n",
    "                # Update the network using the experience buffer at the end of the episode.\n",
    "                if len(episode_buffer) != 0 and train == True:\n",
    "                    v_l,p_l,e_l,g_n,v_n = self.train(episode_buffer,sess,gamma,0.0)\n",
    "            \n",
    "                    \n",
    "                # Periodically save gifs of episodes, model parameters, and summary statistics.\n",
    "                if episode_count % 50 == 0 and episode_count != 0:\n",
    "                    if episode_count % 500 == 0 and self.name == 'worker_0' and train == True:\n",
    "                        saver.save(sess,self.model_path+'/model-'+str(episode_count)+'.cptk')\n",
    "                        print(\"Saved Model\")\n",
    "\n",
    "                    if episode_count % 100 == 0 and self.name == 'worker_0':\n",
    "                        self.images = np.array(episode_frames)\n",
    "                        make_gif(self.images,'./frames/image'+str(episode_count)+'.gif',\n",
    "                            duration=len(self.images)*0.1,true_image=True,salience=False)\n",
    "\n",
    "                    mean_reward = np.mean(self.episode_rewards[-50:])\n",
    "                    mean_length = np.mean(self.episode_lengths[-50:])\n",
    "                    mean_value = np.mean(self.episode_mean_values[-50:])\n",
    "                    summary = tf.Summary()\n",
    "                    summary.value.add(tag='Perf/Reward', simple_value=float(mean_reward))\n",
    "                    summary.value.add(tag='Perf/Length', simple_value=float(mean_length))\n",
    "                    summary.value.add(tag='Perf/Value', simple_value=float(mean_value))\n",
    "                    if train == True:\n",
    "                        summary.value.add(tag='Losses/Value Loss', simple_value=float(v_l))\n",
    "                        summary.value.add(tag='Losses/Policy Loss', simple_value=float(p_l))\n",
    "                        summary.value.add(tag='Losses/Entropy', simple_value=float(e_l))\n",
    "                        summary.value.add(tag='Losses/Grad Norm', simple_value=float(g_n))\n",
    "                        summary.value.add(tag='Losses/Var Norm', simple_value=float(v_n))\n",
    "                    self.summary_writer.add_summary(summary, episode_count)\n",
    "\n",
    "                    self.summary_writer.flush()\n",
    "                if self.name == 'worker_0':\n",
    "                    sess.run(self.increment)\n",
    "                episode_count += 1\n",
    "                \n",
    "                \n",
    "gamma = .8 # discount rate for advantage estimation and reward discounting\n",
    "a_size = 2 # Agent can move Left, Right, or Fire\n",
    "load_model = True\n",
    "train = False\n",
    "model_path = './model_meta'\n",
    "\n",
    "\n",
    "tf.reset_default_graph()\n",
    "\n",
    "if not os.path.exists(model_path):\n",
    "    os.makedirs(model_path)\n",
    "    \n",
    "if not os.path.exists('./frames'):\n",
    "    os.makedirs('./frames')\n",
    "\n",
    "    \n",
    "with tf.device(\"/cpu:0\"): \n",
    "    global_episodes = tf.Variable(0,dtype=tf.int32,name='global_episodes',trainable=False)\n",
    "    trainer = tf.train.AdamOptimizer(learning_rate=1e-3)\n",
    "    master_network = AC_Network(a_size,'global',None) # Generate global network\n",
    "    #num_workers = multiprocessing.cpu_count() # Set workers ot number of available CPU threads\n",
    "    num_workers = 1\n",
    "    workers = []\n",
    "    # Create worker classes\n",
    "    for i in range(num_workers):\n",
    "        workers.append(Worker(dependent_bandit('uniform'),i,a_size,trainer,model_path,global_episodes))\n",
    "    saver = tf.train.Saver(max_to_keep=5)\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    coord = tf.train.Coordinator()\n",
    "    if load_model == True:\n",
    "        print('Loading Model...')\n",
    "        ckpt = tf.train.get_checkpoint_state(model_path)\n",
    "        saver.restore(sess,ckpt.model_checkpoint_path)\n",
    "    else:\n",
    "        sess.run(tf.global_variables_initializer())\n",
    "        \n",
    "    worker_threads = []\n",
    "    for worker in workers:\n",
    "        worker_work = lambda: worker.work(gamma,sess,coord,saver,train)\n",
    "        thread = threading.Thread(target=(worker_work))\n",
    "        thread.start()\n",
    "        worker_threads.append(thread)\n",
    "    coord.join(worker_threads)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
